{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L \n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torchmetrics\n",
    "import random\n",
    "import glob\n",
    "import math\n",
    "from einops import rearrange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MTCNPatchEmbed(nn.Module):\n",
    "    '''\n",
    "    输入: [B, L, M]\n",
    "    1D卷积同时完成patch和embedding\n",
    "    输出: [B, M, D, N] (CI)  |  [B 1 D N] (CM)\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                P=8,\n",
    "                S=4,\n",
    "                D=32,\n",
    "                M = 6,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.P = P\n",
    "        self.S = S\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=1, \n",
    "            out_channels=D, \n",
    "            kernel_size=P, \n",
    "            stride=S\n",
    "            )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = rearrange(inputs, 'B L M -> B M L')  # [B L M] -> [B M L]\n",
    "        bs = x.shape[0]\n",
    "        x = x.unsqueeze(2)  # [B, M, L] -> [B, M, 1, L]\n",
    "        x = rearrange(x, 'B M R L -> (B M) R L')  # [B, M, 1, L] -> [B*M, 1, L]\n",
    "        # padding\n",
    "        x_pad = F.pad(\n",
    "            x,\n",
    "            pad=(0, self.P - self.S),\n",
    "            mode='replicate'\n",
    "            )  # [B*M, 1, L] -> [B*M, 1, L+P-S]\n",
    "        x_emb = self.conv(x_pad)  # [B*M, 1, L+P-S] -> [B*M, D, N]\n",
    "        x_emb = rearrange(x_emb, '(B M) D N -> B M D N', B=bs)  # [B*M, D, N] -> [B, M, D, N]\n",
    "        return x_emb  # x_emb: [B, M, D, N]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====================Norm====================\n",
    "class MixerNormLayer(nn.Module):\n",
    "    '''\n",
    "    Batch / Layer Norm\n",
    "    输入维度: (B, M, N, D)\n",
    "    输出维度: (B, M, N, D)\n",
    "    如果是batchnorm,会先合并B和M的维度,然后把D转到倒数第二个维度，再进行batchnorm\n",
    "    '''\n",
    "    def __init__(self, norm_type,d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_type = norm_type\n",
    "        \n",
    "        if \"batch\" in norm_type.lower():\n",
    "            self.norm = nn.BatchNorm1d(d_model) # 默认格式为(N,C) 或 (N,C,L)\n",
    "        else:\n",
    "            self.norm = nn.LayerNorm(d_model)  # 默认对最后一个维度进行LayerNorm\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        if \"batch\" in self.norm_type.lower():\n",
    "            # 将数据转为N C L 的格式\n",
    "            B = inputs.shape[0]\n",
    "            inputs = rearrange(inputs, \"B M N D -> (B M) N D\")\n",
    "            inputs = inputs.transpose(1, 2)  # [BM, D, N])\n",
    "            inputs = self.norm(inputs)\n",
    "            inputs = inputs.transpose(1, 2)  # [BM, N, D])\n",
    "            output = rearrange(inputs, \"(B M) N D -> B M N D\", B=B)\n",
    "        else:\n",
    "            output = self.norm(inputs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IC LOSS\n",
    "class ICLoss(nn.Module):\n",
    "    def __init__(self, gamma=0):\n",
    "        super(ICLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred_ = torch.mean(y_pred, dim=1).unsqueeze(1)\n",
    "        y_pred_demean = y_pred_ - y_pred_.mean(dim=0, keepdim=True)\n",
    "        y_true_demean = y_true - y_true.mean(dim=0, keepdim=True)\n",
    "        cos_sim = F.cosine_similarity(y_pred_demean, y_true_demean, dim=0)\n",
    "        loss1 = 1 - cos_sim.mean()\n",
    "        \n",
    "        if self.gamma > 0:\n",
    "            F_inv = torch.linalg.inv(torch.matmul(y_pred_demean.T, y_pred_demean))\n",
    "            penalty = torch.trace(F_inv)\n",
    "            loss1 = loss1 + self.gamma * penalty\n",
    "        return loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MtcnTSMixer(nn.Module):\n",
    "    '''\n",
    "    DW卷积--时间依赖\n",
    "    inputs: [B, M, D, N]\n",
    "    output: [B, M*D, N]\n",
    "    '''\n",
    "    def __init__(self, M, D, kernel_size):\n",
    "        super().__init__()\n",
    "        self.dw_conv = nn.Conv1d(\n",
    "            in_channels=M*D, \n",
    "            out_channels=M*D, \n",
    "            kernel_size=kernel_size,\n",
    "            groups=M*D,\n",
    "            padding='same'\n",
    "        )\n",
    "        self.bn = nn.BatchNorm1d(M*D)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        # inputs: [B, M, D, N]\n",
    "        x_out = rearrange(inputs, 'b m d n -> b (m d) n')             # [B, M, D, N] -> [B, M*D, N]\n",
    "        x_out = self.dw_conv(x_out)                                   # [B, M*D, N]  -> [B, M*D, N]\n",
    "        x_out = self.bn(x_out)                                        # [B, M*D, N]  -> [B, M*D, N]\n",
    "        return x_out\n",
    "\n",
    "class MtcnFeatureMixer(nn.Module):\n",
    "    '''\n",
    "    特征依赖\n",
    "    PW卷积1--同一个变量的特征交互\n",
    "    inputs: [B, M*D, N]\n",
    "    output: [B, M, D, N]\n",
    "    '''\n",
    "    def __init__(self, M, D, r):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.pw_con1 = nn.Conv1d(\n",
    "            in_channels=M*D, \n",
    "            out_channels=r*M*D, \n",
    "            kernel_size=1,\n",
    "            groups=M\n",
    "        )\n",
    "        self.pw_con2 = nn.Conv1d(\n",
    "            in_channels=r*M*D, \n",
    "            out_channels=M*D, \n",
    "            kernel_size=1,\n",
    "            groups=M\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # inputs: [B, M, D, N]\n",
    "        x = F.gelu(self.pw_con1(x))\n",
    "        x = self.pw_con2(x)\n",
    "        # reshape\n",
    "        x = rearrange(x, 'b (m d) n -> b m d n', d = self.D)          # [B, M*D, N] -> [B, M, D,N]\n",
    "        return x # out shape :[B, M, D, N] \n",
    "    \n",
    "    \n",
    "class MtcnChannelMixer(nn.Module):\n",
    "    '''\n",
    "    通道依赖\n",
    "    同一个特征的不同变量交互\n",
    "    inputs:  [B, M, D, N]\n",
    "    output:  [B, M, D, N]\n",
    "    '''\n",
    "    def __init__(self, M, D, r):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.pw_con3 = nn.Conv1d(\n",
    "            in_channels=M*D, \n",
    "            out_channels=r*M*D, \n",
    "            kernel_size=1,\n",
    "            groups=D\n",
    "        )\n",
    "        self.pw_con4 = nn.Conv1d(\n",
    "            in_channels=r*M*D, \n",
    "            out_channels=M*D, \n",
    "            kernel_size=1,\n",
    "            groups=D\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # inputs: [B, M, D, N]        \n",
    "        x = x.permute(0,2,1,3)                                      # [B, D, M, N]\n",
    "        x = rearrange(x, 'b d m n -> b (d m) n')                    # [B, D, M, N] -> [B, D*M, N]\n",
    "        \n",
    "        x = F.gelu(self.pw_con3(x))\n",
    "        x = self.pw_con4(x)\n",
    "        # reshape\n",
    "        x = rearrange(x, 'b (d m) n -> b d m n', d=self.D)  \n",
    "        x = x.permute(0,2,1,3)  \n",
    "        return x  # out shape :[B, M, D, N] \n",
    "\n",
    "\n",
    "class ModernTCNBlock(nn.Module):\n",
    "    def __init__(self, M, D, kernel_size, expansion_factor = 1,N = None,channel_mixer = True):\n",
    "        super().__init__()\n",
    "        self.channel_mixer = channel_mixer\n",
    "        self.ts_mixer = MtcnTSMixer(M, D, kernel_size)\n",
    "        self.feature_mixer = MtcnFeatureMixer(M, D, r = expansion_factor)\n",
    "        if self.channel_mixer:\n",
    "            self.channel_mixer = MtcnChannelMixer(M, D, r = expansion_factor)\n",
    "    \n",
    "    def forward(self, x_emb):\n",
    "        # x_emb: [B, M, D, N]\n",
    "        res = x_emb\n",
    "        x_emb = self.ts_mixer(x_emb)\n",
    "        x_emb = self.feature_mixer(x_emb)\n",
    "\n",
    "        if self.channel_mixer:\n",
    "            x_emb = self.channel_mixer(x_emb)\n",
    "\n",
    "        out = res + x_emb  # Residual connection\n",
    "        return out # out: [B, M, D, N]\n",
    "    \n",
    "class TcnFactorNetV3(L.LightningModule):\n",
    "    '''\n",
    "    B : batch size\n",
    "    M : 多变量序列的变量数\n",
    "    L : 过去序列的长度\n",
    "    T : 预测序列的长度\n",
    "    N : 分Patch后Patch的个数\n",
    "    D : 每个变量的通道数\n",
    "    P : kernel size of embedding layer\n",
    "    S : stride of embedding layer\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 M = 6,\n",
    "                 L = 30,\n",
    "                 T = 1,\n",
    "                 D = 16,\n",
    "                 P = 8,\n",
    "                 kernel_size = 10,\n",
    "                 expansion_factor = 1,\n",
    "                 num_layers = 1,\n",
    "                 label_idx = 1,\n",
    "                 factormean = True,\n",
    "                 S = None,\n",
    "                 num_MLP = 1,\n",
    "                 gamma = 0,\n",
    "                 channel_mixer = True,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        if S is None:\n",
    "            S = int(P // 2)\n",
    "            print(f\"设定S为P的一半: .{S}\")\n",
    "            \n",
    "        N = L // S\n",
    "        self.embed_layer = MTCNPatchEmbed(P, S, D,M=M)\n",
    "\n",
    "        \n",
    "        layers = [ModernTCNBlock(M=M,\n",
    "                                 D=D,\n",
    "                                 kernel_size=kernel_size,\n",
    "                                 N =N,\n",
    "                                 expansion_factor=expansion_factor,\n",
    "                                 channel_mixer=channel_mixer\n",
    "                                 ) for _ in range(num_layers)\n",
    "                  ]\n",
    "        self.tcn_encoder = nn.Sequential(*layers)\n",
    "\n",
    "        # predict head\n",
    "        self.ts_linear = nn.Linear(D*N,D)   \n",
    "        \n",
    "        # task specific setting\n",
    "        self.label_idx = label_idx\n",
    "        \n",
    "        # predict layer\n",
    "        self.predict_layer = nn.Sequential(\n",
    "            nn.Linear(M*D, 50)\n",
    "        )\n",
    "        self.loss_fn = ICLoss(gamma)\n",
    "    \n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        x_emb = self.embed_layer(inputs)  # [B, M, L] -> [B, M, D, N]\n",
    "        x_out = self.tcn_encoder(x_emb)  # [B, M, D, N]\n",
    "        x_out = rearrange(x_out, 'b m d n -> b m (d n)')\n",
    "        \n",
    "        x_out = F.gelu(self.ts_linear(x_out))  # [B, M, D]\n",
    "        x_out = rearrange(x_out, 'b m d -> b (m d)') # [B, M*D]\n",
    "        return x_out\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs = batch[0].squeeze(0)\n",
    "        \n",
    "        y_pred = self.forward(inputs)\n",
    "        y_pred = self.predict_layer(y_pred)\n",
    "        \n",
    "        labels  = batch[1].squeeze(0)[:,self.label_idx].reshape(-1,1)\n",
    "        loss = self.loss_fn(y_pred,labels)\n",
    "        self.log(\"train_loss\",loss, on_epoch=True,prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        inputs = batch[0].squeeze(0)\n",
    "        y_pred = self.predict_layer(y_pred)\n",
    "        ids = batch[1].squeeze(0)[:,0]\n",
    "        y_pred = self.forward(inputs)\n",
    "        return torch.cat((y_pred,ids.unsqueeze(1)),1).cpu().numpy()\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        inputs = batch[0].squeeze(0)\n",
    "        y_pred = self.forward(inputs)\n",
    "        y_pred = self.predict_layer(y_pred)\n",
    "        labels  = batch[1].squeeze(0)[:,self.label_idx].reshape(-1,1)\n",
    "        loss = self.loss_fn(y_pred,labels)\n",
    "        self.log(\"val_loss\",loss,prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
