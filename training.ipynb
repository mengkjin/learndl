{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main path: /Users/mengkjin/workspace/learndl\n",
      "src.INSTANCE_RECORD can be accessed to check ['trainer', 'account', 'factor']\n"
     ]
    }
   ],
   "source": [
    "from src.algo.nn.model import TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main path: /Users/mengkjin/workspace/learndl\n",
      "src.INSTANCE_RECORD can be accessed to check ['trainer', 'account', 'factor']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'simple_lstm': src.algo.nn.model.Recurrent.simple_lstm,\n",
       " 'gru': src.algo.nn.model.Recurrent.gru,\n",
       " 'lstm': src.algo.nn.model.Recurrent.lstm,\n",
       " 'resnet_lstm': src.algo.nn.model.Recurrent.resnet_lstm,\n",
       " 'resnet_gru': src.algo.nn.model.Recurrent.resnet_gru,\n",
       " 'transformer': src.algo.nn.model.Recurrent.transformer,\n",
       " 'tcn': src.algo.nn.model.Recurrent.tcn,\n",
       " 'rnn_ntask': src.algo.nn.model.Recurrent.rnn_ntask,\n",
       " 'rnn_general': src.algo.nn.model.Recurrent.rnn_general,\n",
       " 'gru_dsize': src.algo.nn.model.Recurrent.gru_dsize,\n",
       " 'patch_tst': src.algo.nn.model.PatchTST.patch_tst,\n",
       " 'modern_tcn': src.algo.nn.model.ModernTCN.modern_tcn,\n",
       " 'ts_mixer': src.algo.nn.model.TSMixer.ts_mixer,\n",
       " 'tra': src.algo.nn.model.TRA.tra,\n",
       " 'factor_vae': src.algo.nn.model.FactorVAE.FactorVAE,\n",
       " 'risk_att_gru': src.algo.nn.model.RiskAttGRU.risk_att_gru,\n",
       " 'ple_gru': src.algo.nn.model.PLE.ple_gru,\n",
       " 'tft': src.algo.nn.model.TemporalFusionTransformer.TemporalFusionTransformer,\n",
       " 'lgbm': src.algo.boost.booster.lgbm.Lgbm,\n",
       " 'ada': src.algo.boost.booster.ada.AdaBoost,\n",
       " 'xgboost': src.algo.boost.booster.xgboost.XgBoost,\n",
       " 'catboost': src.algo.boost.booster.catboost.CatBoost}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.api import ModelAPI  \n",
    "ModelAPI.Trainer.available_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main path: /Users/mengkjin/workspace/learndl\n",
      "src.INSTANCE_RECORD can be accessed to check ['trainer', 'account', 'factor']\n",
      "--------------------------------------------------------------------------------\n",
      "***************************** PARSER TRAINING ARGS *****************************\n",
      "--Process Queue : Data + Fit + Test\n",
      "--Start Training New!\n",
      "--Model_name is set to tft_day_ShortTest!\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "******************************* SETUP CALLBACKS *******************************\n",
      "ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True) , reset optimizer on some epoch (can speedup scheduler)\n",
      "CallbackTimer(verbosity=10) , record time cost of callback hooks\n",
      "EarlyStoppage(patience=20) , stop fitting when validation score cease to improve\n",
      "ValidationConverge(patience=5,eps=1e-05) , stop fitting when valid_score converge\n",
      "EarlyExitRetrain(earliest=10,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100, 1]) , retrain with new lr if fitting stopped too early\n",
      "NanLossRetrain(max_attempt=4) , retrain if fitting encounters nan loss\n",
      "BatchDisplay(verbosity=10) , display batch progress bar\n",
      "StatusDisplay(verbosity=10) , display epoch and event information\n",
      "DetailedAlphaAnalysis(use_num=avg)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[44m25-06-16 23:48:13|MOD:logger      |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "********************************** MODEL INFO **********************************\n",
      "Model Name   : tft_day_ShortTest\n",
      "Model Module : tft\n",
      "  -->  Model Params :\n",
      "    -->  hidden_dim : [16]\n",
      "    -->  seqlens : [{'day': 30, 'indus': 30}]\n",
      "    -->  dropout : [0.1]\n",
      "    -->  ffn_dim : [32]\n",
      "    -->  num_heads : [4]\n",
      "    -->  pred_len : [1]\n",
      "    -->  encoder_layers : [2]\n",
      "    -->  model.submodels : ['best']\n",
      "    -->  model.input_type : data\n",
      "    -->  model.data.types : day+indus\n",
      "    -->  verbosity : 10\n",
      "Model Inputs : data\n",
      "  -->  Data Types : ['day', 'indus']\n",
      "Model Labels : ['std_lag1_10', 'rtn_lag1_10']\n",
      "Model Period : 20240701 ~ 20240930\n",
      "Sampling     : sequential\n",
      "Shuffling    : epoch\n",
      "Random Seed  : None\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m25-06-16 23:48:13|MOD:logger      |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Mon Jun 16 23:48:13 2025!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try using /Users/mengkjin/workspace/learndl/data/Interim/DataSet/day+indus.20241211.pt , success!\n",
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n",
      "Pre-Norming method of [indus] : {'divlast': False, 'histnorm': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m25-06-16 23:48:16|MOD:logger      |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 2.4 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m25-06-16 23:48:16|MOD:logger      |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Mon Jun 16 23:48:16 2025!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m25-06-16 23:48:16|MOD:logger      |\u001b[0m: \u001b[1m\u001b[34mFirst Iterance: (20240701 , 0)\u001b[0m\n",
      "Train Ep#  0 loss : 0.00000:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score function of [spearman] calculated and success!\n",
      "loss function of [net_specific] calculated and success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Ep#  0 loss : nan: 100%|██████████| 38/38 [00:26<00:00,  1.43it/s]\n",
      "Valid Ep#  0 score : -0.01147: 100%|██████████| 10/10 [00:03<00:00,  2.85it/s]\n",
      "\u001b[32mFirstBite Ep#  0 : loss  nan, train 0.01383, valid 0.01028, best 0.0103, lr1.3e-03\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[45m25-06-16 23:48:49|MOD:logger      |\u001b[0m: \u001b[1m\u001b[35mInitialize a new model to retrain! Lives remaining 4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_epoch': 1, 'stage': 'fit', 'dataset': 'valid', 'epoch': -1, 'attempt': 0, 'round': 0, 'model_num': 0, 'model_date': 20240701, 'model_submodel': 'best', 'epoch_event': [], 'best_attempt_metric': None, 'fitted_model_num': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m25-06-16 23:48:49|MOD:logger      |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 35.9 Seconds\u001b[0m\n",
      "\u001b[41m\u001b[1mMessageCapturer Finished Capturing Train Model Of Short Test, saved to /Users/mengkjin/workspace/learndl/models/tft_day_ShortTest/detailed_analysis/training_output.html\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'model_date'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelAPI  \n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbasic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MessageCapturer\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m app = \u001b[43mModelAPI\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshort_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtft\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learndl/src/api/model.py:120\u001b[39m, in \u001b[36mModelAPI.short_test\u001b[39m\u001b[34m(cls, module, verbosity)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshort_test\u001b[39m(\u001b[38;5;28mcls\u001b[39m , module : \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m , verbosity : \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[32m10\u001b[39m):\n\u001b[32m    111\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    Short test a module\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    module :\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    118\u001b[39m \u001b[33;03m        int : use the verbosity level , if above 10 will print more details\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mTrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshort_test\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mstage\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresume\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckname\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learndl/src/model/model_module/application/trainer.py:56\u001b[39m, in \u001b[36mModelTrainer.train\u001b[39m\u001b[34m(cls, module, short_test, message_capturer, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m         trainer = \u001b[38;5;28mcls\u001b[39m.initialize(module = module , short_test = short_test , **kwargs)\n\u001b[32m     55\u001b[39m         capture.set_export_path(trainer.path_training_output)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m         \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     58\u001b[39m     trainer = \u001b[38;5;28mcls\u001b[39m.initialize(module = module , short_test = short_test , **kwargs).go()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learndl/src/model/util/classes.py:365\u001b[39m, in \u001b[36mBaseTrainer.go\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgo\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    364\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''alias of main_process'''\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmain_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learndl/src/model/util/classes.py:355\u001b[39m, in \u001b[36mBaseTrainer.main_process\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    351\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mstage_queue is empty , please check src.INSTANCE_RECORD[\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtrainer\u001b[39m\u001b[33m'\u001b[39m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stage_queue: \u001b[38;5;28mself\u001b[39m.stage_data()\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stage_queue:  \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstage_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stage_queue: \u001b[38;5;28mself\u001b[39m.stage_test()\n\u001b[32m    359\u001b[39m \u001b[38;5;28mself\u001b[39m.on_summarize_model()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learndl/src/model/util/classes.py:274\u001b[39m, in \u001b[36mBaseTrainer.hook_wrapper.<locals>.wrapper\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    272\u001b[39m trainer.callback.at_enter(hook , verbosity)\n\u001b[32m    273\u001b[39m action_status()\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m \u001b[43maction_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m action_model()\n\u001b[32m    276\u001b[39m trainer.callback.at_exit(hook , verbosity)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learndl/src/model/util/classes.py:380\u001b[39m, in \u001b[36mBaseTrainer.stage_fit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    378\u001b[39m         \u001b[38;5;28mself\u001b[39m.logger.warning(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mFirst Iterance: (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status.model_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m , \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status.model_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_fit_model_start()\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    381\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_fit_model_end()\n\u001b[32m    382\u001b[39m \u001b[38;5;28mself\u001b[39m.on_fit_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learndl/src/model/model_module/module/nn.py:67\u001b[39m, in \u001b[36mNNPredictor.fit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer.verbosity > \u001b[32m10\u001b[39m: \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mmodel fit start\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     66\u001b[39m \u001b[38;5;28mself\u001b[39m.new_model()\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_fit_epoches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learndl/src/model/util/classes.py:421\u001b[39m, in \u001b[36mBaseTrainer.iter_fit_epoches\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m.status\n\u001b[32m    420\u001b[39m \u001b[38;5;28mself\u001b[39m.on_before_fit_epoch_end()\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mon_fit_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learndl/src/model/util/classes.py:276\u001b[39m, in \u001b[36mBaseTrainer.hook_wrapper.<locals>.wrapper\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    274\u001b[39m action_trainer()\n\u001b[32m    275\u001b[39m action_model()\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m.\u001b[49m\u001b[43mat_exit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbosity > \u001b[32m10\u001b[39m: \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of stage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.status.stage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m end\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learndl/src/model/callback/manager.py:19\u001b[39m, in \u001b[36mCallBackManager.at_exit\u001b[39m\u001b[34m(self, hook, verbosity)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mat_exit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hook , verbosity : \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     [\u001b[43mcb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mat_exit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learndl/src/model/util/classes.py:570\u001b[39m, in \u001b[36mBaseCallBack.at_exit\u001b[39m\u001b[34m(self, hook, verbosity)\u001b[39m\n\u001b[32m    569\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mat_exit\u001b[39m(\u001b[38;5;28mself\u001b[39m , hook : \u001b[38;5;28mstr\u001b[39m , verbosity : \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m): \n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    571\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m verbosity > \u001b[32m10\u001b[39m: \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of callback \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m end\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learndl/src/model/callback/display.py:175\u001b[39m, in \u001b[36mStatusDisplay.on_fit_epoch_end\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_fit_epoch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.status.fit_loop_breaker: \u001b[38;5;28mself\u001b[39m.record_texts[\u001b[33m'\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.status.fit_loop_breaker.trigger_reason\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.status.epoch_event: \u001b[38;5;28mself\u001b[39m.display(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevent_sdout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m.\u001b[49m\u001b[43mepoch_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learndl/src/model/callback/display.py:102\u001b[39m, in \u001b[36mStatusDisplay.event_sdout\u001b[39m\u001b[34m(self, event)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m event == \u001b[33m'\u001b[39m\u001b[33mnanloss\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    101\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m.status.as_dict())\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     sdout = \u001b[33;43m'\u001b[39;49m\u001b[33;43mModel \u001b[39;49m\u001b[38;5;132;43;01m{model_date}\u001b[39;49;00m\u001b[33;43m.\u001b[39;49m\u001b[38;5;132;43;01m{model_num}\u001b[39;49;00m\u001b[33;43m Attempt\u001b[39;49m\u001b[38;5;132;43;01m{attempt}\u001b[39;49;00m\u001b[33;43m, epoch\u001b[39;49m\u001b[38;5;132;43;01m{epoch}\u001b[39;49;00m\u001b[33;43m got nanloss!\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(event)\n",
      "\u001b[31mKeyError\u001b[39m: 'model_date'"
     ]
    }
   ],
   "source": [
    "from src.api import ModelAPI  \n",
    "from src.basic.util.logger import MessageCapturer\n",
    "\n",
    "app = ModelAPI.short_test(module = 'tft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4993, device='mps:0'),\n",
       " {'x': (torch.Size([4993, 30, 6]), torch.Size([4993, 30, 35])),\n",
       "  'y': torch.Size([4993, 1]),\n",
       "  'w': None,\n",
       "  'i': torch.Size([4993, 2]),\n",
       "  'valid': torch.Size([4993])})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.basic import INSTANCE_RECORD\n",
    "trainer = INSTANCE_RECORD.trainer\n",
    "trainer.model.batch_data.valid.sum() , trainer.model.batch_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.util import BatchOutput\n",
    "batch_output = BatchOutput.nn_module(trainer.model.net , trainer.model.batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3959, -0.0357,  0.7765]],\n",
       "\n",
       "        [[-0.2095,  0.4502,  0.3229]],\n",
       "\n",
       "        [[ 0.5385, -0.0360,  0.9484]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0537,  0.4237,  0.7610]],\n",
       "\n",
       "        [[-0.2501,  0.3930,  0.1985]],\n",
       "\n",
       "        [[ 0.4268,  0.1428,  0.7888]]], device='mps:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_output['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan], device='mps:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.net.loss(trainer.model.batch_data.y , **batch_output.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred': torch.Size([4993, 1]),\n",
       " 'predictions': torch.Size([4993, 1, 3]),\n",
       " 'static_weights': torch.Size([4993, 30, 8]),\n",
       " 'historical_weights': torch.Size([4993, 30, 6]),\n",
       " 'future_weights': float}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='mps:0') torch.Size([4993, 1]) torch.Size([4993, 1])\n",
      "tensor([0.2509, 0.7786, 1.0646,  ..., 0.0471, 0.0449, 0.6996], device='mps:0')\n",
      "tensor([], device='mps:0')\n",
      "tensor([], device='mps:0')\n",
      "[tensor([0.4907], device='mps:0'), tensor([nan], device='mps:0'), tensor([nan], device='mps:0')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([nan], device='mps:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def loss(label : torch.Tensor , pred : torch.Tensor | None = None , w : torch.Tensor | None = None , dim = None , \n",
    "         quantiles : list[float] = [0.1,0.5,0.9] , predictions : torch.Tensor | None = None , **kwargs):\n",
    "    assert predictions is not None , f'predictions should be provided'\n",
    "    assert predictions.shape[-1] == len(quantiles) , f'shape of predictions {predictions.shape} should be (...,{len(quantiles)})'\n",
    "    if predictions.ndim == label.ndim + 1: predictions = predictions[...,0]\n",
    "    assert predictions.ndim == label.ndim == 2 , f'shape of predictions {predictions.shape} and label {label.shape} should be (...,1)'\n",
    "    if w is None:\n",
    "        w1 = 1.\n",
    "    else:\n",
    "        w1 = w / w.sum(dim=dim,keepdim=True) * (w.numel() if dim is None else w.size(dim=dim))\n",
    "    \n",
    "    losses = []\n",
    "    label = label.expand_as(predictions)\n",
    "    for i, q in enumerate(quantiles):\n",
    "        pred_q = predictions[..., i:i+1]\n",
    "        error = label - pred_q\n",
    "        valid = ~error.isnan()\n",
    "        loss = torch.max(q * error[valid], (q - 1) * error[valid])\n",
    "        print(loss)\n",
    "        losses.append((w1 * loss).mean(dim=dim,keepdim=True))\n",
    "    print(losses)\n",
    "    v = torch.stack(losses,dim=-1).mean(dim=-1)\n",
    "    return v\n",
    "\n",
    "loss(trainer.model.batch_data.y , **batch_output.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1\n",
      "\n",
      "Test 1: torch.unique\n",
      "Iter 0: Driver Allocated Memory: 10.73MB, Current Allocated Memory: 0.15MB\n",
      "Iter 10: Driver Allocated Memory: 10.73MB, Current Allocated Memory: 0.15MB\n",
      "Iter 20: Driver Allocated Memory: 10.73MB, Current Allocated Memory: 0.15MB\n",
      "Iter 30: Driver Allocated Memory: 10.72MB, Current Allocated Memory: 0.15MB\n",
      "Iter 40: Driver Allocated Memory: 10.72MB, Current Allocated Memory: 0.15MB\n",
      "Iter 50: Driver Allocated Memory: 10.72MB, Current Allocated Memory: 0.15MB\n",
      "Iter 60: Driver Allocated Memory: 10.72MB, Current Allocated Memory: 0.15MB\n",
      "Iter 70: Driver Allocated Memory: 10.72MB, Current Allocated Memory: 0.15MB\n",
      "Iter 80: Driver Allocated Memory: 10.72MB, Current Allocated Memory: 0.15MB\n",
      "Iter 90: Driver Allocated Memory: 10.72MB, Current Allocated Memory: 0.15MB\n",
      "\n",
      "Test 2: torch.sort\n",
      "Iter 0: Driver memory: 10.72MB, Current memory: 0.15MB\n",
      "Iter 10: Driver memory: 10.72MB, Current memory: 0.15MB\n",
      "Iter 20: Driver memory: 10.72MB, Current memory: 0.15MB\n",
      "Iter 30: Driver memory: 10.72MB, Current memory: 0.15MB\n",
      "Iter 40: Driver memory: 10.72MB, Current memory: 0.15MB\n",
      "Iter 50: Driver memory: 10.72MB, Current memory: 0.15MB\n",
      "Iter 60: Driver memory: 10.72MB, Current memory: 0.15MB\n",
      "Iter 70: Driver memory: 10.72MB, Current memory: 0.15MB\n",
      "Iter 80: Driver memory: 10.72MB, Current memory: 0.15MB\n",
      "Iter 90: Driver memory: 10.72MB, Current memory: 0.15MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "\n",
    "def test_operations(iterations: int, shape: tuple[int, int]) -> None:\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    # Test 1: torch.unique\n",
    "    print(\"\\nTest 1: torch.unique\")\n",
    "    x = torch.randint(0, 2, shape, device=\"mps\")\n",
    "    for i in range(iterations):\n",
    "        y = torch.unique(x)\n",
    "        del y\n",
    "\n",
    "        # Empty cache and collect garbage to make sure\n",
    "        torch.mps.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\n",
    "                f\"Iter {i}: Driver Allocated Memory: {torch.mps.driver_allocated_memory() / (1024**2):.2f}MB, Current Allocated Memory: {torch.mps.current_allocated_memory() / (1024**2):.2f}MB\"\n",
    "            )\n",
    "\n",
    "    # Test 2: torch.sort (comparison)\n",
    "    print(\"\\nTest 2: torch.sort\")\n",
    "    for i in range(iterations):\n",
    "        y = torch.sort(x)[0]\n",
    "        del y\n",
    "        # Empty cache and collect garbage to make sure\n",
    "        torch.mps.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\n",
    "                f\"Iter {i}: Driver memory: {torch.mps.driver_allocated_memory() / (1024**2):.2f}MB, Current memory: {torch.mps.current_allocated_memory() / (1024**2):.2f}MB\"\n",
    "            )\n",
    "\n",
    "\n",
    "test_operations(iterations=100, shape=(2000, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting lgbm_day_ShortTest in /Users/mengkjin/workspace/learndl/models\n"
     ]
    }
   ],
   "source": [
    "from src.api import ModelAPI\n",
    "ModelAPI.clear_st_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InstanceRecord(names=['trainer', 'account'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src import INSTANCE_RECORD\n",
    "INSTANCE_RECORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name is None, update all hidden models\n",
      "{'name': 'gru_day', 'nums': None, 'submodels': ['best']}\n",
      "Beware! Should be at server or short_test, but short_test is False now!\n",
      "try using d:\\Coding\\learndl\\learndl\\data\\DataSet/day.20240607.pt , success!\n",
      "Load  2 DataBlocks...... finished! Cost 0.08 secs\n",
      "Align 2 DataBlocks...... finished! Cost 0.21 secs\n",
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316/316 [00:00<00:00, 2933.69it/s]\n",
      "100%|██████████| 316/316 [00:00<00:00, 3047.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<src.model.model_module.application.extractor.ModelHiddenExtractor at 0x1bcbb3520d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.api import ModelAPI\n",
    "ModelAPI.Extractor.update()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.basic.INSTANCE_RECORD can be accessed to check ['trainer', 'account']\n",
      "Basic module imported!\n",
      "Use device name: NVIDIA GeForce RTX 4090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[44m24-12-15 18:40:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Sun Dec 15 18:40:32 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "**************************** RECONSTRUCT TRAIN DATA ****************************\n",
      "predict is False , Data Processing start!\n",
      "6 datas : ['y', 'day', '30m', 'style', 'indus', 'week'] , from 20070101 to None\n",
      "y is up to 20241215121914 already!\n",
      "day is up to 20241215122636 already!\n",
      "30m is up to 20241215122719 already!\n",
      "style is up to 20241215124444 already!\n",
      "indus is up to 20241215124618 already!\n",
      "week is up to 20241215124746 already!\n",
      "Data Processing Finished! Cost 0.00 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "******************************** UPDATE MODELS ********************************\n",
      "--------------------------------------------------------------------------------\n",
      "***************************** PARSER TRAINING ARGS *****************************\n",
      "--Process Queue : Data + Fit + Test\n",
      "--Confirm Resume Training!\n",
      "--Model_name is set to gru_day!\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "******************************* SETUP CALLBACKS *******************************\n",
      "ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True) , reset optimizer on some epoch (can speedup scheduler)\n",
      "CallbackTimer(verbosity=2) , record time cost of callback hooks\n",
      "EarlyStoppage(patience=20) , stop fitting when validation score cease to improve\n",
      "ValidationConverge(patience=5,eps=1e-05) , stop fitting when valid_score converge\n",
      "EarlyExitRetrain(earliest=10,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100, 1]) , retrain with new lr if fitting stopped too early\n",
      "NanLossRetrain(max_attempt=4) , retrain if fitting encounters nan loss\n",
      "BatchDisplay(verbosity=2) , display batch progress bar\n",
      "StatusDisplay(verbosity=2) , display epoch and event information\n",
      "DetailedAlphaAnalysis(use_num=avg)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "********************************** MODEL INFO **********************************\n",
      "Model Name   : gru_day\n",
      "Model Module : gru\n",
      "  -->  Model Params :\n",
      "    -->  hidden_dim : [32, 64]\n",
      "    -->  seqlens : [{'day': 30, '30m': 30, 'dms': 30}]\n",
      "    -->  tra_seqlens : [{'hist_loss': 40}]\n",
      "    -->  dropout : [0.1]\n",
      "    -->  enc_in : [True]\n",
      "    -->  enc_att : [False]\n",
      "    -->  rnn_type : ['lstm']\n",
      "    -->  rnn_att : [False]\n",
      "    -->  rnn_layers : [2]\n",
      "    -->  dec_mlp_layers : [2]\n",
      "    -->  num_output : [1]\n",
      "    -->  kernel_size : [3]\n",
      "    -->  hidden_as_factor : [False]\n",
      "    -->  ordered_param_group : [False]\n",
      "    -->  tra_num_states : [5]\n",
      "    -->  verbosity : 2\n",
      "Model Inputs : data\n",
      "  -->  Data Types : ['day']\n",
      "Model Labels : ['std_lag1_10']\n",
      "Model Period : 20170103 ~ 99991231\n",
      "Sampling     : sequential\n",
      "Shuffling    : epoch\n",
      "Random Seed  : None\n",
      "--------------------------------------------------------------------------------\n",
      "try using /home/mengkjin/Workspace/learndl/data/Interim/DataSet/day.20241128.pt , success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 3.7 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Sun Dec 15 18:40:36 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Fit], Cost 0.0 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Sun Dec 15 18:40:36 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-12-15 18:40:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mTesting Mean Score(spearman):\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 0.0 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:36|MOD:timer       |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 3.8 Seconds\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-12-15 18:40:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Sun Dec 15 18:40:36 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n",
      "--------------------------------------------------------------------------------\n",
      "***************************** PARSER TRAINING ARGS *****************************\n",
      "--Process Queue : Data + Fit + Test\n",
      "--Confirm Resume Training!\n",
      "--Model_name is set to gruRTN_day!\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "******************************* SETUP CALLBACKS *******************************\n",
      "ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True) , reset optimizer on some epoch (can speedup scheduler)\n",
      "CallbackTimer(verbosity=2) , record time cost of callback hooks\n",
      "EarlyStoppage(patience=20) , stop fitting when validation score cease to improve\n",
      "ValidationConverge(patience=5,eps=1e-05) , stop fitting when valid_score converge\n",
      "EarlyExitRetrain(earliest=10,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100, 1]) , retrain with new lr if fitting stopped too early\n",
      "NanLossRetrain(max_attempt=4) , retrain if fitting encounters nan loss\n",
      "BatchDisplay(verbosity=2) , display batch progress bar\n",
      "StatusDisplay(verbosity=2) , display epoch and event information\n",
      "DetailedAlphaAnalysis(use_num=avg)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "********************************** MODEL INFO **********************************\n",
      "Model Name   : gruRTN_day\n",
      "Model Module : gru\n",
      "  -->  Model Params :\n",
      "    -->  hidden_dim : [32, 64]\n",
      "    -->  seqlens : [{'day': 30, '30m': 30, 'dms': 30}]\n",
      "    -->  tra_seqlens : [{'hist_loss': 40}]\n",
      "    -->  dropout : [0.1]\n",
      "    -->  enc_in : [True]\n",
      "    -->  enc_att : [False]\n",
      "    -->  rnn_type : ['lstm']\n",
      "    -->  rnn_att : [False]\n",
      "    -->  rnn_layers : [2]\n",
      "    -->  dec_mlp_layers : [2]\n",
      "    -->  num_output : [1]\n",
      "    -->  kernel_size : [3]\n",
      "    -->  hidden_as_factor : [False]\n",
      "    -->  ordered_param_group : [False]\n",
      "    -->  tra_num_states : [5]\n",
      "    -->  verbosity : 2\n",
      "Model Inputs : data\n",
      "  -->  Data Types : ['day']\n",
      "Model Labels : ['rtn_lag1_10']\n",
      "Model Period : 20170103 ~ 99991231\n",
      "Sampling     : sequential\n",
      "Shuffling    : epoch\n",
      "Random Seed  : None\n",
      "--------------------------------------------------------------------------------\n",
      "try using /home/mengkjin/Workspace/learndl/data/Interim/DataSet/day.20241128.pt , success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:40|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 3.7 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:40|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Sun Dec 15 18:40:40 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:40|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Fit], Cost 0.0 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:40|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Sun Dec 15 18:40:40 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-12-15 18:40:40|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mTesting Mean Score(spearman):\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:40|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 0.0 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:40|MOD:timer       |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 3.7 Seconds\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-12-15 18:40:40|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:40|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Sun Dec 15 18:40:40 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n",
      "--------------------------------------------------------------------------------\n",
      "***************************** PARSER TRAINING ARGS *****************************\n",
      "--Process Queue : Data + Fit + Test\n",
      "--Confirm Resume Training!\n",
      "--Model_name is set to gru_avg!\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "******************************* SETUP CALLBACKS *******************************\n",
      "ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True) , reset optimizer on some epoch (can speedup scheduler)\n",
      "CallbackTimer(verbosity=2) , record time cost of callback hooks\n",
      "EarlyStoppage(patience=20) , stop fitting when validation score cease to improve\n",
      "ValidationConverge(patience=5,eps=1e-05) , stop fitting when valid_score converge\n",
      "EarlyExitRetrain(earliest=10,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100, 1]) , retrain with new lr if fitting stopped too early\n",
      "NanLossRetrain(max_attempt=4) , retrain if fitting encounters nan loss\n",
      "BatchDisplay(verbosity=2) , display batch progress bar\n",
      "StatusDisplay(verbosity=2) , display epoch and event information\n",
      "DetailedAlphaAnalysis(use_num=avg)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "********************************** MODEL INFO **********************************\n",
      "Model Name   : gru_avg\n",
      "Model Module : gru\n",
      "  -->  Model Params :\n",
      "    -->  hidden_dim : [32, 32, 32, 32, 32]\n",
      "    -->  seqlens : [{'day': 30, '30m': 30}]\n",
      "    -->  dropout : [0.1]\n",
      "    -->  enc_in : [True]\n",
      "    -->  enc_att : [False]\n",
      "    -->  rnn_type : ['gru']\n",
      "    -->  rnn_att : [False]\n",
      "    -->  rnn_layers : [2]\n",
      "    -->  dec_mlp_layers : [2]\n",
      "    -->  num_output : [1]\n",
      "    -->  which_output : [0]\n",
      "    -->  kernel_size : [3]\n",
      "    -->  hidden_as_factor : [True]\n",
      "    -->  ordered_param_group : [False]\n",
      "    -->  verbosity : 2\n",
      "Model Inputs : data\n",
      "  -->  Data Types : ['day']\n",
      "Model Labels : ['std_lag1_10']\n",
      "Model Period : 20170103 ~ 99991231\n",
      "Sampling     : sequential\n",
      "Shuffling    : epoch\n",
      "Random Seed  : None\n",
      "--------------------------------------------------------------------------------\n",
      "try using /home/mengkjin/Workspace/learndl/data/Interim/DataSet/day.20241128.pt , success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:44|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 3.7 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:44|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Sun Dec 15 18:40:44 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:44|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Fit], Cost 0.0 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:44|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Sun Dec 15 18:40:44 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-12-15 18:40:44|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mTesting Mean Score(spearman):\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:44|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 0.0 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:44|MOD:timer       |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 3.7 Seconds\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from src.api import ModelAPI\n",
    "\n",
    "ModelAPI.update_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
