{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lgbm': src.res.algo.boost.booster.lgbm.Lgbm,\n",
       " 'ada': src.res.algo.boost.booster.ada.AdaBoost,\n",
       " 'xgboost': src.res.algo.boost.booster.xgboost.XgBoost,\n",
       " 'catboost': src.res.algo.boost.booster.catboost.CatBoost,\n",
       " 'simple_lstm': src.res.algo.nn.model.Recurrent.simple_lstm,\n",
       " 'gru': src.res.algo.nn.model.Recurrent.gru,\n",
       " 'lstm': src.res.algo.nn.model.Recurrent.lstm,\n",
       " 'resnet_lstm': src.res.algo.nn.model.Recurrent.resnet_lstm,\n",
       " 'resnet_gru': src.res.algo.nn.model.Recurrent.resnet_gru,\n",
       " 'transformer': src.res.algo.nn.model.Recurrent.transformer,\n",
       " 'tcn': src.res.algo.nn.model.Recurrent.tcn,\n",
       " 'rnn_ntask': src.res.algo.nn.model.Recurrent.rnn_ntask,\n",
       " 'rnn_general': src.res.algo.nn.model.Recurrent.rnn_general,\n",
       " 'gru_dsize': src.res.algo.nn.model.Recurrent.gru_dsize,\n",
       " 'patch_tst': src.res.algo.nn.model.PatchTST.patch_tst,\n",
       " 'modern_tcn': src.res.algo.nn.model.ModernTCN.modern_tcn,\n",
       " 'ts_mixer': src.res.algo.nn.model.TSMixer.ts_mixer,\n",
       " 'tra': src.res.algo.nn.model.TRA.tra,\n",
       " 'factor_vae': src.res.algo.nn.model.FactorVAE.FactorVAE,\n",
       " 'risk_att_gru': src.res.algo.nn.model.RiskAttGRU.risk_att_gru,\n",
       " 'ple_gru': src.res.algo.nn.model.PLE.ple_gru,\n",
       " 'tft': src.res.algo.nn.model.TFT.TemporalFusionTransformer}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.api import ModelAPI  \n",
    "ModelAPI.Trainer.available_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.res.algo.nn.util'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelAPI  \n\u001b[32m      2\u001b[39m app = ModelAPI.train_model(module = \u001b[33m'\u001b[39m\u001b[33mgru\u001b[39m\u001b[33m'\u001b[39m , short_test = \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learndl/src/api/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelAPI\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfactor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FactorAPI\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataAPI\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learndl/src/api/model.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mres\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_module\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapplication\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mproj\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PATH , MACHINE , Logger , Proj\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataPreProcessor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learndl/src/res/__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m'''import all modules , order matters'''\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m algo , deap\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m factor\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learndl/src/res/algo/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AlgoModule\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learndl/src/res/algo/api.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any , Literal\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mproj\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PATH\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_nn_module , get_nn_category , get_nn_datatype , AVAILABLE_NNS\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mboost\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AVAILABLE_BOOSTERS , OptunaBooster , GeneralBooster\n\u001b[32m      9\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33mAlgoModule\u001b[39m\u001b[33m'\u001b[39m , \u001b[33m'\u001b[39m\u001b[33mget_nn_module\u001b[39m\u001b[33m'\u001b[39m , \u001b[33m'\u001b[39m\u001b[33mget_nn_category\u001b[39m\u001b[33m'\u001b[39m , \u001b[33m'\u001b[39m\u001b[33mget_nn_datatype\u001b[39m\u001b[33m'\u001b[39m , \u001b[33m'\u001b[39m\u001b[33mOptunaBooster\u001b[39m\u001b[33m'\u001b[39m , \u001b[33m'\u001b[39m\u001b[33mGeneralBooster\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learndl/src/res/algo/nn/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m api\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learndl/src/res/algo/nn/api.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layer \u001b[38;5;28;01mas\u001b[39;00m Layer\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m model \u001b[38;5;28;01mas\u001b[39;00m Model\n\u001b[32m      6\u001b[39m __all__ = [\n\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mAVAILABLE_NNS\u001b[39m\u001b[33m'\u001b[39m , \u001b[33m'\u001b[39m\u001b[33mget_nn_module\u001b[39m\u001b[33m'\u001b[39m , \u001b[33m'\u001b[39m\u001b[33mget_nn_category\u001b[39m\u001b[33m'\u001b[39m , \u001b[33m'\u001b[39m\u001b[33mget_nn_datatype\u001b[39m\u001b[33m'\u001b[39m , \n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mLayer\u001b[39m\u001b[33m'\u001b[39m , \u001b[33m'\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     10\u001b[39m AVAILABLE_NNS = {\n\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msimple_lstm\u001b[39m\u001b[33m'\u001b[39m       : Model.Recurrent.simple_lstm,\n\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mgru\u001b[39m\u001b[33m'\u001b[39m               : Model.Recurrent.gru, \n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtft\u001b[39m\u001b[33m'\u001b[39m               : Model.TFT.TemporalFusionTransformer\n\u001b[32m     29\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learndl/src/res/algo/nn/model/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     CNN, TFT , ModernTCN , PatchTST , PLE , Recurrent , RiskAttGRU , \n\u001b[32m      3\u001b[39m     TSMixer , TRA , FactorVAE ,\n\u001b[32m      4\u001b[39m     Attention ,\n\u001b[32m      5\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learndl/src/res/algo/nn/model/PLE.py:5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layer \u001b[38;5;28;01mas\u001b[39;00m Layer\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiHeadLosses\n\u001b[32m      7\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33mple_gru\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mple_gru\u001b[39;00m(nn.Module):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src.res.algo.nn.util'"
     ]
    }
   ],
   "source": [
    "from src.api import ModelAPI  \n",
    "app = ModelAPI.train_model(module = 'gru' , short_test = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NNPredictor',\n",
       " 'BasePredictorModel',\n",
       " 'ModelStreamLineWithTrainer',\n",
       " 'ModelStreamLine',\n",
       " 'ABC',\n",
       " 'object']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.proj import Proj\n",
    "[cls.__qualname__ for cls in Proj.States.trainer.model.__class__.__mro__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'None of [None] are in the columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m/var/folders/vh/4jxc0z614tx3bmgdb5v1tqn80000gr/T/ipykernel_80399/1846452064.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m self = Proj.States.trainer.metrics.model_metrics\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m dfs = {}\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;28;01min\u001b[39;00m self.metric_names:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     dfs[name] = self.get_table(name)\n\u001b[32m      7\u001b[39m dfs\n",
      "\u001b[32m~/workspace/learndl/src/res/model/util/metrics.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    426\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self.tables[name]:\n\u001b[32m    427\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame()\n\u001b[32m    428\u001b[39m         df = pd.concat(self.tables[name])\n\u001b[32m    429\u001b[39m         cols = [*self.key.keys() , *df.index.names]\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m df.assign(**self.key).reset_index(drop = \u001b[38;5;28;01mFalse\u001b[39;00m).set_index(cols)\n",
      "\u001b[32m~/workspace/learndl/.venv/lib/python3.12/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, keys, drop, append, inplace, verify_integrity)\u001b[39m\n\u001b[32m   6125\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m found:\n\u001b[32m   6126\u001b[39m                         missing.append(col)\n\u001b[32m   6127\u001b[39m \n\u001b[32m   6128\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[32m-> \u001b[39m\u001b[32m6129\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(f\"None of {missing} are in the columns\")\n\u001b[32m   6130\u001b[39m \n\u001b[32m   6131\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   6132\u001b[39m             frame = self\n",
      "\u001b[31mKeyError\u001b[39m: 'None of [None] are in the columns'"
     ]
    }
   ],
   "source": [
    "from src.proj import Proj\n",
    "self = Proj.States.trainer.metrics.model_metrics\n",
    "\n",
    "dfs = {}\n",
    "for name in self.metric_names:\n",
    "    dfs[name] = self.get_table(name)\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Empty DataFrame\n",
       " Columns: []\n",
       " Index: []]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.tables['test_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2509, 0.2509, 0.2509,  ..., 0.6996, 0.6996, 0.6996], device='mps:0')\n",
      "tensor([0.0764, 0.0764, 0.0764,  ..., 0.2467, 0.2467, 0.2467], device='mps:0')\n",
      "tensor([0.0659, 0.0659, 0.0659,  ..., 0.1139, 0.1139, 0.1139], device='mps:0')\n",
      "[tensor([0.4907], device='mps:0'), tensor([0.3703], device='mps:0'), tensor([0.2238], device='mps:0')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.3616], device='mps:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def loss(label : torch.Tensor , pred : torch.Tensor | None = None , w : torch.Tensor | None = None , dim = None , \n",
    "         quantiles : list[float] = [0.1,0.5,0.9] , predictions : torch.Tensor | None = None , **kwargs):\n",
    "    assert predictions is not None , f'predictions should be provided'\n",
    "    assert predictions.shape[-1] == len(quantiles) , f'shape of predictions {predictions.shape} should be (...,{len(quantiles)})'\n",
    "    if predictions.ndim == label.ndim + 1: \n",
    "        predictions = predictions.squeeze(-2)\n",
    "    assert predictions.ndim == label.ndim == 2 , f'shape of predictions {predictions.shape} and label {label.shape} should be (...,1)'\n",
    "    if w is None:\n",
    "        w1 = 1.\n",
    "    else:\n",
    "        w1 = w / w.sum(dim=dim,keepdim=True) * (w.numel() if dim is None else w.size(dim=dim))\n",
    "    \n",
    "    losses = []\n",
    "    label = label.expand_as(predictions)\n",
    "    for i, q in enumerate(quantiles):\n",
    "        pred_q = predictions[..., i:i+1]\n",
    "        error = label - pred_q\n",
    "        valid = ~error.isnan()\n",
    "        loss = torch.max(q * error[valid], (q - 1) * error[valid])\n",
    "        print(loss)\n",
    "        losses.append((w1 * loss).mean(dim=dim,keepdim=True))\n",
    "    print(losses)\n",
    "    v = torch.stack(losses,dim=-1).mean(dim=-1)\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1\n",
      "\n",
      "Test 1: torch.unique\n",
      "Iter 0: Driver Allocated Memory: 10.73MB, Current Allocated Memory: 0.15MB\n",
      "Iter 10: Driver Allocated Memory: 10.73MB, Current Allocated Memory: 0.15MB\n",
      "Iter 20: Driver Allocated Memory: 10.73MB, Current Allocated Memory: 0.15MB\n",
      "Iter 30: Driver Allocated Memory: 10.72MB, Current Allocated Memory: 0.15MB\n",
      "Iter 40: Driver Allocated Memory: 10.72MB, Current Allocated Memory: 0.15MB\n",
      "Iter 50: Driver Allocated Memory: 10.72MB, Current Allocated Memory: 0.15MB\n",
      "Iter 60: Driver Allocated Memory: 10.72MB, Current Allocated Memory: 0.15MB\n",
      "Iter 70: Driver Allocated Memory: 10.72MB, Current Allocated Memory: 0.15MB\n",
      "Iter 80: Driver Allocated Memory: 10.72MB, Current Allocated Memory: 0.15MB\n",
      "Iter 90: Driver Allocated Memory: 10.72MB, Current Allocated Memory: 0.15MB\n",
      "\n",
      "Test 2: torch.sort\n",
      "Iter 0: Driver memory: 10.72MB, Current memory: 0.15MB\n",
      "Iter 10: Driver memory: 10.72MB, Current memory: 0.15MB\n",
      "Iter 20: Driver memory: 10.72MB, Current memory: 0.15MB\n",
      "Iter 30: Driver memory: 10.72MB, Current memory: 0.15MB\n",
      "Iter 40: Driver memory: 10.72MB, Current memory: 0.15MB\n",
      "Iter 50: Driver memory: 10.72MB, Current memory: 0.15MB\n",
      "Iter 60: Driver memory: 10.72MB, Current memory: 0.15MB\n",
      "Iter 70: Driver memory: 10.72MB, Current memory: 0.15MB\n",
      "Iter 80: Driver memory: 10.72MB, Current memory: 0.15MB\n",
      "Iter 90: Driver memory: 10.72MB, Current memory: 0.15MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "\n",
    "def test_operations(iterations: int, shape: tuple[int, int]) -> None:\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    # Test 1: torch.unique\n",
    "    print(\"\\nTest 1: torch.unique\")\n",
    "    x = torch.randint(0, 2, shape, device=\"mps\")\n",
    "    for i in range(iterations):\n",
    "        y = torch.unique(x)\n",
    "        del y\n",
    "\n",
    "        # Empty cache and collect garbage to make sure\n",
    "        torch.mps.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\n",
    "                f\"Iter {i}: Driver Allocated Memory: {torch.mps.driver_allocated_memory() / (1024**2):.2f}MB, Current Allocated Memory: {torch.mps.current_allocated_memory() / (1024**2):.2f}MB\"\n",
    "            )\n",
    "\n",
    "    # Test 2: torch.sort (comparison)\n",
    "    print(\"\\nTest 2: torch.sort\")\n",
    "    for i in range(iterations):\n",
    "        y = torch.sort(x)[0]\n",
    "        del y\n",
    "        # Empty cache and collect garbage to make sure\n",
    "        torch.mps.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\n",
    "                f\"Iter {i}: Driver memory: {torch.mps.driver_allocated_memory() / (1024**2):.2f}MB, Current memory: {torch.mps.current_allocated_memory() / (1024**2):.2f}MB\"\n",
    "            )\n",
    "\n",
    "\n",
    "test_operations(iterations=100, shape=(2000, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting lgbm_day_ShortTest in /Users/mengkjin/workspace/learndl/models\n"
     ]
    }
   ],
   "source": [
    "from src.api import ModelAPI\n",
    "ModelAPI.clear_st_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InstanceRecord(names=['trainer', 'account'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.proj import Proj\n",
    "Proj.States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name is None, update all hidden models\n",
      "{'name': 'gru_day', 'nums': None, 'submodels': ['best']}\n",
      "Beware! Should be at server or short_test, but short_test is False now!\n",
      "try using d:\\Coding\\learndl\\learndl\\data\\DataSet/day.20240607.pt , success!\n",
      "Load  2 DataBlocks...... finished! Cost 0.08 secs\n",
      "Align 2 DataBlocks...... finished! Cost 0.21 secs\n",
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316/316 [00:00<00:00, 2933.69it/s]\n",
      "100%|██████████| 316/316 [00:00<00:00, 3047.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<src.model.model_module.application.extractor.ModelHiddenExtractor at 0x1bcbb3520d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.api import ModelAPI\n",
    "ModelAPI.Extractor.update()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.basic.INSTANCE_RECORD can be accessed to check ['trainer', 'account']\n",
      "Basic module imported!\n",
      "Use device name: NVIDIA GeForce RTX 4090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[44m24-12-15 18:40:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Sun Dec 15 18:40:32 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "**************************** RECONSTRUCT TRAIN DATA ****************************\n",
      "predict is False , Data Processing start!\n",
      "6 datas : ['y', 'day', '30m', 'style', 'indus', 'week'] , from 20070101 to None\n",
      "y is up to 20241215121914 already!\n",
      "day is up to 20241215122636 already!\n",
      "30m is up to 20241215122719 already!\n",
      "style is up to 20241215124444 already!\n",
      "indus is up to 20241215124618 already!\n",
      "week is up to 20241215124746 already!\n",
      "Data Processing Finished! Cost 0.00 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "******************************** UPDATE MODELS ********************************\n",
      "--------------------------------------------------------------------------------\n",
      "***************************** PARSER TRAINING ARGS *****************************\n",
      "--Process Queue : Data + Fit + Test\n",
      "--Confirm Resume Training!\n",
      "--Model_name is set to gru_day!\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "******************************* SETUP CALLBACKS *******************************\n",
      "ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True) , reset optimizer on some epoch (can speedup scheduler)\n",
      "CallbackTimer(verbosity=2) , record time cost of callback hooks\n",
      "EarlyStoppage(patience=20) , stop fitting when validation score cease to improve\n",
      "ValidationConverge(patience=5,eps=1e-05) , stop fitting when valid_score converge\n",
      "EarlyExitRetrain(earliest=10,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100, 1]) , retrain with new lr if fitting stopped too early\n",
      "NanLossRetrain(max_attempt=4) , retrain if fitting encounters nan loss\n",
      "BatchDisplay(verbosity=2) , display batch progress bar\n",
      "StatusDisplay(verbosity=2) , display epoch and event information\n",
      "DetailedAlphaAnalysis(use_num=avg)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "********************************** MODEL INFO **********************************\n",
      "Model Name   : gru_day\n",
      "Model Module : gru\n",
      "  -->  Model Params :\n",
      "    -->  hidden_dim : [32, 64]\n",
      "    -->  seqlens : [{'day': 30, '30m': 30, 'dms': 30}]\n",
      "    -->  tra_seqlens : [{'hist_loss': 40}]\n",
      "    -->  dropout : [0.1]\n",
      "    -->  enc_in : [True]\n",
      "    -->  enc_att : [False]\n",
      "    -->  rnn_type : ['lstm']\n",
      "    -->  rnn_att : [False]\n",
      "    -->  rnn_layers : [2]\n",
      "    -->  dec_mlp_layers : [2]\n",
      "    -->  num_output : [1]\n",
      "    -->  kernel_size : [3]\n",
      "    -->  hidden_as_factor : [False]\n",
      "    -->  ordered_param_group : [False]\n",
      "    -->  tra_num_states : [5]\n",
      "    -->  verbosity : 2\n",
      "Model Inputs : data\n",
      "  -->  Data Types : ['day']\n",
      "Model Labels : ['std_lag1_10']\n",
      "Model Period : 20170103 ~ 99991231\n",
      "Sampling     : sequential\n",
      "Shuffling    : epoch\n",
      "Random Seed  : None\n",
      "--------------------------------------------------------------------------------\n",
      "try using /home/mengkjin/Workspace/learndl/data/Interim/DataSet/day.20241128.pt , success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 3.7 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Sun Dec 15 18:40:36 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Fit], Cost 0.0 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Sun Dec 15 18:40:36 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-12-15 18:40:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mTesting Mean Score(spearman):\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 0.0 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:36|MOD:timer       |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 3.8 Seconds\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-12-15 18:40:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Sun Dec 15 18:40:36 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n",
      "--------------------------------------------------------------------------------\n",
      "***************************** PARSER TRAINING ARGS *****************************\n",
      "--Process Queue : Data + Fit + Test\n",
      "--Confirm Resume Training!\n",
      "--Model_name is set to gruRTN_day!\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "******************************* SETUP CALLBACKS *******************************\n",
      "ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True) , reset optimizer on some epoch (can speedup scheduler)\n",
      "CallbackTimer(verbosity=2) , record time cost of callback hooks\n",
      "EarlyStoppage(patience=20) , stop fitting when validation score cease to improve\n",
      "ValidationConverge(patience=5,eps=1e-05) , stop fitting when valid_score converge\n",
      "EarlyExitRetrain(earliest=10,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100, 1]) , retrain with new lr if fitting stopped too early\n",
      "NanLossRetrain(max_attempt=4) , retrain if fitting encounters nan loss\n",
      "BatchDisplay(verbosity=2) , display batch progress bar\n",
      "StatusDisplay(verbosity=2) , display epoch and event information\n",
      "DetailedAlphaAnalysis(use_num=avg)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "********************************** MODEL INFO **********************************\n",
      "Model Name   : gruRTN_day\n",
      "Model Module : gru\n",
      "  -->  Model Params :\n",
      "    -->  hidden_dim : [32, 64]\n",
      "    -->  seqlens : [{'day': 30, '30m': 30, 'dms': 30}]\n",
      "    -->  tra_seqlens : [{'hist_loss': 40}]\n",
      "    -->  dropout : [0.1]\n",
      "    -->  enc_in : [True]\n",
      "    -->  enc_att : [False]\n",
      "    -->  rnn_type : ['lstm']\n",
      "    -->  rnn_att : [False]\n",
      "    -->  rnn_layers : [2]\n",
      "    -->  dec_mlp_layers : [2]\n",
      "    -->  num_output : [1]\n",
      "    -->  kernel_size : [3]\n",
      "    -->  hidden_as_factor : [False]\n",
      "    -->  ordered_param_group : [False]\n",
      "    -->  tra_num_states : [5]\n",
      "    -->  verbosity : 2\n",
      "Model Inputs : data\n",
      "  -->  Data Types : ['day']\n",
      "Model Labels : ['rtn_lag1_10']\n",
      "Model Period : 20170103 ~ 99991231\n",
      "Sampling     : sequential\n",
      "Shuffling    : epoch\n",
      "Random Seed  : None\n",
      "--------------------------------------------------------------------------------\n",
      "try using /home/mengkjin/Workspace/learndl/data/Interim/DataSet/day.20241128.pt , success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:40|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 3.7 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:40|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Sun Dec 15 18:40:40 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:40|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Fit], Cost 0.0 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:40|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Sun Dec 15 18:40:40 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-12-15 18:40:40|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mTesting Mean Score(spearman):\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:40|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 0.0 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:40|MOD:timer       |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 3.7 Seconds\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-12-15 18:40:40|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:40|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Sun Dec 15 18:40:40 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n",
      "--------------------------------------------------------------------------------\n",
      "***************************** PARSER TRAINING ARGS *****************************\n",
      "--Process Queue : Data + Fit + Test\n",
      "--Confirm Resume Training!\n",
      "--Model_name is set to gru_avg!\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "******************************* SETUP CALLBACKS *******************************\n",
      "ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True) , reset optimizer on some epoch (can speedup scheduler)\n",
      "CallbackTimer(verbosity=2) , record time cost of callback hooks\n",
      "EarlyStoppage(patience=20) , stop fitting when validation score cease to improve\n",
      "ValidationConverge(patience=5,eps=1e-05) , stop fitting when valid_score converge\n",
      "EarlyExitRetrain(earliest=10,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100, 1]) , retrain with new lr if fitting stopped too early\n",
      "NanLossRetrain(max_attempt=4) , retrain if fitting encounters nan loss\n",
      "BatchDisplay(verbosity=2) , display batch progress bar\n",
      "StatusDisplay(verbosity=2) , display epoch and event information\n",
      "DetailedAlphaAnalysis(use_num=avg)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "********************************** MODEL INFO **********************************\n",
      "Model Name   : gru_avg\n",
      "Model Module : gru\n",
      "  -->  Model Params :\n",
      "    -->  hidden_dim : [32, 32, 32, 32, 32]\n",
      "    -->  seqlens : [{'day': 30, '30m': 30}]\n",
      "    -->  dropout : [0.1]\n",
      "    -->  enc_in : [True]\n",
      "    -->  enc_att : [False]\n",
      "    -->  rnn_type : ['gru']\n",
      "    -->  rnn_att : [False]\n",
      "    -->  rnn_layers : [2]\n",
      "    -->  dec_mlp_layers : [2]\n",
      "    -->  num_output : [1]\n",
      "    -->  which_output : [0]\n",
      "    -->  kernel_size : [3]\n",
      "    -->  hidden_as_factor : [True]\n",
      "    -->  ordered_param_group : [False]\n",
      "    -->  verbosity : 2\n",
      "Model Inputs : data\n",
      "  -->  Data Types : ['day']\n",
      "Model Labels : ['std_lag1_10']\n",
      "Model Period : 20170103 ~ 99991231\n",
      "Sampling     : sequential\n",
      "Shuffling    : epoch\n",
      "Random Seed  : None\n",
      "--------------------------------------------------------------------------------\n",
      "try using /home/mengkjin/Workspace/learndl/data/Interim/DataSet/day.20241128.pt , success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:44|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 3.7 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:44|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Sun Dec 15 18:40:44 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:44|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Fit], Cost 0.0 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:44|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Sun Dec 15 18:40:44 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-12-15 18:40:44|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mTesting Mean Score(spearman):\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:44|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 0.0 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-12-15 18:40:44|MOD:timer       |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 3.7 Seconds\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from src.api import ModelAPI\n",
    "\n",
    "ModelAPI.update_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
