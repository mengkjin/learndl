{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchData:\n",
      "x : (torch.Size([5171, 30, 6]), torch.Size([5171, 30, 10]))\n",
      "y : torch.Size([5171, 2])\n",
      "w : None\n",
      "i : torch.Size([5171, 2])\n",
      "valid : torch.Size([5171])\n"
     ]
    }
   ],
   "source": [
    "# get data first\n",
    "from src.res.model.util import BatchInput , BatchData\n",
    "batch_input = BatchInput.generate('day+style' , 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([5171, 30, 16])\n",
      "enc_in shape: torch.Size([5171, 30, 64])\n",
      "rnn outpur shape: torch.Size([5171, 30, 128])\n",
      "last rnn output shape: torch.Size([5171, 128])\n",
      "second last rnn output shape: torch.Size([5171, 128])\n",
      "alphas shape: torch.Size([5171, 60])\n",
      "betas shape: torch.Size([5171, 10])\n",
      "pred shape: torch.Size([5171, 1])\n",
      "BatchOutput:\n",
      "pred : torch.Size([5171, 1])\n",
      "alphas : torch.Size([5171, 60])\n",
      "betas : torch.Size([5171, 10])\n",
      "betas_1 : torch.Size([5171, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from src.res.algo.nn import layer as Layer\n",
    "from src.res.model.util import BatchOutput\n",
    "\n",
    "class mod_gru(nn.Module):\n",
    "    def __init__(self , input_dim , output_dim , dropout=0.0 , num_layers = 2):\n",
    "        super().__init__()\n",
    "        num_layers = min(3,num_layers)\n",
    "        self.gru = nn.GRU(input_dim , output_dim , num_layers = num_layers , dropout = dropout , batch_first = True)\n",
    "    def forward(self, x : Tensor) -> Tensor:\n",
    "        return self.gru(x)[0]\n",
    "\n",
    "class Astgnn(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim = 128,dropout = 0.1,rnn_layers = 2,enc_in=None,enc_in_dim=64,\n",
    "                 act_type='leaky',dec_mlp_layers=2,dec_mlp_dim=128,\n",
    "                 alpha_num = 60 , beta_num = 10 ,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.fc_enc_in = nn.Sequential(nn.Linear(input_dim, enc_in_dim),nn.Tanh())\n",
    "\n",
    "        rnn_kwargs = {'input_dim':enc_in_dim,'output_dim':hidden_dim,'num_layers':rnn_layers, 'dropout':dropout}\n",
    "        self.fc_rnn = mod_gru(**rnn_kwargs)\n",
    "\n",
    "        self.alpha_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim , alpha_num), \n",
    "            Layer.Act.get_activation_fn(act_type), \n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.beta_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim , beta_num), \n",
    "            Layer.Act.get_activation_fn(act_type), \n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.alpha_map_out = Layer.EwLinear()\n",
    "    def forward(self, input : Tensor | tuple[Tensor,...] | list[Tensor]):\n",
    "        '''\n",
    "        in: [bs x seq_len x input_dim]\n",
    "        out:[bs x hidden_dim]\n",
    "        '''\n",
    "        x = input if isinstance(input , Tensor) else torch.concat(input , dim = -1) \n",
    "        print(f'input shape: {x.shape}')\n",
    "        x = self.fc_enc_in(x)\n",
    "        print(f'enc_in shape: {x.shape}')\n",
    "        x = self.fc_rnn(x)\n",
    "        print(f'rnn outpur shape: {x.shape}')\n",
    "        x , x_1 = x[:,-1] , x[: , -2]\n",
    "        print(f'last rnn output shape: {x.shape}')\n",
    "        print(f'second last rnn output shape: {x_1.shape}')\n",
    "  \n",
    "        alphas = self.alpha_net(x)\n",
    "        print(f'alphas shape: {alphas.shape}')\n",
    "        betas = self.beta_net(x)\n",
    "        print(f'betas shape: {betas.shape}')\n",
    "        betas_1 = self.beta_net(x_1)\n",
    "\n",
    "        pred = self.alpha_map_out(alphas) \n",
    "        print(f'pred shape: {pred.shape}')\n",
    "        return pred , {'alphas':alphas , 'betas':betas , 'betas_1':betas_1}\n",
    "\n",
    "class AstgnnLoss(nn.Module):\n",
    "    def __init__(self, lamb : float = 0.1):\n",
    "        super().__init__()\n",
    "        self.lamb = lamb\n",
    "\n",
    "    def forward(self, pred , labels , alphas , betas , betas_1 , **kwargs):\n",
    "        assert labels.shape[-1] == 2 , labels.shape\n",
    "        mse = F.mse_loss(pred.squeeze() , labels[...,0].squeeze())\n",
    "        rsquare = self.rsquare_loss(alphas , labels[...,1])\n",
    "        corr = self.corr_loss(betas)\n",
    "        corr2 = self.corr_loss2(betas)\n",
    "        turnover = self.turnover_loss(betas , betas_1)\n",
    "        print('mse' , mse)\n",
    "        print('rsquare' , rsquare)\n",
    "        print('corr' , corr)\n",
    "        print('corr2' , corr2)\n",
    "        print('turnover' , turnover)\n",
    "        return mse + rsquare + self.lamb * corr + turnover\n",
    "\n",
    "    def rsquare_loss(self, hiddens : Tensor , label : Tensor , **kwargs):\n",
    "        assert hiddens.ndim == 2 , hiddens.shape\n",
    "        y_norm = label.norm()\n",
    "        pred = hiddens @ (hiddens.T @ hiddens).inverse() @ hiddens.T @ label\n",
    "        res_norm = (label - pred).norm()\n",
    "        return 1 - res_norm / y_norm\n",
    "\n",
    "    def corr_loss(self, hiddens : Tensor , **kwargs):\n",
    "        h = (hiddens - hiddens.mean(dim=0,keepdim=True)) / (hiddens.std(dim=0,keepdim=True) + 1e-6)\n",
    "        pen = h.T.cov().norm()\n",
    "        return pen\n",
    "\n",
    "    def corr_loss2(self, hiddens : Tensor , **kwargs):\n",
    "        h = (hiddens - hiddens.mean(dim=0,keepdim=True)) / (hiddens.std(dim=0,keepdim=True) + 1e-6)\n",
    "        pen = h.T.cov().square().mean().sqrt() * h.shape[-1]\n",
    "        return pen\n",
    "\n",
    "    def turnover_loss(self, betas : Tensor , betas_1 : Tensor , **kwargs):\n",
    "        return (betas - betas_1).norm()\n",
    "\n",
    "model = Astgnn(16)\n",
    "model = model.to(batch_input.device)\n",
    "batch_output = BatchOutput.from_module(model , batch_input)\n",
    "batch_data = BatchData(batch_input , batch_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse tensor(1.0023, device='mps:0', grad_fn=<MseLossBackward0>)\n",
      "rsquare tensor(0.0525, device='mps:0', grad_fn=<RsubBackward1>)\n",
      "corr tensor(3.6176, device='mps:0', grad_fn=<NormBackward1>)\n",
      "corr2 tensor(3.6176, device='mps:0', grad_fn=<MulBackward0>)\n",
      "turnover tensor(9.6108, device='mps:0', grad_fn=<NormBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(11.0274, device='mps:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loss = AstgnnLoss()\n",
    "model_loss(**batch_data.loss_inputs())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
