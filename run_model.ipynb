{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[44m24-02-08 06:55:39|MOD:run_model   |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-02-08 06:55:39|MOD:run_model   |\u001b[0m: \u001b[1m\u001b[31mStart Process [Load Data]!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Process Queue : Data + Instance\n",
      "--Confirm Resume Training!\n",
      "--Model_name is set to TRA_LSTM_day!\n",
      "{'verbosity': 2,\n",
      " 'storage_type': 'mem',\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'precision': 'float',\n",
      " 'batch_size': 10000,\n",
      " 'model_name': 'TRA_LSTM_day',\n",
      " 'model_module': 'TRA_LSTM',\n",
      " 'model_data_type': 'day',\n",
      " 'model_num': 1,\n",
      " 'beg_date': 20170103,\n",
      " 'end_date': 99991231,\n",
      " 'interval': 120,\n",
      " 'input_step_day': 5,\n",
      " 'test_step_day': 1,\n",
      " 'MODEL_PARAM': {'hidden_dim': [64],\n",
      "                 'seqlens': [{'day': 40, '15m': 20, 'dms': 40}],\n",
      "                 'tra_seqlens': [{'hist_loss': 40}],\n",
      "                 'rnn_layers': [2],\n",
      "                 'mlp_layers': [1],\n",
      "                 'dropout': [0.1],\n",
      "                 'fc_in': [True],\n",
      "                 'fc_att': [False],\n",
      "                 'type_rnn': ['lstm'],\n",
      "                 'rnn_att': [False],\n",
      "                 'num_output': [1],\n",
      "                 'kernel_size': [3, 3],\n",
      "                 'hidden_as_factor': [False],\n",
      "                 'ordered_param_group': [False],\n",
      "                 'tra_num_states': [3]},\n",
      " 'train_params': {'dataloader': {'random_seed': None, 'random_tv_split': True, 'sample_method': 'train_shuffle', 'train_ratio': 0.8},\n",
      "                  'trainer': {'optimizer': {'name': 'Adam', 'param': {}},\n",
      "                              'scheduler': {'name': 'cycle', 'param': {'base_lr': 1e-07, 'step_size_up': 4}},\n",
      "                              'learn_rate': {'base': 0.005,\n",
      "                                             'ratio': {'attempt': [1, 0.1, 10, 0.01, 100], 'round': [1.0], 'transfer': 0.1},\n",
      "                                             'reset': {'num_reset': 2, 'trigger': 40, 'recover_level': 1.0, 'speedup2x': True}},\n",
      "                              'nanloss': {'retry': 5},\n",
      "                              'gradient': {'clip_value': 10.0},\n",
      "                              'retrain': {'attempts': 4, 'min_epoch': 20, 'min_epoch_round': 10}},\n",
      "                  'criterion': {'loss': 'pearson',\n",
      "                                'score': {'train': 'pearson', 'valid': 'pearson', 'test': 'pearson'},\n",
      "                                'penalty': {'hidden_orthogonality': {'lamb': 0.001}, 'tra_ot_penalty': {'lamb': 0.01, 'rho': 0.999}},\n",
      "                                'weight': {'train': 'equal', 'test': 'equal'}},\n",
      "                  'transfer': False,\n",
      "                  'output_types': ['best', 'swalast', 'swabest'],\n",
      "                  'multitask': {'type': 'hybrid',\n",
      "                                'param_dict': {'dwa': {'tau': 2},\n",
      "                                               'ruw': {'phi': None},\n",
      "                                               'ewa': {},\n",
      "                                               'gls': {},\n",
      "                                               'rws': {},\n",
      "                                               'hybrid': {'phi': None, 'tau': 2}}},\n",
      "                  'terminate': {'overall': {'early_stop': 20, 'max_epoch': 200, 'valid_converge': {'min_epoch': 5, 'eps': 1e-05}},\n",
      "                                'round': {'early_stop': 10, 'max_epoch': 100, 'valid_converge': {'min_epoch': 5, 'eps': 1e-05}}}},\n",
      " 'compt_params': {'cuda_first': True, 'num_worker': 10}}\n",
      "use /home/mengkjin/Workspace/learndl/scripts/util/../../data/torch_pack/day+rtn11+res11.20231220.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-02-08 06:55:43|MOD:run_model   |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Load Data]! Cost 3.2Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-02-08 06:55:43|MOD:run_model   |\u001b[0m: \u001b[1m\u001b[31mStart Process [Copy to Instance]!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-02-08 06:55:43|MOD:run_model   |\u001b[0m: \u001b[1m\u001b[34mCopy from model to instance finished , Start going forward\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : [endpoint_division(True) , history_standardize(True)]\n",
      "score function of [pearson] calculated and success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m20170103     0.1458  0.1442  0.1438\u001b[0m\n",
      "\u001b[32m20170704     0.1285  0.1275  0.1259\u001b[0m\n",
      "\u001b[32m20171226     0.1450  0.1483  0.1458\u001b[0m\n",
      "\u001b[32m20180627     0.1281  0.1280  0.1259\u001b[0m\n",
      "\u001b[32m20181220     0.0968  0.0947  0.0962\u001b[0m\n",
      "\u001b[32m20190624     0.0945  0.0961 -0.0072\u001b[0m\n",
      "\u001b[32m20191217     0.0889  0.0884  0.0874\u001b[0m\n",
      "\u001b[32m20200617     0.0747  0.0755  0.0754\u001b[0m\n",
      "\u001b[32m20201214     0.0979  0.0989  0.0980\u001b[0m\n",
      "\u001b[32m20210615     0.0462  0.0471  0.0477\u001b[0m\n",
      "\u001b[32m20211209     0.0965  0.0943  0.0963\u001b[0m\n",
      "\u001b[32m20220613     0.0803  0.0825  0.0199\u001b[0m\n",
      "\u001b[32m20221206     0.0569  0.0598  0.0591\u001b[0m\n",
      "\u001b[32m20230606     0.0640  0.0675  0.0693\u001b[0m\n",
      "\u001b[32m20231201     0.0949  0.0932  0.0935\u001b[0m\n",
      "\u001b[32mAllTimeAvg   0.0960  0.0966  0.0847\u001b[0m\n",
      "\u001b[32mAllTimeSum   163.55  164.59  144.28\u001b[0m\n",
      "\u001b[32mStd          0.0772  0.0757  0.0816\u001b[0m\n",
      "\u001b[32mTValue        51.33   52.67   42.83\u001b[0m\n",
      "\u001b[32mAnnIR        6.0915  6.2503  5.0828\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-02-08 06:59:16|MOD:run_model   |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Copy to Instance]! Cost 213.0 Secs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%run run_model.py --process=3 --rawname=1 --resume=1 --anchoring=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--process PROCESS] [--rawname RAWNAME]\n",
      "                             [--resume RESUME] [--anchoring ANCHORING]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\jinmeng\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-2172NZnzfjqfsPP3.json\n",
      "\u001b[1m\u001b[37m\u001b[44m24-02-12 10:12:10|MOD:run_model   |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbosity': 2,\n",
      " 'storage_type': 'mem',\n",
      " 'device': device(type='cpu'),\n",
      " 'precision': 'float',\n",
      " 'batch_size': 10000,\n",
      " 'model_name': 'TRA_LSTM_day',\n",
      " 'model_module': 'TRA_LSTM',\n",
      " 'model_data_type': 'day',\n",
      " 'model_num': 1,\n",
      " 'beg_date': 20170103,\n",
      " 'end_date': 99991231,\n",
      " 'interval': 120,\n",
      " 'input_step_day': 5,\n",
      " 'test_step_day': 1,\n",
      " 'MODEL_PARAM': {'hidden_dim': [64],\n",
      "                 'seqlens': [{'day': 40, '30m': 30, '15m': 20, 'dms': 40}],\n",
      "                 'tra_seqlens': [{'hist_loss': 40}],\n",
      "                 'rnn_layers': [2],\n",
      "                 'mlp_layers': [1],\n",
      "                 'dropout': [0.1],\n",
      "                 'fc_in': [True],\n",
      "                 'fc_att': [False],\n",
      "                 'type_rnn': ['lstm'],\n",
      "                 'rnn_att': [False],\n",
      "                 'num_output': [1],\n",
      "                 'kernel_size': [3, 3],\n",
      "                 'hidden_as_factor': [False],\n",
      "                 'ordered_param_group': [False],\n",
      "                 'tra_num_states': [3]},\n",
      " 'train_params': {'dataloader': {'random_seed': None, 'random_tv_split': True, 'sample_method': 'train_shuffle', 'train_ratio': 0.8},\n",
      "                  'trainer': {'optimizer': {'name': 'Adam', 'param': {}},\n",
      "                              'scheduler': {'name': 'cycle', 'param': {'base_lr': 1e-07, 'step_size_up': 4}},\n",
      "                              'learn_rate': {'base': 0.005,\n",
      "                                             'ratio': {'attempt': [1, 0.1, 10, 0.01, 100], 'round': [1.0], 'transfer': 0.1},\n",
      "                                             'reset': {'num_reset': 2, 'trigger': 40, 'recover_level': 1.0, 'speedup2x': True}},\n",
      "                              'nanloss': {'retry': 5},\n",
      "                              'gradient': {'clip_value': 10.0},\n",
      "                              'retrain': {'attempts': 4, 'min_epoch': 20, 'min_epoch_round': 10}},\n",
      "                  'criterion': {'loss': 'pearson',\n",
      "                                'score': {'train': 'pearson', 'valid': 'pearson', 'test': 'pearson'},\n",
      "                                'penalty': {'hidden_orthogonality': {'lamb': 0.001}, 'tra_ot_penalty': {'lamb': 0.01, 'rho': 0.999}},\n",
      "                                'weight': {'train': 'equal', 'test': 'equal'}},\n",
      "                  'transfer': False,\n",
      "                  'output_types': ['best', 'swalast', 'swabest'],\n",
      "                  'multitask': {'type': 'hybrid',\n",
      "                                'param_dict': {'dwa': {'tau': 2},\n",
      "                                               'ruw': {'phi': None},\n",
      "                                               'ewa': {},\n",
      "                                               'gls': {},\n",
      "                                               'rws': {},\n",
      "                                               'hybrid': {'phi': None, 'tau': 2}}},\n",
      "                  'terminate': {'overall': {'early_stop': 20, 'max_epoch': 200, 'valid_converge': {'min_epoch': 5, 'eps': 1e-05}},\n",
      "                                'round': {'early_stop': 10, 'max_epoch': 100, 'valid_converge': {'min_epoch': 5, 'eps': 1e-05}}}},\n",
      " 'compt_params': {'cuda_first': True, 'num_worker': 10}}\n"
     ]
    }
   ],
   "source": [
    "from run_model import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = train_config(override_config = {'model_module' : 'ResNet_LSTM' , 'model_data_type' : '30m'} , do_process=False)\n",
    "for smp in config.model_params: smp.update(model_params_filler())\n",
    "config.get_dict(['model_module' , 'model_data_type' , 'model_params' ])\n",
    "\n",
    "data_type_list = config.data_type_list\n",
    "model_params = config.model_params\n",
    "def new_random_input(data_type_list , model_params):\n",
    "    smp = model_params[0]\n",
    "    batch = 2\n",
    "    x = {}\n",
    "    for i , mdt in enumerate(data_type_list):\n",
    "        seqlen = smp['seqlens'].get(mdt , 20)\n",
    "        inday_dim = (lambda x,i:x[i] if isinstance(x,(tuple,list)) else x)(smp.get('inday_dim',8) , i)\n",
    "        input_dim = (lambda x,i:x[i] if isinstance(x,(tuple,list)) else x)(smp.get('input_dim',6) , i)\n",
    "        x[mdt] = torch.rand(batch , seqlen , inday_dim , input_dim)\n",
    "    y = torch.rand(batch , 1)\n",
    "    return x , y\n",
    "x , y = new_random_input(data_type_list , model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 30, 8, 6]), torch.Size([2, 1]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['30m'].shape , y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "smp = model_params[0]\n",
    "batch = 2\n",
    "x = {}\n",
    "for i , mdt in enumerate(config.data_type_list):\n",
    "    seqlen = smp['seqlens'].get(mdt , 20)\n",
    "    inday_dim = smp['inday_dim'][i] if isinstance(smp['inday_dim'] , (tuple , list)) else smp['inday_dim']\n",
    "    input_dim = smp['input_dim'][i] if isinstance(smp['input_dim'] , (tuple , list)) else smp['input_dim']\n",
    "    x[mdt] = torch.rand(batch , seqlen , inday_dim , input_dim)\n",
    "y = torch.rand(batch , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 2\n",
    "x = {}\n",
    "for i , mdt in enumerate(config.data_type_list):\n",
    "    seqlen = smp['seqlens'].get(mdt , 20)\n",
    "    inday_dim = smp['inday_dim'][i] if isinstance(smp['inday_dim'] , (tuple , list)) else smp['inday_dim']\n",
    "    input_dim = smp['input_dim'][i] if isinstance(smp['input_dim'] , (tuple , list)) else smp['input_dim']\n",
    "    x[mdt] = torch.rand(batch , seqlen , inday_dim , input_dim)\n",
    "y = torch.rand(batch , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 30, 8, 6])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['30m'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden shape : torch.Size([2, 30, 16])\n",
      "output shape : torch.Size([2, 64])\n",
      "hidden shape : torch.Size([2, 30, 16])\n",
      "output shape : torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# pip install pytimedinput -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "from scripts.nn.My import mod_gru\n",
    "from scripts.nn.ResNet import resnet_1d, resnet_2d\n",
    "    \n",
    "class resnet1d_gru(nn.Module):\n",
    "    def __init__(self, seq_len , feat_len , dim_res = 16 , dim_rnn = 64 , *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.resnet = resnet_1d(seq_len , feat_len , dim_res , 10 , 3) \n",
    "        self.gru    = mod_gru(dim_res , dim_rnn , 0.1 , 2)\n",
    "    def forward(self , x):\n",
    "        hidden = self.resnet(x)\n",
    "        print(\"hidden shape :\" , hidden.shape)\n",
    "        output = self.gru(hidden)[:,-1,:]\n",
    "        print(\"output shape :\" , output.shape)\n",
    "        return output\n",
    "    \n",
    "class resnet2d_gru(nn.Module):\n",
    "    def __init__(self, seq_len , feat_len , dim_res = 16 , dim_rnn = 64 , *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.resnet = resnet_2d(seq_len , feat_len , dim_res , 10 , 3) \n",
    "        self.gru    = mod_gru(dim_res , dim_rnn , 0.1 , 2)\n",
    "    def forward(self , x):\n",
    "        hidden = self.resnet(x)\n",
    "        output = self.gru(hidden)[:,-1,:]\n",
    "        print(\"hidden shape :\" , hidden.shape)\n",
    "        print(\"output shape :\" , output.shape)\n",
    "        return output\n",
    "    \n",
    "\n",
    "batch_n  = 2\n",
    "seq_day  = 30\n",
    "seq_inday  = 8\n",
    "feat_len = 5\n",
    "dim_out  = 64\n",
    "x = torch.randn(batch_n,seq_day,seq_inday,feat_len)\n",
    "\n",
    "net_1d = resnet1d_gru(seq_inday , feat_len , dim_res = dim_out // 4 , dim_rnn = dim_out)\n",
    "net_2d = resnet2d_gru(seq_inday , feat_len , dim_res = dim_out // 4 , dim_rnn = dim_out)\n",
    "y1 = net_1d(x)\n",
    "y2 = net_2d(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
