{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[44m24-02-08 06:55:39|MOD:run_model   |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-02-08 06:55:39|MOD:run_model   |\u001b[0m: \u001b[1m\u001b[31mStart Process [Load Data]!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Process Queue : Data + Instance\n",
      "--Confirm Resume Training!\n",
      "--Model_name is set to TRA_LSTM_day!\n",
      "{'verbosity': 2,\n",
      " 'storage_type': 'mem',\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'precision': 'float',\n",
      " 'batch_size': 10000,\n",
      " 'model_name': 'TRA_LSTM_day',\n",
      " 'model_module': 'TRA_LSTM',\n",
      " 'model_data_type': 'day',\n",
      " 'model_num': 1,\n",
      " 'beg_date': 20170103,\n",
      " 'end_date': 99991231,\n",
      " 'interval': 120,\n",
      " 'input_step_day': 5,\n",
      " 'test_step_day': 1,\n",
      " 'MODEL_PARAM': {'hidden_dim': [64],\n",
      "                 'seqlens': [{'day': 40, '15m': 20, 'dms': 40}],\n",
      "                 'tra_seqlens': [{'hist_loss': 40}],\n",
      "                 'rnn_layers': [2],\n",
      "                 'mlp_layers': [1],\n",
      "                 'dropout': [0.1],\n",
      "                 'fc_in': [True],\n",
      "                 'fc_att': [False],\n",
      "                 'type_rnn': ['lstm'],\n",
      "                 'rnn_att': [False],\n",
      "                 'num_output': [1],\n",
      "                 'kernel_size': [3, 3],\n",
      "                 'hidden_as_factor': [False],\n",
      "                 'ordered_param_group': [False],\n",
      "                 'tra_num_states': [3]},\n",
      " 'train_params': {'dataloader': {'random_seed': None, 'random_tv_split': True, 'sample_method': 'train_shuffle', 'train_ratio': 0.8},\n",
      "                  'trainer': {'optimizer': {'name': 'Adam', 'param': {}},\n",
      "                              'scheduler': {'name': 'cycle', 'param': {'base_lr': 1e-07, 'step_size_up': 4}},\n",
      "                              'learn_rate': {'base': 0.005,\n",
      "                                             'ratio': {'attempt': [1, 0.1, 10, 0.01, 100], 'round': [1.0], 'transfer': 0.1},\n",
      "                                             'reset': {'num_reset': 2, 'trigger': 40, 'recover_level': 1.0, 'speedup2x': True}},\n",
      "                              'nanloss': {'retry': 5},\n",
      "                              'gradient': {'clip_value': 10.0},\n",
      "                              'retrain': {'attempts': 4, 'min_epoch': 20, 'min_epoch_round': 10}},\n",
      "                  'criterion': {'loss': 'pearson',\n",
      "                                'score': {'train': 'pearson', 'valid': 'pearson', 'test': 'pearson'},\n",
      "                                'penalty': {'hidden_orthogonality': {'lamb': 0.001}, 'tra_ot_penalty': {'lamb': 0.01, 'rho': 0.999}},\n",
      "                                'weight': {'train': 'equal', 'test': 'equal'}},\n",
      "                  'transfer': False,\n",
      "                  'output_types': ['best', 'swalast', 'swabest'],\n",
      "                  'multitask': {'type': 'hybrid',\n",
      "                                'param_dict': {'dwa': {'tau': 2},\n",
      "                                               'ruw': {'phi': None},\n",
      "                                               'ewa': {},\n",
      "                                               'gls': {},\n",
      "                                               'rws': {},\n",
      "                                               'hybrid': {'phi': None, 'tau': 2}}},\n",
      "                  'terminate': {'overall': {'early_stop': 20, 'max_epoch': 200, 'valid_converge': {'min_epoch': 5, 'eps': 1e-05}},\n",
      "                                'round': {'early_stop': 10, 'max_epoch': 100, 'valid_converge': {'min_epoch': 5, 'eps': 1e-05}}}},\n",
      " 'compt_params': {'cuda_first': True, 'num_worker': 10}}\n",
      "use /home/mengkjin/Workspace/learndl/scripts/util/../../data/torch_pack/day+rtn11+res11.20231220.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-02-08 06:55:43|MOD:run_model   |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Load Data]! Cost 3.2Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-02-08 06:55:43|MOD:run_model   |\u001b[0m: \u001b[1m\u001b[31mStart Process [Copy to Instance]!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-02-08 06:55:43|MOD:run_model   |\u001b[0m: \u001b[1m\u001b[34mCopy from model to instance finished , Start going forward\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : [endpoint_division(True) , history_standardize(True)]\n",
      "score function of [pearson] calculated and success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m20170103     0.1458  0.1442  0.1438\u001b[0m\n",
      "\u001b[32m20170704     0.1285  0.1275  0.1259\u001b[0m\n",
      "\u001b[32m20171226     0.1450  0.1483  0.1458\u001b[0m\n",
      "\u001b[32m20180627     0.1281  0.1280  0.1259\u001b[0m\n",
      "\u001b[32m20181220     0.0968  0.0947  0.0962\u001b[0m\n",
      "\u001b[32m20190624     0.0945  0.0961 -0.0072\u001b[0m\n",
      "\u001b[32m20191217     0.0889  0.0884  0.0874\u001b[0m\n",
      "\u001b[32m20200617     0.0747  0.0755  0.0754\u001b[0m\n",
      "\u001b[32m20201214     0.0979  0.0989  0.0980\u001b[0m\n",
      "\u001b[32m20210615     0.0462  0.0471  0.0477\u001b[0m\n",
      "\u001b[32m20211209     0.0965  0.0943  0.0963\u001b[0m\n",
      "\u001b[32m20220613     0.0803  0.0825  0.0199\u001b[0m\n",
      "\u001b[32m20221206     0.0569  0.0598  0.0591\u001b[0m\n",
      "\u001b[32m20230606     0.0640  0.0675  0.0693\u001b[0m\n",
      "\u001b[32m20231201     0.0949  0.0932  0.0935\u001b[0m\n",
      "\u001b[32mAllTimeAvg   0.0960  0.0966  0.0847\u001b[0m\n",
      "\u001b[32mAllTimeSum   163.55  164.59  144.28\u001b[0m\n",
      "\u001b[32mStd          0.0772  0.0757  0.0816\u001b[0m\n",
      "\u001b[32mTValue        51.33   52.67   42.83\u001b[0m\n",
      "\u001b[32mAnnIR        6.0915  6.2503  5.0828\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-02-08 06:59:16|MOD:run_model   |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Copy to Instance]! Cost 213.0 Secs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%run run_model.py --process=3 --rawname=1 --resume=1 --anchoring=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[-0.0191],\n",
       "          [-0.0260]], grad_fn=<SumBackward1>),\n",
       "  tensor([[-0.1066, -0.0298, -0.0047],\n",
       "          [-0.1008, -0.0258, -0.0168]], grad_fn=<AddmmBackward0>)),\n",
       " tensor([[0.8924],\n",
       "         [0.6569]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from run_model import * \n",
    "\n",
    "\"\"\"\n",
    "net , x , y = new_random_input('ResNet_LSTM' , '30m')\n",
    "net(x) , y\n",
    "\n",
    "net , x , y = new_random_input('TRA_LSTM' , 'day')\n",
    "# x = torch.concat(x.unbind(1),1)\n",
    "hist_loss = torch.rand((*x.shape[:-1],3))\n",
    "net((x , hist_loss)) , y\n",
    "\"\"\"\n",
    "net , x , y = new_random_input('TRA_LSTM' , 'day')\n",
    "# x = torch.concat(x.unbind(1),1)\n",
    "hist_loss = torch.rand((*x.shape[:-1],3))\n",
    "net((x , hist_loss)) , y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0803, 0.2445, 0.6752],\n",
       "         [0.0032, 0.9855, 0.0112]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0835, 1.2300, 0.6865]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.probs , net.probs_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-78de06a6da7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mhist_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'hist_loss' is not defined"
     ]
    }
   ],
   "source": [
    "x.shape , hist_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyTRA_LSTM(\n",
       "  (base_model): Sequential(\n",
       "    (0): mod_parallel(\n",
       "      (mod_list): ModuleList(\n",
       "        (0): uni_rnn_encoder(\n",
       "          (fc_enc_in): Sequential()\n",
       "          (fc_rnn): mod_lstm(\n",
       "            (lstm): LSTM(16, 64, num_layers=2, batch_first=True, dropout=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): mod_parallel(\n",
       "      (mod_list): ModuleList(\n",
       "        (0): uni_rnn_decoder(\n",
       "          (fc_dec_mlp): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (1): LeakyReLU(negative_slope=0.01)\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (fc_hid_out): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (router): LSTM(3, 8, batch_first=True)\n",
       "  (fc): Linear(in_features=72, out_features=3, bias=True)\n",
       "  (predictors): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 30, 8, 6])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unbind() received an invalid combination of arguments - got (tuple), but expected one of:\n * (int dim)\n      didn't match because some of the arguments have invalid types: (!tuple of (int, int)!)\n * (name dim)\n      didn't match because some of the arguments have invalid types: (!tuple of (int, int)!)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-79d7715aea30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unbind() received an invalid combination of arguments - got (tuple), but expected one of:\n * (int dim)\n      didn't match because some of the arguments have invalid types: (!tuple of (int, int)!)\n * (name dim)\n      didn't match because some of the arguments have invalid types: (!tuple of (int, int)!)\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.concat(x.unbind(1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = x.reshape(2,-1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 240, 6])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
