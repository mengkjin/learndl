{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-02-19 01:46:14|MOD:run_model   |\u001b[0m: \u001b[1m\u001b[31mStart Process [Load Data]!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Process Queue : Data + Train\n",
      "--Start Training New!\n",
      "--Model_name is set to tra_lstm2_day_ShortTest!\n",
      "use /home/mengkjin/Workspace/learndl/scripts/util/../../data/torch_pack/day+rtn11+res11.20231220.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-02-19 01:46:17|MOD:run_model   |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Load Data]! Cost 3.3Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-02-19 01:46:17|MOD:run_model   |\u001b[0m: \u001b[1m\u001b[31mStart Process [Train Model]!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : [endpoint_division(True) , history_standardize(True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mtra_lstm2_day_ShortTest #0 @20170103 LoadData Cost    2.3Secs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score function of [pearson] calculated and success!\n",
      "loss function of [pearson] calculated and success!\n",
      "penalty function of [hidden_orthogonality] calculated and success!\n",
      "penalty function of [tra_ot_penalty] calculated and success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mFirstBite Ep#  0 : loss  1.00249, train-0.00246, valid 0.00048, max 0.0005, best 0.0005, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.88438, train 0.12351, valid 0.05509, max 0.0551, best 0.0551, lr3.8e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.85125, train 0.16161, valid 0.06219, max 0.0622, best 0.0622, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.80899, train 0.21267, valid 0.08637, max 0.0864, best 0.0864, lr6.3e-04\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-02-19 01:46:34|MOD:run_model   |\u001b[0m: \u001b[1m\u001b[34mtra_lstm2_day_ShortTest #0 @20170103|Round 0 FirstBite Ep# 19 Max Epoch|Train 0.2216 Valid 0.0825 BestVal 0.0881|Cost  0.3Min,  0.7Sec/Ep\u001b[0m\n",
      "\u001b[32mtra_lstm2_day_ShortTest #0 @20170704 LoadData Cost    2.1Secs\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  0.99649, train 0.00356, valid 0.00404, max 0.0040, best 0.0040, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.88273, train 0.12482, valid 0.05928, max 0.0593, best 0.0593, lr3.8e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.84420, train 0.16967, valid 0.06660, max 0.0828, best 0.0828, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.81683, train 0.20265, valid 0.08681, max 0.0868, best 0.0868, lr6.3e-04\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-02-19 01:46:50|MOD:run_model   |\u001b[0m: \u001b[1m\u001b[34mtra_lstm2_day_ShortTest #0 @20170704|Round 0 FirstBite Ep# 19 Max Epoch|Train 0.2083 Valid 0.0869 BestVal 0.0872|Cost  0.3Min,  0.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-02-19 01:46:50|MOD:run_model   |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Train Model]! Cost 0.0 Hours, 0.3 Min/model, 0.8 Sec/Epoch\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%run run_model.py --process=1 --rawname=1 --resume=0 --anchoring=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengkjin/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use /home/mengkjin/Workspace/learndl/scripts/util/../../data/torch_pack/day+rtn11+res11.20231220.pt\n",
      "Pre-Norming method of [day] : [endpoint_division(True) , history_standardize(True)]\n"
     ]
    }
   ],
   "source": [
    "from run_model import RunModel\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "config , net , batch_data , model_data , metrics , multiloss = RunModel.new_random_input('simple_lstm' , 'day')\n",
    "optimizer = RunModel.new_optimizer(net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = deepcopy(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score function of [pearson] calculated and success!\n",
      "loss function of [pearson] calculated and success!\n",
      "{'loss': tensor(0.9808, grad_fn=<ExpBackward0>), 'loss_item': 0.9807661175727844, 'score': 0.01942128874361515, 'penalty': 0.0, 'losses': None}\n",
      "tensor([[ 0.0140, -0.0108,  0.0078, -0.0284, -0.0481,  0.0078, -0.0258,  0.0035,\n",
      "         -0.0371, -0.0073,  0.0234,  0.0281,  0.0275,  0.0296,  0.0173, -0.0114,\n",
      "          0.0221,  0.0060, -0.0328, -0.0356,  0.0031, -0.0325,  0.0141, -0.0161,\n",
      "          0.0009, -0.0024,  0.0167,  0.0265,  0.0198,  0.0362, -0.0010, -0.0116,\n",
      "         -0.0077, -0.0318, -0.0098,  0.0167, -0.0049, -0.0375,  0.0130, -0.0538,\n",
      "         -0.0527,  0.0242,  0.0462,  0.0368,  0.0090,  0.0316, -0.0234,  0.0166,\n",
      "         -0.0204, -0.0324,  0.0425,  0.0208, -0.0011,  0.0560, -0.0481,  0.0026,\n",
      "          0.0176,  0.0150, -0.0724, -0.0247, -0.0164,  0.0246,  0.0033, -0.0301]])\n"
     ]
    }
   ],
   "source": [
    "#pipeline 1\n",
    "\n",
    "sd = deepcopy(net.state_dict())\n",
    "optimizer = RunModel.new_optimizer(net)\n",
    "optimizer.zero_grad()\n",
    "if hasattr(net , 'dynamic_data_assign'): net.dynamic_data_assign(batch_data , model_data)\n",
    "pred , hidden = net(batch_data['x'])\n",
    "penalty_kwargs = {'net' : net , 'hidden' : hidden , 'label' : batch_data['y']}\n",
    "metric = RunModel.calculate_metrics('train' , metrics, label=batch_data['y'], pred=pred,\n",
    "                            weight=batch_data['w'], multiloss=multiloss,net=net,valid_sample=None,\n",
    "                            penalty_kwargs=penalty_kwargs)\n",
    "print(metric)\n",
    "(metric['loss'] + metric['penalty']).backward()\n",
    "print(net.get_parameter('fc.weight').grad)\n",
    "clip_value = config.train_params['trainer']['gradient'].get('clip_value')\n",
    "if clip_value is not None : nn.utils.clip_grad_value_(net.parameters(), clip_value = clip_value)\n",
    "optimizer.step()\n",
    "sd_step = deepcopy(net.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(0.9808, grad_fn=<ExpBackward0>), 'loss_item': 0.9807661175727844, 'score': 0.01942128874361515, 'penalty': 0.0, 'losses': None}\n",
      "tensor([[ 0.0140, -0.0108,  0.0078, -0.0284, -0.0481,  0.0078, -0.0258,  0.0035,\n",
      "         -0.0371, -0.0073,  0.0234,  0.0281,  0.0275,  0.0296,  0.0173, -0.0114,\n",
      "          0.0221,  0.0060, -0.0328, -0.0356,  0.0031, -0.0325,  0.0141, -0.0161,\n",
      "          0.0009, -0.0024,  0.0167,  0.0265,  0.0198,  0.0362, -0.0010, -0.0116,\n",
      "         -0.0077, -0.0318, -0.0098,  0.0167, -0.0049, -0.0375,  0.0130, -0.0538,\n",
      "         -0.0527,  0.0242,  0.0462,  0.0368,  0.0090,  0.0316, -0.0234,  0.0166,\n",
      "         -0.0204, -0.0324,  0.0425,  0.0208, -0.0011,  0.0560, -0.0481,  0.0026,\n",
      "          0.0176,  0.0150, -0.0724, -0.0247, -0.0164,  0.0246,  0.0033, -0.0301]])\n"
     ]
    }
   ],
   "source": [
    "#pipeline 2\n",
    "sd2 = deepcopy(net2.state_dict())\n",
    "optimizer2 = RunModel.new_optimizer(net2)\n",
    "if hasattr(net2 , 'dynamic_data_assign'): net2.dynamic_data_assign(batch_data , model_data)\n",
    "pred , hidden = net2(batch_data['x'])\n",
    "penalty_kwargs = {'net' : net2 , 'hidden' : hidden , 'label' : batch_data['y']}\n",
    "metric = RunModel.calculate_metrics('train' , metrics, label=batch_data['y'], pred=pred,\n",
    "                            weight=batch_data['w'], multiloss=multiloss,net=net2,valid_sample=None,\n",
    "                            penalty_kwargs=penalty_kwargs)\n",
    "optimizer2.zero_grad()\n",
    "print(metric)\n",
    "(metric['loss'] + metric['penalty']).backward()\n",
    "print(net2.get_parameter('fc.weight').grad)\n",
    "clip_value = config.train_params['trainer']['gradient'].get('clip_value')\n",
    "if clip_value is not None : nn.utils.clip_grad_value_(net2.parameters(), clip_value = clip_value)\n",
    "optimizer2.step()\n",
    "sd2_step = deepcopy(net2.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm.weight_ih_l0 lstm.weight_ih_l0 tensor(True)\n",
      "lstm.weight_hh_l0 lstm.weight_hh_l0 tensor(True)\n",
      "lstm.bias_ih_l0 lstm.bias_ih_l0 tensor(True)\n",
      "lstm.bias_hh_l0 lstm.bias_hh_l0 tensor(True)\n",
      "fc.weight fc.weight tensor(True)\n",
      "fc.bias fc.bias tensor(True)\n"
     ]
    }
   ],
   "source": [
    "for (k,v),(k2,v2) in zip(sd_step.items(),sd2_step.items()):\n",
    "    print(k , k2 , (v == v2).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "class Test():\n",
    "    @staticmethod\n",
    "    def aa(x):\n",
    "        print(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def bb(x):\n",
    "        print(x)\n",
    "\n",
    "    def trytry(self , y = 1):\n",
    "        Test.aa(y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a = Test()\n",
    "a.trytry(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is static method 1\n",
      "This is static method 2\n",
      "0\n",
      "This is static method 2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "class MyClass:\n",
    "    def static_method1(self):\n",
    "        print(\"This is static method 1\")\n",
    "        self.static_method2()\n",
    "\n",
    "    @classmethod\n",
    "    def static_method2(cls , x : int = 0) -> None:\n",
    "        print(f\"This is static method 2\")\n",
    "        print(x)\n",
    "        return None\n",
    "\n",
    "# 调用静态方法1\n",
    "a = MyClass()\n",
    "a.static_method1()\n",
    "MyClass.static_method2(x=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "pool = np.arange(100)\n",
    "random.shuffle(pool) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lambda x=0:1)(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lstm.weight_ih_l0',\n",
       "              tensor([[ 0.0910,  0.0800,  0.0628,  0.0726, -0.1145, -0.1069],\n",
       "                      [ 0.0649, -0.1171, -0.1229,  0.0789, -0.0136, -0.1013],\n",
       "                      [-0.0654,  0.0215, -0.0326, -0.0674, -0.0718, -0.0181],\n",
       "                      ...,\n",
       "                      [-0.0225, -0.0093, -0.0572,  0.1153,  0.0017,  0.0702],\n",
       "                      [ 0.1080,  0.1080,  0.1071,  0.0696, -0.0466,  0.0816],\n",
       "                      [ 0.0576, -0.0698,  0.0820,  0.1018, -0.0175,  0.0566]])),\n",
       "             ('lstm.weight_hh_l0',\n",
       "              tensor([[-0.0280, -0.0936,  0.0853,  ..., -0.0813, -0.1271, -0.1084],\n",
       "                      [-0.0192, -0.0755,  0.0637,  ...,  0.0753,  0.0598,  0.0185],\n",
       "                      [ 0.0307,  0.0137,  0.0812,  ..., -0.0837, -0.0918, -0.0508],\n",
       "                      ...,\n",
       "                      [-0.0289,  0.0206, -0.0827,  ...,  0.1069,  0.0243, -0.1109],\n",
       "                      [-0.0859,  0.0232, -0.0917,  ..., -0.0174, -0.0597,  0.0848],\n",
       "                      [ 0.0366,  0.0733, -0.0168,  ..., -0.0157,  0.0892, -0.0741]])),\n",
       "             ('lstm.bias_ih_l0',\n",
       "              tensor([ 0.0191, -0.0746, -0.0396, -0.0141,  0.0038, -0.0083,  0.1003,  0.0409,\n",
       "                       0.1198, -0.0119, -0.0639,  0.0693, -0.0330, -0.0458,  0.0855, -0.0679,\n",
       "                      -0.0744,  0.0140, -0.0287, -0.0987,  0.0438, -0.0394, -0.0564, -0.0499,\n",
       "                      -0.0671,  0.0057, -0.0117,  0.0957, -0.0763, -0.0820, -0.0899, -0.0484,\n",
       "                       0.0807, -0.1250,  0.0337, -0.0455, -0.0529,  0.1024,  0.0419, -0.1134,\n",
       "                       0.0531,  0.0296, -0.0274, -0.0653,  0.1151,  0.0220, -0.1196, -0.1238,\n",
       "                       0.0945,  0.0823,  0.0274,  0.0777, -0.0018, -0.0074,  0.0274,  0.0546,\n",
       "                       0.0709,  0.0588,  0.0872, -0.0416, -0.0822,  0.0054, -0.0199,  0.0918,\n",
       "                       0.0202, -0.1180,  0.1075, -0.0389, -0.0285, -0.0412, -0.0231,  0.0722,\n",
       "                       0.0403,  0.1063,  0.0028,  0.1151, -0.0995, -0.0875,  0.0494, -0.0099,\n",
       "                      -0.0218,  0.0593, -0.1183, -0.0232, -0.0525,  0.0346, -0.1204,  0.1160,\n",
       "                       0.1286,  0.0876,  0.0789, -0.0588,  0.0021,  0.0850,  0.0929,  0.0830,\n",
       "                       0.0324, -0.0251,  0.1146,  0.0827,  0.0535, -0.0651, -0.0082, -0.0471,\n",
       "                      -0.0977,  0.0690,  0.0600,  0.1240, -0.1115,  0.1121,  0.0962, -0.0509,\n",
       "                      -0.0685, -0.1064,  0.0246,  0.0216, -0.0092,  0.0810, -0.1299,  0.0248,\n",
       "                       0.0235, -0.0550, -0.1054, -0.1239, -0.0160,  0.0740,  0.0160, -0.0961,\n",
       "                       0.0100,  0.1118, -0.1289,  0.0109,  0.0239,  0.0069,  0.0771, -0.0248,\n",
       "                       0.0159,  0.0632, -0.0608, -0.0988,  0.0210,  0.0676, -0.0270, -0.1007,\n",
       "                      -0.0643, -0.1038,  0.0737,  0.0225,  0.0297, -0.0365, -0.0258,  0.0042,\n",
       "                       0.0251, -0.1205,  0.0640, -0.0582,  0.0603,  0.0540,  0.0846, -0.0021,\n",
       "                       0.0436,  0.0105, -0.0377,  0.0684,  0.0077, -0.0786, -0.1108, -0.0411,\n",
       "                       0.0809, -0.1047,  0.0511,  0.1093, -0.1085,  0.1075, -0.0161,  0.1070,\n",
       "                      -0.1276, -0.0751, -0.0694, -0.0189, -0.0160, -0.0765, -0.0686,  0.1142,\n",
       "                      -0.1082,  0.0634, -0.0013,  0.0584,  0.0667,  0.0689,  0.0911, -0.0751,\n",
       "                       0.0092,  0.0314,  0.1020,  0.0496, -0.0460, -0.0268,  0.1155,  0.0294,\n",
       "                       0.0297,  0.0935, -0.0533, -0.0776,  0.0688, -0.0881,  0.0971, -0.0980,\n",
       "                      -0.1249, -0.0068, -0.0847,  0.0627, -0.0610, -0.0423, -0.1300, -0.0243,\n",
       "                       0.0870,  0.0074, -0.0749,  0.1192, -0.0578, -0.0766,  0.0950,  0.0155,\n",
       "                       0.0475, -0.0734, -0.0437,  0.0593, -0.0462,  0.0725,  0.0467,  0.0680,\n",
       "                      -0.0663,  0.1045,  0.0774,  0.1032, -0.1158,  0.0054,  0.0423,  0.1126,\n",
       "                       0.0494,  0.0075,  0.0054, -0.0629, -0.0271, -0.0586,  0.0953, -0.0693,\n",
       "                       0.0435, -0.1092, -0.0347,  0.0917,  0.0693,  0.0091, -0.1078, -0.0773])),\n",
       "             ('lstm.bias_hh_l0',\n",
       "              tensor([-0.0679,  0.1002,  0.0016,  0.1253, -0.0299,  0.0723, -0.0513,  0.0205,\n",
       "                      -0.1295, -0.1281,  0.0734, -0.0957, -0.1162,  0.0429, -0.0343,  0.0438,\n",
       "                      -0.0777,  0.0099, -0.0384,  0.1028,  0.0523, -0.0802,  0.0399, -0.0406,\n",
       "                      -0.0793,  0.0264, -0.0516, -0.0366,  0.0355, -0.1175, -0.0294,  0.0540,\n",
       "                       0.0311, -0.0571, -0.0668, -0.0448, -0.1273, -0.1192, -0.0119,  0.0182,\n",
       "                       0.1074,  0.0850,  0.1133,  0.0680,  0.0913,  0.1194, -0.0952, -0.0393,\n",
       "                       0.0581, -0.0862, -0.1219, -0.0151,  0.0965,  0.0913, -0.1001,  0.0393,\n",
       "                       0.1259,  0.1084, -0.1057, -0.0298,  0.0983, -0.0420, -0.0226,  0.0115,\n",
       "                       0.0322,  0.0568, -0.0428, -0.0957, -0.0862,  0.1276, -0.1038, -0.0382,\n",
       "                       0.0871,  0.0627, -0.0961, -0.0693,  0.0139,  0.1265, -0.0518, -0.0220,\n",
       "                       0.0055,  0.0138,  0.0626, -0.0895,  0.0912, -0.0519,  0.0011,  0.0661,\n",
       "                      -0.0040,  0.0226, -0.0365,  0.0081, -0.0620,  0.0177,  0.0447, -0.0357,\n",
       "                       0.1284, -0.0757, -0.0313,  0.0632, -0.1149, -0.0980, -0.0702, -0.0002,\n",
       "                       0.1188,  0.0665, -0.0687, -0.0883, -0.0409, -0.0280, -0.1052, -0.1158,\n",
       "                      -0.0487, -0.0670, -0.0003, -0.0370,  0.0804, -0.1016, -0.0936,  0.0459,\n",
       "                       0.1131,  0.0914,  0.0745, -0.1137, -0.0566, -0.1219,  0.0297,  0.0022,\n",
       "                      -0.0309, -0.0867, -0.1191,  0.0544,  0.0136, -0.1222,  0.1281,  0.0092,\n",
       "                      -0.0869, -0.0654,  0.0975,  0.0140,  0.1289,  0.0941,  0.0861, -0.0870,\n",
       "                       0.0266, -0.0647, -0.0021,  0.0537,  0.0554, -0.0172, -0.0509, -0.0593,\n",
       "                      -0.0841,  0.0765, -0.0832,  0.0570, -0.1173,  0.0913,  0.1182,  0.1119,\n",
       "                      -0.0646, -0.1288, -0.1264, -0.1006,  0.0895,  0.0169, -0.0128,  0.0462,\n",
       "                      -0.0349, -0.0033,  0.1043, -0.0463, -0.0179, -0.0934, -0.0540,  0.0063,\n",
       "                       0.0608, -0.0776, -0.0387,  0.0334, -0.0947, -0.0911, -0.1106,  0.0994,\n",
       "                       0.0534, -0.0644, -0.1208,  0.0498,  0.0440, -0.0205,  0.0163, -0.0064,\n",
       "                      -0.0361, -0.0416,  0.1031, -0.0885, -0.1199, -0.0078, -0.0498, -0.0829,\n",
       "                      -0.0668,  0.0052, -0.0900, -0.0585, -0.0733,  0.0585,  0.0038,  0.1114,\n",
       "                       0.0279, -0.0499,  0.0853,  0.0380,  0.0655, -0.1116,  0.0966, -0.0721,\n",
       "                      -0.0056, -0.1050,  0.0796,  0.0114,  0.1237, -0.0625, -0.0847, -0.0011,\n",
       "                      -0.1169,  0.0864, -0.0757, -0.1102, -0.1182, -0.0920, -0.0223,  0.0926,\n",
       "                      -0.0916,  0.0562, -0.0689, -0.0101, -0.0221, -0.0086, -0.0036, -0.0093,\n",
       "                       0.0365,  0.0406,  0.0091, -0.0124, -0.0593, -0.0711, -0.0601,  0.0161,\n",
       "                       0.0382, -0.0658, -0.0966,  0.1171, -0.1012, -0.1066,  0.1027, -0.0883])),\n",
       "             ('fc.weight',\n",
       "              tensor([[-0.0217, -0.1234,  0.0678,  0.0253,  0.0283, -0.1199, -0.0025, -0.1178,\n",
       "                        0.0973, -0.0186,  0.0734,  0.0133, -0.1014, -0.0919,  0.0246,  0.0543,\n",
       "                        0.0546,  0.0942, -0.0880, -0.0958, -0.0656,  0.1047,  0.0508,  0.0113,\n",
       "                        0.0990, -0.0692, -0.1104, -0.0767, -0.0039, -0.0765,  0.0166, -0.0615,\n",
       "                        0.0143,  0.0189,  0.0767,  0.0443,  0.0828,  0.1069, -0.0269,  0.0865,\n",
       "                       -0.0148, -0.0489,  0.0533, -0.0161,  0.1266, -0.0524, -0.0883, -0.0403,\n",
       "                       -0.0875, -0.0551, -0.0175,  0.0242,  0.0700,  0.0162, -0.1245, -0.0126,\n",
       "                        0.0409, -0.1209, -0.0381,  0.0557,  0.0020,  0.0390,  0.0625,  0.0696]])),\n",
       "             ('fc.bias', tensor([-0.1200]))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (k,v),(k2,v2) in zip(sd.items(),sd2.items()):\n",
    "    print(k , k2 , (v == v2).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
