{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:01<00:00, 14.0MB/s]\n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 327kB/s]\n",
      "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.75MB/s]\n",
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 3.20MB/s]\n"
     ]
    }
   ],
   "source": [
    "## tensorboard --logdir=runs/tensorboard\n",
    "\n",
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# datasets\n",
    "trainset = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform)\n",
    "\n",
    "# dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                        shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "# constant for classes\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# helper function to show an image\n",
    "# (used in the `plot_classes_preds` function below)\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/tensorboard/fashion_mnist_experiment_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# show images\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "# write to tensorboard\n",
    "writer.add_image('four_fashion_mnist_images', img_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input type (unsigned char) and bias type (float) should be the same\n",
      "Error occurs, No graph saved\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (unsigned char) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\jinmeng\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:889\u001b[0m, in \u001b[0;36mSummaryWriter.add_graph\u001b[1;34m(self, model, input_to_model, verbose, use_strict_trace)\u001b[0m\n\u001b[0;32m    885\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard.logging.add_graph\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;66;03m# A valid PyTorch model should have a 'forward' method\u001b[39;00m\n\u001b[0;32m    888\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file_writer()\u001b[38;5;241m.\u001b[39madd_graph(\n\u001b[1;32m--> 889\u001b[0m         \u001b[43mgraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_to_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_strict_trace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m     )\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;66;03m# Caffe2 models do not have the 'forward' method\u001b[39;00m\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaffe2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m caffe2_pb2\n",
      "File \u001b[1;32mc:\\Users\\jinmeng\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\tensorboard\\_pytorch_graph.py:336\u001b[0m, in \u001b[0;36mgraph\u001b[1;34m(model, args, verbose, use_strict_trace)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;28mprint\u001b[39m(e)\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurs, No graph saved\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 336\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mprint\u001b[39m(graph)\n",
      "File \u001b[1;32mc:\\Users\\jinmeng\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\tensorboard\\_pytorch_graph.py:330\u001b[0m, in \u001b[0;36mgraph\u001b[1;34m(model, args, verbose, use_strict_trace)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _set_model_to_eval(model):\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 330\u001b[0m         trace \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_strict_trace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m         graph \u001b[38;5;241m=\u001b[39m trace\u001b[38;5;241m.\u001b[39mgraph\n\u001b[0;32m    332\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_jit_pass_inline(graph)\n",
      "File \u001b[1;32mc:\\Users\\jinmeng\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\jit\\_trace.py:806\u001b[0m, in \u001b[0;36mtrace\u001b[1;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    805\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_kwarg_inputs should be a dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrap_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    814\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs_is_kwarg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    818\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    823\u001b[0m ):\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m example_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jinmeng\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\jit\\_trace.py:1074\u001b[0m, in \u001b[0;36mtrace_module\u001b[1;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[0m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1073\u001b[0m     example_inputs \u001b[38;5;241m=\u001b[39m make_tuple(example_inputs)\n\u001b[1;32m-> 1074\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_method_from_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar_lookup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m        \u001b[49m\u001b[43margument_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1085\u001b[0m check_trace_method \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_c\u001b[38;5;241m.\u001b[39m_get_method(method_name)\n\u001b[0;32m   1087\u001b[0m \u001b[38;5;66;03m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jinmeng\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jinmeng\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jinmeng\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1500\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1501\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m16\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jinmeng\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jinmeng\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jinmeng\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1500\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1501\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[1;32mc:\\Users\\jinmeng\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jinmeng\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (unsigned char) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "writer.add_graph(net, images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "# helper function\n",
    "def select_n_random(data, labels, n=100):\n",
    "    '''\n",
    "    Selects n random datapoints and their corresponding labels from a dataset\n",
    "    '''\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "# select random images and their target indices\n",
    "images, labels = select_n_random(trainset.data, trainset.targets)\n",
    "\n",
    "# get the class labels for each image\n",
    "class_labels = [classes[lab] for lab in labels]\n",
    "\n",
    "# log embeddings\n",
    "features = images.view(-1, 28 * 28)\n",
    "writer.add_embedding(features,\n",
    "                     metadata=class_labels,\n",
    "                     label_img=images.unsqueeze(1))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def images_to_probs(net, images):\n",
    "    '''\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    '''\n",
    "    output = net(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "\n",
    "def plot_classes_preds(net, images, labels):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            classes[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            classes[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # every 1000 mini-batches...\n",
    "\n",
    "            # ...log the running loss\n",
    "            writer.add_scalar('training loss',\n",
    "                            running_loss / 1000,\n",
    "                            epoch * len(trainloader) + i)\n",
    "\n",
    "            # ...log a Matplotlib Figure showing the model's predictions on a\n",
    "            # random mini-batch\n",
    "            writer.add_figure('predictions vs. actuals',\n",
    "                            plot_classes_preds(net, inputs, labels),\n",
    "                            global_step=epoch * len(trainloader) + i)\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
