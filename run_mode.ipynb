{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[44m24-02-03 23:48:28|MOD:run_model_copy|\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-02-03 23:48:28|MOD:run_model_copy|\u001b[0m: \u001b[1m\u001b[31mStart Process [Load Data]!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Process Queue : Data + Train + Test + Instance\n",
      "--Start Training New!\n",
      "--Model_name is set to LSTM_day_SHORTTEST!\n",
      "{'storage_type': 'mem',\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'precision': 'float',\n",
      " 'batch_size': 10000,\n",
      " 'model_name': 'LSTM_day_SHORTTEST',\n",
      " 'model_module': 'LSTM',\n",
      " 'model_data_type': 'day',\n",
      " 'model_num': 1,\n",
      " 'beg_date': 20170103,\n",
      " 'test_dates_end': None,\n",
      " 'interval': 120,\n",
      " 'input_step_day': 5,\n",
      " 'test_step_day': 1,\n",
      " 'MODEL_PARAM': {'hidden_dim': [64],\n",
      "                 'seqlens': [{'day': 40, '15m': 20, 'dms': 40}],\n",
      "                 'rnn_layers': [2],\n",
      "                 'mlp_layers': [1],\n",
      "                 'dropout': [0.1],\n",
      "                 'fc_in': [True],\n",
      "                 'fc_att': [False],\n",
      "                 'type_rnn': ['lstm'],\n",
      "                 'rnn_att': [False],\n",
      "                 'num_output': [1],\n",
      "                 'kernel_size': [3, 3],\n",
      "                 'hidden_as_factor': [False],\n",
      "                 'ordered_param_group': [False],\n",
      "                 'tra_num_states': [3],\n",
      "                 'ATF_mask': {'causal': False, 'gaussian': False, 'tradegap': False}},\n",
      " 'train_params': {'dataloader': {'random_seed': None, 'random_tv_split': True, 'sample_method': 'total_shuffle', 'train_ratio': 0.8},\n",
      "                  'trainer': {'optimizer': {'name': 'Adam', 'param': {}},\n",
      "                              'scheduler': {'name': 'cycle', 'param': {'base_lr': 1e-07, 'step_size_up': 4}},\n",
      "                              'learn_rate': {'base': 0.005,\n",
      "                                             'ratio': {'attempt': [1, 0.1, 10, 0.01, 100], 'round': [1.0], 'transfer': 0.1},\n",
      "                                             'reset': {'num_reset': 2, 'trigger': 40, 'recover_level': 1.0, 'speedup2x': True}},\n",
      "                              'nanloss': {'retry': 2},\n",
      "                              'gradient': {'clip_value': 10.0},\n",
      "                              'retrain': {'attempts': 4, 'min_epoch': 20, 'min_epoch_round': 10}},\n",
      "                  'criterion': {'loss': 'pearson',\n",
      "                                'score': {'train': 'pearson', 'valid': 'pearson', 'test': 'pearson'},\n",
      "                                'penalty': {'hidden_orthogonality': {'lamb': 0.001}, 'tra_ot_penalty': {'lamb': 0.01, 'rho': 0.999}}},\n",
      "                  'transfer': False,\n",
      "                  'output_types': ['best', 'swalast', 'swabest'],\n",
      "                  'multitask': {'type': 'hybrid',\n",
      "                                'param_dict': {'dwa': {'tau': 2},\n",
      "                                               'ruw': {'phi': None},\n",
      "                                               'ewa': {},\n",
      "                                               'gls': {},\n",
      "                                               'rws': {},\n",
      "                                               'hybrid': {'phi': None, 'tau': 2}}},\n",
      "                  'terminate': {'overall': {'early_stop': 20, 'max_epoch': 200, 'valid_converge': {'min_epoch': 5, 'eps': 1e-05}},\n",
      "                                'round': {'early_stop': 10, 'max_epoch': 100, 'valid_converge': {'min_epoch': 5, 'eps': 1e-05}}}},\n",
      " 'compt_params': {'cuda_first': True, 'num_worker': 10}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-02-03 23:48:43|MOD:run_model_copy|\u001b[0m: \u001b[1m\u001b[31mFinish Process [Load Data]! Cost 15.2Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-02-03 23:48:43|MOD:run_model_copy|\u001b[0m: \u001b[1m\u001b[31mStart Process [Train Model]!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day {'avg': tensor([[1.0085, 1.0083, 1.0104, 1.0106, 1.6968, 1.0094],\n",
      "        [1.0097, 1.0092, 1.0109, 1.0104, 1.7055, 1.0102],\n",
      "        [1.0092, 1.0090, 1.0101, 1.0103, 1.6933, 1.0097],\n",
      "        [1.0091, 1.0084, 1.0098, 1.0097, 1.6816, 1.0092],\n",
      "        [1.0087, 1.0080, 1.0092, 1.0086, 1.6852, 1.0085],\n",
      "        [1.0077, 1.0074, 1.0096, 1.0097, 1.6630, 1.0086],\n",
      "        [1.0088, 1.0083, 1.0100, 1.0096, 1.6749, 1.0093],\n",
      "        [1.0082, 1.0080, 1.0092, 1.0095, 1.6626, 1.0088],\n",
      "        [1.0081, 1.0075, 1.0089, 1.0088, 1.6557, 1.0083],\n",
      "        [1.0078, 1.0071, 1.0083, 1.0077, 1.6499, 1.0076],\n",
      "        [1.0067, 1.0065, 1.0087, 1.0089, 1.6412, 1.0077],\n",
      "        [1.0078, 1.0074, 1.0092, 1.0088, 1.6540, 1.0085],\n",
      "        [1.0074, 1.0072, 1.0084, 1.0086, 1.6429, 1.0080],\n",
      "        [1.0073, 1.0067, 1.0081, 1.0080, 1.6295, 1.0074],\n",
      "        [1.0070, 1.0063, 1.0075, 1.0069, 1.6236, 1.0068],\n",
      "        [1.0059, 1.0057, 1.0079, 1.0081, 1.6144, 1.0068],\n",
      "        [1.0071, 1.0066, 1.0084, 1.0080, 1.6246, 1.0077],\n",
      "        [1.0066, 1.0064, 1.0076, 1.0077, 1.6191, 1.0072],\n",
      "        [1.0065, 1.0058, 1.0073, 1.0072, 1.6097, 1.0066],\n",
      "        [1.0062, 1.0055, 1.0067, 1.0061, 1.6033, 1.0060],\n",
      "        [1.0052, 1.0050, 1.0072, 1.0073, 1.5905, 1.0061],\n",
      "        [1.0064, 1.0059, 1.0077, 1.0072, 1.5941, 1.0069],\n",
      "        [1.0059, 1.0057, 1.0069, 1.0070, 1.5869, 1.0065],\n",
      "        [1.0059, 1.0053, 1.0068, 1.0066, 1.5796, 1.0061],\n",
      "        [1.0056, 1.0050, 1.0062, 1.0056, 1.5843, 1.0055],\n",
      "        [1.0047, 1.0045, 1.0068, 1.0068, 1.5651, 1.0057],\n",
      "        [1.0059, 1.0054, 1.0073, 1.0068, 1.5747, 1.0065],\n",
      "        [1.0055, 1.0053, 1.0065, 1.0067, 1.5726, 1.0061],\n",
      "        [1.0055, 1.0049, 1.0064, 1.0063, 1.5592, 1.0057],\n",
      "        [1.0052, 1.0045, 1.0058, 1.0052, 1.5556, 1.0051],\n",
      "        [1.0042, 1.0041, 1.0063, 1.0064, 1.5438, 1.0053],\n",
      "        [1.0055, 1.0050, 1.0069, 1.0064, 1.5531, 1.0061],\n",
      "        [1.0051, 1.0049, 1.0061, 1.0062, 1.5487, 1.0057],\n",
      "        [1.0050, 1.0044, 1.0059, 1.0057, 1.5320, 1.0052],\n",
      "        [1.0046, 1.0040, 1.0051, 1.0046, 1.5256, 1.0045],\n",
      "        [1.0036, 1.0035, 1.0057, 1.0058, 1.5116, 1.0046],\n",
      "        [1.0048, 1.0044, 1.0061, 1.0057, 1.5137, 1.0054],\n",
      "        [1.0043, 1.0041, 1.0054, 1.0055, 1.5056, 1.0050],\n",
      "        [1.0042, 1.0037, 1.0051, 1.0049, 1.4921, 1.0044],\n",
      "        [1.0038, 1.0032, 1.0043, 1.0038, 1.4822, 1.0037],\n",
      "        [1.0027, 1.0026, 1.0048, 1.0048, 1.4660, 1.0038],\n",
      "        [1.0038, 1.0034, 1.0052, 1.0046, 1.4652, 1.0044],\n",
      "        [1.0033, 1.0031, 1.0043, 1.0045, 1.4542, 1.0039],\n",
      "        [1.0032, 1.0027, 1.0041, 1.0040, 1.4371, 1.0034],\n",
      "        [1.0028, 1.0022, 1.0033, 1.0028, 1.4226, 1.0027],\n",
      "        [1.0017, 1.0016, 1.0037, 1.0038, 1.4119, 1.0028],\n",
      "        [1.0027, 1.0024, 1.0040, 1.0035, 1.4125, 1.0033],\n",
      "        [1.0021, 1.0021, 1.0031, 1.0033, 1.4015, 1.0028],\n",
      "        [1.0021, 1.0016, 1.0029, 1.0030, 1.3864, 1.0024],\n",
      "        [1.0018, 1.0013, 1.0022, 1.0018, 1.3759, 1.0017],\n",
      "        [1.0007, 1.0007, 1.0026, 1.0028, 1.3559, 1.0018],\n",
      "        [1.0018, 1.0015, 1.0030, 1.0027, 1.3494, 1.0024],\n",
      "        [1.0013, 1.0013, 1.0022, 1.0025, 1.3345, 1.0020],\n",
      "        [1.0012, 1.0008, 1.0020, 1.0021, 1.3190, 1.0015],\n",
      "        [1.0009, 1.0004, 1.0012, 1.0009, 1.2955, 1.0008],\n",
      "        [0.9999, 0.9999, 1.0016, 1.0020, 1.2603, 1.0009],\n",
      "        [1.0010, 1.0008, 1.0021, 1.0018, 1.2381, 1.0016],\n",
      "        [1.0005, 1.0005, 1.0011, 1.0016, 1.1991, 1.0011],\n",
      "        [1.0003, 1.0000, 1.0008, 1.0011, 1.1316, 1.0005],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]), 'std': tensor([[0.2527, 0.2517, 0.2523, 0.2530, 4.2304, 0.2616],\n",
      "        [0.2510, 0.2500, 0.2509, 0.2513, 4.0603, 0.2513],\n",
      "        [0.2492, 0.2483, 0.2490, 0.2498, 4.0808, 0.2490],\n",
      "        [0.2467, 0.2456, 0.2461, 0.2459, 4.0358, 0.2461],\n",
      "        [0.2436, 0.2426, 0.2429, 0.2425, 4.1018, 0.2427],\n",
      "        [0.2394, 0.2387, 0.2392, 0.2404, 3.9582, 0.2486],\n",
      "        [0.2374, 0.2368, 0.2375, 0.2383, 3.7802, 0.2382],\n",
      "        [0.2351, 0.2347, 0.2352, 0.2365, 3.5745, 0.2353],\n",
      "        [0.2325, 0.2320, 0.2326, 0.2330, 3.8854, 0.2326],\n",
      "        [0.2298, 0.2292, 0.2295, 0.2296, 3.5963, 0.2294],\n",
      "        [0.2257, 0.2254, 0.2259, 0.2274, 3.5894, 0.2258],\n",
      "        [0.2238, 0.2239, 0.2244, 0.2259, 3.8270, 0.2254],\n",
      "        [0.2221, 0.2219, 0.2224, 0.2235, 3.8636, 0.2225],\n",
      "        [0.2197, 0.2191, 0.2196, 0.2200, 3.3153, 0.2197],\n",
      "        [0.2170, 0.2163, 0.2168, 0.2169, 3.5903, 0.2167],\n",
      "        [0.2135, 0.2130, 0.2137, 0.2151, 3.7557, 0.2134],\n",
      "        [0.2119, 0.2115, 0.2121, 0.2133, 3.7047, 0.2129],\n",
      "        [0.2097, 0.2093, 0.2101, 0.2108, 3.4941, 0.2100],\n",
      "        [0.2074, 0.2066, 0.2072, 0.2076, 3.3625, 0.2071],\n",
      "        [0.2051, 0.2042, 0.2049, 0.2049, 3.5896, 0.2046],\n",
      "        [0.2020, 0.2012, 0.2023, 0.2030, 3.8066, 0.2018],\n",
      "        [0.2003, 0.1995, 0.2005, 0.2011, 3.7178, 0.2011],\n",
      "        [0.1981, 0.1975, 0.1991, 0.1992, 3.4201, 0.1985],\n",
      "        [0.1967, 0.1956, 0.1968, 0.1966, 3.2123, 0.1965],\n",
      "        [0.1940, 0.1932, 0.1943, 0.1940, 4.4779, 0.1938],\n",
      "        [0.1908, 0.1902, 0.1914, 0.1918, 3.1821, 0.1909],\n",
      "        [0.1889, 0.1883, 0.1894, 0.1901, 3.2240, 0.1899],\n",
      "        [0.1870, 0.1864, 0.1881, 0.1886, 3.0584, 0.1875],\n",
      "        [0.1855, 0.1845, 0.1858, 0.1856, 3.2100, 0.1854],\n",
      "        [0.1819, 0.1813, 0.1825, 0.1827, 3.0745, 0.1821],\n",
      "        [0.1786, 0.1782, 0.1792, 0.1802, 3.2311, 0.1789],\n",
      "        [0.1762, 0.1759, 0.1773, 0.1784, 3.0628, 0.1777],\n",
      "        [0.1744, 0.1740, 0.1759, 0.1762, 3.4691, 0.1752],\n",
      "        [0.1723, 0.1713, 0.1725, 0.1722, 3.2419, 0.1721],\n",
      "        [0.1680, 0.1674, 0.1684, 0.1685, 3.5062, 0.1680],\n",
      "        [0.1643, 0.1635, 0.1648, 0.1654, 3.0161, 0.1643],\n",
      "        [0.1609, 0.1606, 0.1620, 0.1634, 3.3694, 0.1625],\n",
      "        [0.1582, 0.1579, 0.1603, 0.1606, 2.8475, 0.1594],\n",
      "        [0.1559, 0.1549, 0.1565, 0.1561, 2.7551, 0.1561],\n",
      "        [0.1514, 0.1506, 0.1524, 0.1519, 2.9498, 0.1517],\n",
      "        [0.1471, 0.1459, 0.1475, 0.1473, 4.5080, 0.1469],\n",
      "        [0.1420, 0.1416, 0.1430, 0.1435, 2.3828, 0.1439],\n",
      "        [0.1384, 0.1378, 0.1403, 0.1403, 2.2827, 0.1398],\n",
      "        [0.1354, 0.1342, 0.1365, 0.1360, 2.3226, 0.1361],\n",
      "        [0.1302, 0.1292, 0.1316, 0.1311, 2.1734, 0.1311],\n",
      "        [0.1247, 0.1238, 0.1256, 0.1259, 2.2124, 0.1256],\n",
      "        [0.1191, 0.1189, 0.1203, 0.1207, 2.2262, 0.1219],\n",
      "        [0.1147, 0.1142, 0.1163, 0.1171, 2.2736, 0.1162],\n",
      "        [0.1105, 0.1098, 0.1122, 0.1121, 2.3479, 0.1120],\n",
      "        [0.1049, 0.1040, 0.1059, 0.1057, 2.8022, 0.1058],\n",
      "        [0.0981, 0.0974, 0.0984, 0.0997, 2.3151, 0.0992],\n",
      "        [0.0923, 0.0922, 0.0935, 0.0951, 2.3134, 0.0963],\n",
      "        [0.0868, 0.0864, 0.0886, 0.0896, 1.9287, 0.0891],\n",
      "        [0.0807, 0.0799, 0.0815, 0.0821, 1.8590, 0.0824],\n",
      "        [0.0732, 0.0720, 0.0729, 0.0736, 2.0776, 0.0743],\n",
      "        [0.0643, 0.0633, 0.0637, 0.0661, 4.3622, 0.0659],\n",
      "        [0.0563, 0.0550, 0.0565, 0.0583, 1.6378, 0.0613],\n",
      "        [0.0480, 0.0445, 0.0468, 0.0473, 1.2209, 0.0487],\n",
      "        [0.0337, 0.0296, 0.0308, 0.0324, 0.9545, 0.0349],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])} dict_keys(['day'])\n",
      "here\n",
      "tensor([[ 0.0336,  0.1192, -0.0358, -0.0485, -0.0701, -0.0266],\n",
      "        [-0.2622, -0.2607, -0.0386, -0.0484, -0.4288, -0.1225],\n",
      "        [-0.2627, -0.2622, -0.0349, -0.0481, -0.4640, -0.1217],\n",
      "        [-0.2646, -0.2628, -0.0344, -0.0465, -0.4917, -0.1208],\n",
      "        [-0.2667, -0.2644, -0.0319, -0.0418, -0.3538, -0.1196],\n",
      "        [-0.2662, -0.2659, -0.0353, -0.0486, -0.4918, -0.1222],\n",
      "        [-0.2754, -0.2738, -0.0384, -0.0489, -0.4884, -0.1271],\n",
      "        [-0.2761, -0.2757, -0.0348, -0.0488, -0.5142, -0.1268],\n",
      "        [-0.2784, -0.2765, -0.0344, -0.0474, -0.4857, -0.1260],\n",
      "        [-0.2819, -0.2794, -0.0317, -0.0425, -0.5060, -0.1250],\n",
      "        [-0.2819, -0.2817, -0.0353, -0.0498, -0.4778, -0.1282],\n",
      "        [ 0.2445,  0.2473,  0.5209,  0.5049, -0.5011,  0.4150],\n",
      "        [ 0.8474,  0.8509,  1.1517,  1.1310, -0.4375,  1.0365],\n",
      "        [ 1.5342,  1.5473,  1.8842,  1.8683, -0.3613,  1.7550],\n",
      "        [ 1.3890,  2.0644,  1.5564,  1.6901,  1.0183,  1.9649],\n",
      "        [ 0.9384,  1.6562,  0.9693,  1.3925,  0.4204,  1.2591],\n",
      "        [ 1.2981,  1.6045,  1.2344,  1.8448,  0.2005,  1.4696],\n",
      "        [ 1.5374,  1.9418,  1.6654,  1.7672,  0.3981,  1.7698],\n",
      "        [ 1.2958,  1.4340,  1.1057,  1.3665,  0.0042,  1.3839],\n",
      "        [ 1.0328,  1.0427,  0.6899,  0.7771, -0.0656,  0.8285],\n",
      "        [ 0.4764,  1.0306,  0.8128,  1.3455, -0.0138,  0.9826],\n",
      "        [ 1.0499,  1.1070,  0.9780,  1.0995, -0.0280,  1.0960],\n",
      "        [ 0.6078,  0.8227,  0.8218,  0.9811, -0.2970,  0.8725],\n",
      "        [ 0.6933,  1.0836,  1.0701,  1.2963, -0.0931,  1.1180],\n",
      "        [ 1.0034,  1.7487,  1.4052,  2.0120,  0.3056,  1.6334],\n",
      "        [ 1.6993,  2.6724,  1.9559,  3.0754,  1.6680,  2.6456],\n",
      "        [ 2.3608,  2.5724,  2.3576,  2.4944,  1.0731,  2.4828],\n",
      "        [ 1.9487,  2.3839,  2.3141,  2.7451,  0.3156,  2.4020],\n",
      "        [ 2.4407,  2.9918,  2.7411,  2.9397,  0.4801,  2.9074],\n",
      "        [ 2.4809,  3.1165,  2.7913,  3.4872,  0.4769,  3.0358],\n",
      "        [ 3.0866,  3.3075,  2.3786,  2.3138,  0.5908,  2.8166],\n",
      "        [ 1.3747,  2.7735,  1.7900,  3.1212,  0.3602,  2.2952],\n",
      "        [ 2.6963,  2.9635,  2.7322,  2.7489,  0.1248,  2.8838],\n",
      "        [ 2.1501,  2.6026,  2.0981,  2.4793, -0.0226,  2.4595],\n",
      "        [ 1.4464,  1.7801,  1.2044,  1.1612, -0.0827,  1.1864],\n",
      "        [-0.9787, -0.3059, -0.3453, -0.3759, -0.0462, -0.4054],\n",
      "        [-0.9235, -0.3670, -1.7047, -1.0322,  0.1086, -0.5754],\n",
      "        [-1.5781, -1.2106, -1.8179, -0.3548, -0.2131, -1.1100],\n",
      "        [-1.2356,  0.6542, -0.6046,  1.8438,  0.2378,  0.0734],\n",
      "        [-0.0596, -0.0596, -0.0596, -0.0596,  0.0000, -0.0596]])\n",
      "tensor([[ 0.0336,  0.1192, -0.0358, -0.0485, -0.0701, -0.0266],\n",
      "        [-0.2622, -0.2607, -0.0386, -0.0484, -0.4288, -0.1225],\n",
      "        [-0.2627, -0.2622, -0.0349, -0.0481, -0.4640, -0.1217],\n",
      "        [-0.2646, -0.2628, -0.0344, -0.0465, -0.4917, -0.1208],\n",
      "        [-0.2667, -0.2644, -0.0319, -0.0418, -0.3538, -0.1196],\n",
      "        [-0.2662, -0.2659, -0.0353, -0.0486, -0.4918, -0.1222],\n",
      "        [-0.2754, -0.2738, -0.0384, -0.0489, -0.4884, -0.1271],\n",
      "        [-0.2761, -0.2757, -0.0348, -0.0488, -0.5142, -0.1268],\n",
      "        [-0.2784, -0.2765, -0.0344, -0.0474, -0.4857, -0.1260],\n",
      "        [-0.2819, -0.2794, -0.0317, -0.0425, -0.5060, -0.1250],\n",
      "        [-0.2819, -0.2817, -0.0353, -0.0498, -0.4778, -0.1282],\n",
      "        [ 0.2445,  0.2473,  0.5209,  0.5049, -0.5011,  0.4150],\n",
      "        [ 0.8474,  0.8509,  1.1517,  1.1310, -0.4375,  1.0365],\n",
      "        [ 1.5342,  1.5473,  1.8842,  1.8683, -0.3613,  1.7550],\n",
      "        [ 1.3890,  2.0644,  1.5564,  1.6901,  1.0183,  1.9649],\n",
      "        [ 0.9384,  1.6562,  0.9693,  1.3925,  0.4204,  1.2591],\n",
      "        [ 1.2981,  1.6045,  1.2344,  1.8448,  0.2005,  1.4696],\n",
      "        [ 1.5374,  1.9418,  1.6654,  1.7672,  0.3981,  1.7698],\n",
      "        [ 1.2958,  1.4340,  1.1057,  1.3665,  0.0042,  1.3839],\n",
      "        [ 1.0328,  1.0427,  0.6899,  0.7771, -0.0656,  0.8285],\n",
      "        [ 0.4764,  1.0306,  0.8128,  1.3455, -0.0138,  0.9826],\n",
      "        [ 1.0499,  1.1070,  0.9780,  1.0995, -0.0280,  1.0960],\n",
      "        [ 0.6078,  0.8227,  0.8218,  0.9811, -0.2970,  0.8725],\n",
      "        [ 0.6933,  1.0836,  1.0701,  1.2963, -0.0931,  1.1180],\n",
      "        [ 1.0034,  1.7487,  1.4052,  2.0120,  0.3056,  1.6334],\n",
      "        [ 1.6993,  2.6724,  1.9559,  3.0754,  1.6680,  2.6456],\n",
      "        [ 2.3608,  2.5724,  2.3576,  2.4944,  1.0731,  2.4828],\n",
      "        [ 1.9487,  2.3839,  2.3141,  2.7451,  0.3156,  2.4020],\n",
      "        [ 2.4407,  2.9918,  2.7411,  2.9397,  0.4801,  2.9074],\n",
      "        [ 2.4809,  3.1165,  2.7913,  3.4872,  0.4769,  3.0358],\n",
      "        [ 3.0866,  3.3075,  2.3786,  2.3138,  0.5908,  2.8166],\n",
      "        [ 1.3747,  2.7735,  1.7900,  3.1212,  0.3602,  2.2952],\n",
      "        [ 2.6963,  2.9635,  2.7322,  2.7489,  0.1248,  2.8838],\n",
      "        [ 2.1501,  2.6026,  2.0981,  2.4793, -0.0226,  2.4595],\n",
      "        [ 1.4464,  1.7801,  1.2044,  1.1612, -0.0827,  1.1864],\n",
      "        [-0.9787, -0.3059, -0.3453, -0.3759, -0.0462, -0.4054],\n",
      "        [-0.9235, -0.3670, -1.7047, -1.0322,  0.1086, -0.5754],\n",
      "        [-1.5781, -1.2106, -1.8179, -0.3548, -0.2131, -1.1100],\n",
      "        [-1.2356,  0.6542, -0.6046,  1.8438,  0.2378,  0.0734],\n",
      "        [-0.0596, -0.0596, -0.0596, -0.0596,  0.0000, -0.0596]])\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Workspace/learndl/run_model_copy.py:786\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    785\u001b[0m     controller \u001b[38;5;241m=\u001b[39m model_controller()\n\u001b[0;32m--> 786\u001b[0m     \u001b[43mcontroller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    787\u001b[0m     controller\u001b[38;5;241m.\u001b[39mResultOutput()\n\u001b[1;32m    788\u001b[0m     trainer_timer\u001b[38;5;241m.\u001b[39mprint()\n",
      "File \u001b[0;32m~/Workspace/learndl/run_model_copy.py:72\u001b[0m, in \u001b[0;36mmodel_controller.main_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m process_name \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mprocess_queue:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSetProcessName(process_name)\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_process_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprocess_name\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/learndl/run_model_copy.py:119\u001b[0m, in \u001b[0;36mmodel_controller.model_process_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_date , \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_num \u001b[38;5;241m=\u001b[39m model_date , model_num\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mModelPreparation(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrainModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m total_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_process\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinish Process [Train Model]! Cost \u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[38;5;124m Hours, \u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[38;5;124m Min/model, \u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[38;5;124m Sec/Epoch\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    122\u001b[0m     total_time \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3600\u001b[39m , total_time \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_count , \u001b[38;5;241m1\u001b[39m) , total_time \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_count , \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/Workspace/learndl/run_model_copy.py:208\u001b[0m, in \u001b[0;36mmodel_controller.TrainModel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mTrainModel\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrainModelStart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcond\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloop_status\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNewLoop()\n",
      "File \u001b[0;32m~/Workspace/learndl/run_model_copy.py:257\u001b[0m, in \u001b[0;36mmodel_controller.TrainModelStart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m #\u001b[39m\u001b[38;5;132;01m{:d}\u001b[39;00m\u001b[38;5;124m @\u001b[39m\u001b[38;5;132;01m{:4d}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(config\u001b[38;5;241m.\u001b[39mmodel_name , \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_num , \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_date)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdataloader_param \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_date , \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseqlens\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[0;32m--> 257\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_date\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mseqlens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtick[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprinter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_dataloader\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Workspace/learndl/scripts/data_util/ModelData.py:685\u001b[0m, in \u001b[0;36mModelData.create_dataloader\u001b[0;34m(self, process_name, style, model_date, seqlens)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_device(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer)\n\u001b[1;32m    684\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_index(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnonnan_sample)\n\u001b[0;32m--> 685\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatic_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_step\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_step\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonnan_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;66;03m#index = self.sample_index2(self.nonnan_sample)\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;66;03m#self.static_dataloader2(x , y_step , w_step , index , self.nonnan_sample)\u001b[39;00m\n\u001b[1;32m    690\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect() , torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m~/Workspace/learndl/scripts/data_util/ModelData.py:808\u001b[0m, in \u001b[0;36mModelData.static_dataloader\u001b[0;34m(self, x, y, w, index, nonnan_sample)\u001b[0m\n\u001b[1;32m    806\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_norm_x(data,mdt)\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 808\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    809\u001b[0m     batch_x\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[1;32m    810\u001b[0m batch_x \u001b[38;5;241m=\u001b[39m batch_x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch_x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(batch_x)\n",
      "\u001b[0;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run run_model_copy.py --process=0 --rawname=1 --resume=0 --anchoring=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--process PROCESS] [--rawname RAWNAME]\n",
      "                             [--resume RESUME] [--anchoring ANCHORING]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/home/mengkjin/.local/share/jupyter/runtime/kernel-v2-7625Q3lOFmm1dpUU.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--What process would you want to run? 0: all, 1: train only (default), 2: test only , 3: copy to instance\n",
      "--Process Queue : Data + Train\n",
      "--Multiple model path of LSTM_day_SHORTTEST exists, input [yes] to resume training, or start a new one!\n",
      "--Start Training New!\n",
      "--Multiple model path of LSTM_day_SHORTTEST exists, input [yes] to confirm deletion, or a new directory will be made!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[44m24-02-03 23:53:53|MOD:587015923   |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Directories of [LSTM_day_SHORTTEST] deletion Confirmed!\n",
      "--Model_name is set to LSTM_day_SHORTTEST!\n",
      "{'storage_type': 'mem',\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'precision': 'float',\n",
      " 'batch_size': 10000,\n",
      " 'model_name': 'LSTM_day_SHORTTEST',\n",
      " 'model_module': 'LSTM',\n",
      " 'model_data_type': 'day',\n",
      " 'model_num': 1,\n",
      " 'beg_date': 20170103,\n",
      " 'test_dates_end': None,\n",
      " 'interval': 120,\n",
      " 'input_step_day': 5,\n",
      " 'test_step_day': 1,\n",
      " 'MODEL_PARAM': {'hidden_dim': [64],\n",
      "                 'seqlens': [{'day': 40, '15m': 20, 'dms': 40}],\n",
      "                 'rnn_layers': [2],\n",
      "                 'mlp_layers': [1],\n",
      "                 'dropout': [0.1],\n",
      "                 'fc_in': [True],\n",
      "                 'fc_att': [False],\n",
      "                 'type_rnn': ['lstm'],\n",
      "                 'rnn_att': [False],\n",
      "                 'num_output': [1],\n",
      "                 'kernel_size': [3, 3],\n",
      "                 'hidden_as_factor': [False],\n",
      "                 'ordered_param_group': [False],\n",
      "                 'tra_num_states': [3],\n",
      "                 'ATF_mask': {'causal': False, 'gaussian': False, 'tradegap': False}},\n",
      " 'train_params': {'dataloader': {'random_seed': None, 'random_tv_split': True, 'sample_method': 'total_shuffle', 'train_ratio': 0.8},\n",
      "                  'trainer': {'optimizer': {'name': 'Adam', 'param': {}},\n",
      "                              'scheduler': {'name': 'cycle', 'param': {'base_lr': 1e-07, 'step_size_up': 4}},\n",
      "                              'learn_rate': {'base': 0.005,\n",
      "                                             'ratio': {'attempt': [1, 0.1, 10, 0.01, 100], 'round': [1.0], 'transfer': 0.1},\n",
      "                                             'reset': {'num_reset': 2, 'trigger': 40, 'recover_level': 1.0, 'speedup2x': True}},\n",
      "                              'nanloss': {'retry': 2},\n",
      "                              'gradient': {'clip_value': 10.0},\n",
      "                              'retrain': {'attempts': 4, 'min_epoch': 20, 'min_epoch_round': 10}},\n",
      "                  'criterion': {'loss': 'pearson',\n",
      "                                'score': {'train': 'pearson', 'valid': 'pearson', 'test': 'pearson'},\n",
      "                                'penalty': {'hidden_orthogonality': {'lamb': 0.001}, 'tra_ot_penalty': {'lamb': 0.01, 'rho': 0.999}}},\n",
      "                  'transfer': False,\n",
      "                  'output_types': ['best', 'swalast', 'swabest'],\n",
      "                  'multitask': {'type': 'hybrid',\n",
      "                                'param_dict': {'dwa': {'tau': 2},\n",
      "                                               'ruw': {'phi': None},\n",
      "                                               'ewa': {},\n",
      "                                               'gls': {},\n",
      "                                               'rws': {},\n",
      "                                               'hybrid': {'phi': None, 'tau': 2}}},\n",
      "                  'terminate': {'overall': {'early_stop': 20, 'max_epoch': 200, 'valid_converge': {'min_epoch': 5, 'eps': 1e-05}},\n",
      "                                'round': {'early_stop': 10, 'max_epoch': 100, 'valid_converge': {'min_epoch': 5, 'eps': 1e-05}}}},\n",
      " 'compt_params': {'cuda_first': True, 'num_worker': 10}}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time : ${2023-6-27} ${21:05}\n",
    "# @Author : Mathew Jin\n",
    "# @File : ${run_model.py}\n",
    "# chmod +x run_model.py\n",
    "# python3 scripts/run_model3.py --process=0 --rawname=1 --resume=0 --anchoring=0\n",
    "'''\n",
    "1.TRA\n",
    "https://arxiv.org/pdf/2106.12950.pdf\n",
    "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/src/model.py\n",
    "1.1 HIST\n",
    "https://arxiv.org/pdf/2110.13716.pdf\n",
    "https://github.com/Wentao-Xu/HIST\n",
    "2.Lightgbm\n",
    "https://github.com/microsoft/LightGBM/blob/master/examples/python-guide/plot_example.py\n",
    "https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.plot_tree.html\n",
    "3.other factors\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import itertools , os, shutil , gc , time , h5py , yaml\n",
    "from copy import deepcopy\n",
    "from torch.optim.swa_utils import AveragedModel , update_bn\n",
    "from scripts.util.environ import get_logger\n",
    "from scripts.util.basic import process_timer , FilteredIterator , lr_cosine_scheduler , versatile_storage\n",
    "from scripts.util.multiloss import multiloss_calculator \n",
    "from scripts.util.trainer import trainer_parser , train_config , set_trainer_environment , Device\n",
    "from scripts.data_util.ModelData import ModelData\n",
    "from scripts.function.basic import *\n",
    "from scripts.function.metric import loss_function,score_function,penalty_function\n",
    "from scripts.nn.My import *\n",
    "# from audtorch.metrics.functional import *\n",
    "\n",
    "try:\n",
    "    parser = trainer_parser().parse_args()\n",
    "except:\n",
    "    parser = trainer_parser().parse_args(args=[])\n",
    "\n",
    "logger = get_logger()\n",
    "config = train_config(parser = parser , do_process=True)\n",
    "set_trainer_environment(config)\n",
    "trainer_timer   = process_timer(False)\n",
    "trainer_storage = versatile_storage(config.storage_type)\n",
    "trainer_device  = Device(config.device)\n",
    "\n",
    "logger.warning('Model Specifics:')\n",
    "pretty_print_dict(config.get_dict([\n",
    "    'storage_type' , 'device' , 'precision' , 'batch_size' , 'model_name' , 'model_module' , 'model_data_type' , 'model_num' ,\n",
    "    'beg_date' , 'test_dates_end' , 'interval' , 'input_step_day' , 'test_step_day' , 'MODEL_PARAM' , 'train_params' , 'compt_params'\n",
    "]))\n",
    "\n",
    "class model_controller():\n",
    "    \"\"\"\n",
    "    A class to control the whole process of training , includes:\n",
    "    1. Parameters: train_params , compt_params , model_data_type\n",
    "    2. Data : class of train_data\n",
    "    3. loop status: model , round , attempt , epoch\n",
    "    4. file path: model , lastround , transfer(last model date)\n",
    "    5. text: model , round , attempt , epoch , exit , stat , time , trainer\n",
    "    \"\"\"\n",
    "    def __init__(self , **kwargs):\n",
    "        self.model_info = {'init_time' : time.time()}\n",
    "        \n",
    "    def main_process(self):\n",
    "        \"\"\"\n",
    "        Main process of load_data + train + test + instance\n",
    "        \"\"\"\n",
    "        for process_name in config.process_queue:\n",
    "            self.SetProcessName(process_name)\n",
    "            self.__getattribute__(f'model_process_{process_name.lower()}')()\n",
    "    \n",
    "    def SetProcessName(self , key = 'data'):\n",
    "        config.process_name = key.lower()\n",
    "        self.model_count = 0\n",
    "        self.epoch_count = 0\n",
    "        if config.process_name == 'data': \n",
    "            pass\n",
    "        elif config.process_name in ['train' , 'test' , 'instance']: \n",
    "            self.data.reset_dataloaders()\n",
    "            self.metric_function = {\n",
    "                'loss'    : loss_function(config.train_params['criterion']['loss']) , \n",
    "                'score'   : {k:score_function(v) for k,v in config.train_params['criterion']['score'].items()} ,\n",
    "                'penalty' : {k:penalty_function(k,v) for k,v in config.train_params['criterion']['penalty'].items()}\n",
    "            }\n",
    "        else:\n",
    "            raise Exception(f'KeyError : {key}')\n",
    "        \n",
    "    def model_process_data(self):\n",
    "        \"\"\"\n",
    "        Main process of loading basic data\n",
    "        \"\"\"\n",
    "        self.model_info['data_time'] = time.time()\n",
    "        logger.critical(f'Start Process [Load Data]!')\n",
    "        self.data = ModelData(config.model_data_type , config)\n",
    "        # retrieve from data object\n",
    "        config.data_type_list  = self.data.data_type_list\n",
    "        config.model_date_list = self.data.model_date_list\n",
    "        config.test_full_dates = self.data.test_full_dates\n",
    "        config.test_dates_end  = self.data.test_full_dates[-1]\n",
    "        input_dim = tuple(self.data.feat_dims[mdt] for mdt in config.data_type_list)\n",
    "        for smp in config.model_params: \n",
    "            smp.update({'input_dim':input_dim if len(input_dim) > 1 else input_dim[0]})\n",
    "        logger.critical('Finish Process [Load Data]! Cost {:.1f}Secs'.format(time.time() - self.model_info['data_time']))\n",
    "        \n",
    "    def model_process_train(self):\n",
    "        \"\"\"\n",
    "        Main process of training\n",
    "        1. loop over model(model_date , model_num)\n",
    "        2. loop over round(if necessary) , attempt(if converge too soon) , epoch(most prevailing loops)\n",
    "        \"\"\"\n",
    "        self.model_info['train_time'] = time.time()\n",
    "        logger.critical(f'Start Process [Train Model]!')\n",
    "        torch.save(config.model_params , f'{config.model_base_path}/model_params.pt')    \n",
    "        for model_date , model_num in self.ModelIter():\n",
    "            self.model_date , self.model_num = model_date , model_num\n",
    "            self.ModelPreparation('train')\n",
    "            self.TrainModel()\n",
    "        total_time = time.time() - self.model_info['train_time']\n",
    "        self.model_info['train_process'] = 'Finish Process [Train Model]! Cost {:.1f} Hours, {:.1f} Min/model, {:.1f} Sec/Epoch'.format(\n",
    "            total_time / 3600 , total_time / 60 / max(self.model_count , 1) , total_time / max(self.epoch_count , 1))\n",
    "        logger.critical(self.model_info['train_process'])\n",
    "\n",
    "    def model_process_test(self):\n",
    "        self.model_info['test_time'] = time.time()\n",
    "        logger.critical(f'Start Process [Test Model]!')        \n",
    "        logger.warning(f'Each Model Date Testing Mean Score({config.train_params[\"criterion\"][\"score\"]}):')\n",
    "        self.test_result_model_num = np.repeat(config.model_num_list,len(config.output_types))\n",
    "        self.test_result_output_type = np.tile(config.output_types,len(config.model_num_list))\n",
    "        logger.info('{: <11s}'.format('Models') + ('{: >8d}'*len(self.test_result_model_num)).format(*self.test_result_model_num))\n",
    "        logger.info('{: <11s}'.format('Output') + ('{: >8s}'*len(self.test_result_model_num)).format(*self.test_result_output_type))\n",
    "        for model_date , model_num in self.ModelIter():\n",
    "            self.model_date , self.model_num = model_date , model_num\n",
    "            self.ModelPreparation('test')\n",
    "            self.TestModel()\n",
    "        self.ModelResult()\n",
    "        self.model_info['test_process'] = 'Finish Process [Test Model]! Cost {:.1f} Secs'.format(time.time()-self.model_info['test_time'])\n",
    "        logger.critical(self.model_info['test_process'])\n",
    "\n",
    "    def model_process_instance(self):\n",
    "        if config.anchoring < 0:\n",
    "            _text , _cond = ask_for_confirmation(f'Do you want to copy the model to instance?[yes/else no]: ' , timeout = -1)\n",
    "            anchoring = all([_t.lower() in ['yes','y'] for _t in _text])\n",
    "        else:\n",
    "            anchoring = config.anchoring > 0\n",
    "\n",
    "        if anchoring:\n",
    "            self.model_info['instance_time'] = time.time()\n",
    "            logger.critical(f'Start Process [Copy to Instance]!')        \n",
    "            if os.path.exists(config.instance_path): \n",
    "                logger.critical(f'Old instance {config.instance_path} exists , remove manually first to override!')\n",
    "                logger.critical(f'The command can be \"rm -r {config.instance_path}\"')\n",
    "                return\n",
    "            else:\n",
    "                shutil.copytree(config.model_base_path , config.instance_path)\n",
    "        else:\n",
    "            logger.critical(f'Will not copy to instance!')\n",
    "            return\n",
    "                \n",
    "        logger.warning('Copy from model to instance finished , Start going forward')\n",
    "        self.InstanceStart()\n",
    "        for model_date , model_num in self.ModelIter():\n",
    "            self.model_date , self.model_num = model_date , model_num\n",
    "            self.ModelPreparation('instance')\n",
    "            self.TestModel()\n",
    "            self.StorePreds()\n",
    "        self.ModelResult()\n",
    "        logger.critical('Finish Process [Copy to Instance]! Cost {:.1f} Secs'.format(time.time() - self.model_info['instance_time']))  \n",
    "\n",
    "    def ModelIter(self):\n",
    "        model_iter = list(itertools.product(config.model_date_list , config.model_num_list))\n",
    "        if config.resume_training and (config.process_name == 'train'):\n",
    "            models_trained = np.full(len(model_iter) , True)\n",
    "            for i,(model_date,model_num) in enumerate(model_iter):\n",
    "                if not os.path.exists(f'{config.model_base_path}/{model_num}/{model_date}.pt'):\n",
    "                    models_trained[max(i-1,0):] = False\n",
    "                    break\n",
    "            model_iter = FilteredIterator(model_iter , models_trained == 0)\n",
    "        return model_iter\n",
    "    \n",
    "    def ModelPreparation(self , process , last_n = 30 , best_n = 5):\n",
    "        assert process in ['train' , 'test' , 'instance']\n",
    "        with trainer_timer('ModelPreparation' , process):\n",
    "            param = config.model_params[self.model_num]\n",
    "\n",
    "            # In a new model , alters the penalty function's lamb\n",
    "            if 'hidden_orthogonality' in self.metric_function['penalty'].keys():\n",
    "                self.metric_function['penalty']['hidden_orthogonality']['cond'] = (param.get('hidden_as_factors') == True) or config.tra_model\n",
    "            if 'tra_ot_penalty' in self.metric_function['penalty'].keys(): \n",
    "                self.metric_function['penalty']['tra_ot_penalty']['cond'] = config.tra_model\n",
    "\n",
    "            path_prefix = '{}/{}'.format(param.get('path') , self.model_date)\n",
    "            path = {k:f'{path_prefix}.{k}.pt' for k in config.output_types} #['best','swalast','swabest']\n",
    "            path.update({f'src_model.{k}':[] for k in config.output_types})\n",
    "            if 'swalast' in config.output_types: \n",
    "                path['lastn'] = [f'{path_prefix}.lastn.{i}.pt' for i in range(last_n)]\n",
    "            if 'swabest' in config.output_types: \n",
    "                path['bestn'] = [f'{path_prefix}.bestn.{i}.pt' for i in range(best_n)]\n",
    "                path['bestn_score'] = [-10000. for i in range(best_n)]\n",
    "            \n",
    "            if config.train_params['transfer'] and self.model_date > config.model_date_list[0]:\n",
    "                path['transfer'] = '{}/{}.best.pt'.format(param.get('path') , max([d for d in config.model_date_list if d < self.model_date])) \n",
    "                \n",
    "            self.param , self.path = param , path\n",
    "    \n",
    "    def TrainModel(self):\n",
    "        self.TrainModelStart()\n",
    "        while self.cond.get('loop_status') != 'model':\n",
    "            self.NewLoop()\n",
    "            self.TrainerInit()\n",
    "            self.TrainEpoch()\n",
    "            self.LoopCondition()\n",
    "        self.TrainModelEnd()\n",
    "        gc.collect() , torch.cuda.empty_cache()\n",
    "    \n",
    "    def TestModel(self):\n",
    "        self.TestModelStart()\n",
    "        self.Forecast()\n",
    "        self.TestModelEnd()\n",
    "        gc.collect() , torch.cuda.empty_cache()\n",
    "        \n",
    "    def _init_variables(self , key = 'model'):\n",
    "        \"\"\"\n",
    "        Reset variables of 'model' , 'round' , 'attempt' start\n",
    "        \"\"\"\n",
    "        if key == 'epoch' : return\n",
    "        assert key in ['model' , 'round' , 'attempt'] , f'KeyError : {key}'\n",
    "\n",
    "        self.epoch_i = -1\n",
    "        self.epoch_attempt_best = -1\n",
    "        self.score_attempt_best = -10000.\n",
    "        self.loss_list  = {'train' : [] , 'valid' : []}\n",
    "        self.score_list = {'train' : [] , 'valid' : []}\n",
    "        self.lr_list    = []\n",
    "        \n",
    "        if key in ['model' , 'round']:\n",
    "            self.attempt_i = -1\n",
    "            self.score_round_best = -10000.\n",
    "        \n",
    "        if key in ['model']:\n",
    "            self.round_i = -1\n",
    "            self.epoch_all = -1\n",
    "            self.tick = np.ones(10) * time.time()\n",
    "            self.text = {k : '' for k in ['model','round','attempt','epoch','exit','stat','time','trainer']}\n",
    "            self.cond = {'terminate' : {} , 'nan_loss' : False , 'loop_status' : 'round'}\n",
    "\n",
    "    def TrainModelStart(self):\n",
    "        \"\"\"\n",
    "        Reset model specific variables\n",
    "        \"\"\"\n",
    "        with trainer_timer('TrainModelStart'):\n",
    "            self._init_variables('model')\n",
    "            self.nanloss_life = config.train_params['trainer']['nanloss']['retry']\n",
    "            self.text['model'] = '{:s} #{:d} @{:4d}'.format(config.model_name , self.model_num , self.model_date)\n",
    "            if (self.data.dataloader_param != (self.model_date , self.param['seqlens'])):\n",
    "                self.data.create_dataloader(config.process_name , 'train' , self.model_date , self.param['seqlens']) \n",
    "                self.tick[1] = time.time()\n",
    "                self.printer('train_dataloader')\n",
    "            \n",
    "    def TrainModelEnd(self):\n",
    "        \"\"\"\n",
    "        Do necessary things of ending a model(model_data , model_num)\n",
    "        \"\"\"\n",
    "        with trainer_timer('TrainModelEnd'):\n",
    "            trainer_storage.del_path(self.path.get('rounds') , self.path.get('lastn') , self.path.get('bestn'))\n",
    "            if config.process_name == 'train' : self.model_count += 1\n",
    "            self.tick[2] = time.time()\n",
    "            self.printer('model_end')\n",
    "\n",
    "    def NewLoop(self):\n",
    "        \"\"\"\n",
    "        Reset and loop variables giving loop_status\n",
    "        \"\"\"\n",
    "        with trainer_timer('NewLoop'):\n",
    "            self._init_variables(self.cond.get('loop_status'))\n",
    "            self.epoch_i += 1\n",
    "            self.epoch_all += 1\n",
    "            self.epoch_count += 1\n",
    "            if self.cond.get('loop_status') in ['attempt' , 'round']:\n",
    "                self.attempt_i += 1\n",
    "                self.text['attempt'] = f'FirstBite' if self.attempt_i == 0 else f'Retrain#{self.attempt_i}'\n",
    "            if self.cond.get('loop_status') in ['round']:\n",
    "                self.round_i += 1\n",
    "                self.text['round'] = 'Round{:2d}'.format(self.round_i)\n",
    "        \n",
    "    def TrainerInit(self):\n",
    "        \"\"\"\n",
    "        Initialize net , optimizer , scheduler if loop_status in ['round' , 'attempt']\n",
    "        net : 1. Create an instance of f'My{config.model_module}' or inherit from 'lastround'/'transfer'\n",
    "              2. In transfer mode , p_late and p_early with be trained with different lr's. If not net.parameters are trained by same lr\n",
    "        optimizer : Adam or SGD\n",
    "        scheduler : Cosine or StepLR\n",
    "        \"\"\"\n",
    "        with trainer_timer('TrainerInit'):\n",
    "            if self.cond.get('loop_status') == 'epoch': return\n",
    "            self.load_model('train')\n",
    "            self.max_round = self.net.max_round() if 'max_round' in self.net.__dir__() else 1\n",
    "            self.optimizer = self.load_optimizer()\n",
    "            self.scheduler = self.load_scheduler() \n",
    "            self.multiloss = self.load_multiloss()\n",
    "        \n",
    "    def TrainEpoch(self):\n",
    "        \"\"\"\n",
    "        Iterate train and valid dataset, calculate loss/score , update values\n",
    "        If nan loss occurs, turn to _deal_nanloss\n",
    "        \"\"\"\n",
    "        with trainer_timer('TrainEpoch/train_epochs'):\n",
    "            self.net.train()\n",
    "            clip_value = config.train_params['trainer']['gradient'].get('clip_value')\n",
    "            iterator = self.data.dataloaders['train']\n",
    "            _loss , _score = np.full(len(iterator),np.nan) , np.full(len(iterator),np.nan)\n",
    "            for i , batch_data in enumerate(iterator):\n",
    "                x = self.modifier['inputs'](batch_data['x'] , batch_data , self.data)\n",
    "                self.optimizer.zero_grad()\n",
    "                with trainer_timer('TrainEpoch/train/forward'):\n",
    "                    pred , hidden = self.net(x)\n",
    "                with trainer_timer('TrainEpoch/train/loss'):\n",
    "                    penalty_kwargs = {'net' : self.net , 'hidden' : hidden , 'label' : batch_data['y']}\n",
    "                    metric = self.metric_calculator(batch_data['y'] , pred , 'train' , weight = batch_data['w'] , **penalty_kwargs)\n",
    "                    metric = self.modifier['metric'](metric, batch_data, self.data)\n",
    "                with trainer_timer('TrainEpoch/train/backward'):\n",
    "                    metric['loss'].backward()\n",
    "                with trainer_timer('TrainEpoch/train/step'):\n",
    "                    if clip_value is not None : nn.utils.clip_grad_value_(self.net.parameters(), clip_value = clip_value)\n",
    "                    self.optimizer.step()\n",
    "                self.modifier['update'](None , batch_data , self.data)\n",
    "                _loss[i] , _score[i] = metric['loss_item'] , metric['score']\n",
    "                if iterator.progress_bar: iterator.display(f'Ep#{self.epoch_i:3d} train loss:{np.mean(_loss[:i+1]):.5f}')\n",
    "            if np.isnan(sum(_loss)): return self._deal_nanloss()\n",
    "            self.loss_list['train'].append(np.mean(_loss)) , self.score_list['train'].append(np.mean(_score))\n",
    "        \n",
    "        with trainer_timer('TrainEpoch/valid_epochs'):\n",
    "            self.net.eval()     \n",
    "            iterator = self.data.dataloaders['valid']\n",
    "            _loss , _score = np.full(len(iterator),np.nan) , np.full(len(iterator),np.nan)\n",
    "            for i , batch_data in enumerate(iterator):\n",
    "                x = self.modifier['inputs'](batch_data['x'] , batch_data , self.data)\n",
    "                # trainer_device.print_cuda_memory()\n",
    "                with trainer_timer('TrainEpoch/valid/forward'):\n",
    "                    pred , _ = self.net(x)\n",
    "                with trainer_timer('TrainEpoch/valid/loss'):\n",
    "                    metric = self.metric_calculator(batch_data['y'] , pred , 'valid' , weight = batch_data['w'])\n",
    "                    metric = self.modifier['metric'](metric, batch_data, self.data)\n",
    "                self.modifier['update'](None , batch_data , self.data)\n",
    "                _loss[i] , _score[i] = metric['loss_item'] , metric['score']\n",
    "                if iterator.progress_bar: iterator.display(f'Ep#{self.epoch_i:3d} valid ic:{np.mean(_score[:i+1]):.5f}')\n",
    "            self.loss_list['valid'].append(np.mean(_loss)) , self.score_list['valid'].append(np.mean(_score))\n",
    "\n",
    "        self.lr_list.append(self.scheduler.get_last_lr()[0])\n",
    "        self.scheduler.step()\n",
    "        self.reset_scheduler()\n",
    "\n",
    "    def LoopCondition(self):\n",
    "        \"\"\"\n",
    "        Update condition of continuing training epochs , restart attempt if early exit , proceed to next round if convergence , reset round if nan loss\n",
    "        \"\"\"\n",
    "        with trainer_timer('LoopCondition/assess'):\n",
    "            if self.cond['nan_loss']:\n",
    "                logger.error(f'Initialize a new model to retrain! Lives remaining {self.nanloss_life}')\n",
    "                self._init_variables('model')\n",
    "                self.cond['loop_status'] = 'round'\n",
    "                return\n",
    "                \n",
    "            valid_score = self.score_list['valid'][-1]\n",
    "            \n",
    "            save_targets = [] \n",
    "            if valid_score > self.score_attempt_best: \n",
    "                self.epoch_attempt_best = self.epoch_i \n",
    "                self.score_attempt_best = valid_score\n",
    "                \n",
    "            if valid_score > self.score_round_best:\n",
    "                self.score_round_best = valid_score\n",
    "                self.path['src_model.best']  = [self.path['best']]\n",
    "                save_targets.append(self.path['best'])\n",
    "\n",
    "            if 'swalast' in config.output_types:\n",
    "                self.path['lastn'] = self.path['lastn'][1:] + self.path['lastn'][:1]\n",
    "                save_targets.append(self.path['lastn'][-1])\n",
    "                \n",
    "                p_valid = self.path['lastn'][-len(self.score_list['valid']):]\n",
    "                arg_max = np.argmax(self.score_list['valid'][-len(p_valid):])\n",
    "                arg_swa = (lambda x:x[(x>=0) & (x<len(p_valid))])(min(5,len(p_valid)//3)*np.arange(-5,3)+arg_max)[-5:]\n",
    "                self.path['src_model.swalast'] = [p_valid[i] for i in arg_swa]\n",
    "                \n",
    "            if 'swabest' in config.output_types:\n",
    "                arg_min = np.argmin(self.path['bestn_score'])\n",
    "                if valid_score > self.path['bestn_score'][arg_min]:\n",
    "                    self.path['bestn_score'][arg_min] = valid_score\n",
    "                    save_targets.append(self.path['bestn'][arg_min])\n",
    "                    if self.path['bestn'][arg_min] not in self.path['src_model.swabest']: self.path['src_model.swabest'].append(self.path['bestn'][arg_min])\n",
    "                \n",
    "            trainer_storage.save_model_state(self.net , save_targets)\n",
    "            self.printer('epoch_step')\n",
    "        \n",
    "        with trainer_timer('LoopCondition/confirm_status'):\n",
    "            self.cond['terminate'] = {k:self._terminate_cond(k,v) for k , v in config.train_params['terminate'].get('overall' if self.max_round <= 1 else 'round').items()}\n",
    "            if any(self.cond.get('terminate').values()):\n",
    "                self.text['exit'] = {\n",
    "                    'max_epoch'      : 'Max Epoch' , \n",
    "                    'early_stop'     : 'EarlyStop' ,\n",
    "                    'tv_converge'    : 'T&V Convg' , \n",
    "                    'train_converge' : 'Tra Convg' , \n",
    "                    'valid_converge' : 'Val Convg' ,\n",
    "                }[[k for k,v in self.cond.get('terminate').items() if v][0]] \n",
    "                if (self.epoch_i < config.train_params['trainer']['retrain'].get('min_epoch' if self.max_round <= 1 else 'min_epoch_round') - 1 and \n",
    "                    self.attempt_i < config.train_params['trainer']['retrain']['attempts'] - 1):\n",
    "                    self.cond['loop_status'] = 'attempt'\n",
    "                    self.printer('new_attempt')\n",
    "                elif self.round_i < self.max_round - 1:\n",
    "                    self.cond['loop_status'] = 'round'\n",
    "                    self.save_model('best')\n",
    "                    self.printer('new_round')\n",
    "                else:\n",
    "                    self.cond['loop_status'] = 'model'\n",
    "                    self.save_model(config.output_types)\n",
    "            else:\n",
    "                self.cond['loop_status'] = 'epoch'\n",
    "            \n",
    "    def TestModelStart(self):\n",
    "        \"\"\"\n",
    "        Reset model specific variables\n",
    "        \"\"\"\n",
    "        self._init_variables('model')\n",
    "        dataloader_param = (config.process_name , 'test' , self.model_date , self.param['seqlens'])   \n",
    "        if (self.data.dataloader_param != dataloader_param):\n",
    "            self.data.create_dataloader(*dataloader_param)\n",
    "            \n",
    "        if self.model_num == 0:\n",
    "            score_date  = np.zeros((len(self.data.model_test_dates) , len(self.test_result_model_num)))\n",
    "            score_model = np.zeros((1 , len(self.test_result_model_num)))\n",
    "            self.score_by_date  = np.concatenate([getattr(self,'score_by_date' ,np.empty((0,len(self.test_result_model_num)))) , score_date])\n",
    "            self.score_by_model = np.concatenate([getattr(self,'score_by_model',np.empty((0,len(self.test_result_model_num)))) , score_model])\n",
    "            #self.score_by_date  = score_date  if self.score_by_date  is None else np.concatenate([self.score_by_date , score_date])\n",
    "            #self.score_by_model = score_model if self.score_by_model is None else np.concatenate([self.score_by_model, score_model])\n",
    "                \n",
    "    def Forecast(self):\n",
    "        if not os.path.exists(self.path['best']): self.TrainModel()\n",
    "        with trainer_timer('TestModel/Forcast') , torch.no_grad():\n",
    "            #self.y_pred = cuda(torch.zeros(len(self.data.index[0]),len(self.data.model_test_dates),self.data.labels_n,len(config.output_types)).fill_(np.nan))\n",
    "            self.y_pred = trainer_device.torch_nans(len(self.data.index[0]), len(self.data.model_test_dates), len(config.output_types))\n",
    "            iter_dates = np.concatenate([self.data.early_test_dates , self.data.model_test_dates])\n",
    "            assert self.data.dataloaders['test'].__len__() == len(iter_dates)\n",
    "            for oi , okey in enumerate(config.output_types):\n",
    "                self.load_model('test' , okey)\n",
    "                self.net.eval()\n",
    "                iterator = self.data.dataloaders['test']\n",
    "                test_score = np.full(len(iter_dates),np.nan)\n",
    "                for i , batch_data in enumerate(iterator):\n",
    "                    nonnan = torch.where(batch_data['nonnan'])[0]\n",
    "                    pred = torch.full_like(batch_data['y'], fill_value=torch.nan)\n",
    "                    for batch_j in torch.utils.data.DataLoader(trainer_device.torch_arange(len(nonnan)) , batch_size = config.batch_size):\n",
    "                        nnj = nonnan[batch_j]\n",
    "                        batch_nnj = subset(batch_data , nnj)\n",
    "                        x = self.modifier['inputs'](batch_nnj['x'] , batch_nnj , self.data)\n",
    "                        pred_nnj = self.net(x)[0].detach()\n",
    "                        pred[nnj,0] = pred_nnj[:,0]\n",
    "                        self.modifier['update'](None , batch_nnj , self.data)\n",
    "                    \n",
    "                    if i >= len(self.data.early_test_dates):\n",
    "                        # before this date is warmup stage\n",
    "                        self.y_pred[:,i-len(self.data.early_test_dates),oi] = pred[:,0]\n",
    "                        metric = self.metric_calculator(batch_data['y'],pred,'test',weight=batch_data['w'],valid_sample=nonnan)\n",
    "                        test_score[i] = metric['score']\n",
    "                    if (i + 1) % 20 == 0 : torch.cuda.empty_cache()\n",
    "                    if iterator.progress_bar: iterator.display(f'Date#{i-len(self.data.early_test_dates):3d} :{np.mean(test_score[i+1]):.5f}')\n",
    "                self.score_by_date[-len(self.data.model_test_dates):,self.model_num*len(config.output_types) + oi] = np.nan_to_num(test_score[-len(self.data.model_test_dates):])\n",
    "            self.y_pred = self.y_pred.cpu().numpy()\n",
    "        \n",
    "    def TestModelEnd(self):\n",
    "        \"\"\"\n",
    "        Do necessary things of ending a model(model_data , model_num)\n",
    "        \"\"\"\n",
    "        if self.model_num == config.model_num_list[-1]:\n",
    "            self.score_by_model[-1,:] = np.nanmean(self.score_by_date[-len(self.data.model_test_dates):,],axis = 0)\n",
    "            logger.info('{: <11d}'.format(self.model_date)+('{:>8.4f}'*len(self.test_result_model_num)).format(*self.score_by_model[-1,:]))\n",
    "        #if False:\n",
    "        #    df = pd.DataFrame(self.y_pred.T, index = self.data.model_test_dates, columns = self.data.secid.astype(str))\n",
    "        #    with open(f'{config.instance_path}/{config.model_name}_fac{self.model_num}.csv', 'a') as f:\n",
    "        #        df.to_csv(f , mode = 'a', header = f.tell()==0, index = True)\n",
    "\n",
    "    def ResultOutput(self):\n",
    "        out_dict = {\n",
    "            '0_start' : time.ctime(self.model_info.get('init_time')),\n",
    "            '1_basic' :'+'.join(['short' if config.shorttest else 'long' , config.storage_type , config.precision]),\n",
    "            '2_model' :''.join([config.model_module , '_' , config.model_data_type , '(x' , str(config.model_num) , ')']),\n",
    "            '3_time'  :'-'.join([str(config.beg_date),str(config.end_date)]),\n",
    "            '4_typeNN':'+'.join(list(set(config.MODEL_PARAM['type_rnn']))),\n",
    "            '5_train' :self.model_info.get('train_process'),\n",
    "            '6_test'  :self.model_info.get('test_process'),\n",
    "            '7_result':self.model_info.get('test_score_sum'),\n",
    "        }\n",
    "        out_path = f'./results/model_results.yaml'\n",
    "        os.makedirs(os.path.dirname(out_path) , exist_ok=True)\n",
    "        with open(out_path , 'a' if os.path.exists(out_path) else 'w') as f:\n",
    "            yaml.dump(out_dict , f)\n",
    "\n",
    "    def StorePreds(self):\n",
    "        assert config.process_name == 'instance'\n",
    "        if self.model_num == 0:\n",
    "            self.y_pred_models = []\n",
    "            gc.collect()\n",
    "        self.y_pred_models.append(self.y_pred)\n",
    "        if self.model_num == config.model_num_list[-1]:\n",
    "            self.y_pred_models = np.concatenate(self.y_pred_models,axis=-1).transpose(1,0,2)\n",
    "            # idx = np.array(np.meshgrid(self.data.model_test_dates , self.data.sec_id)).T.reshape(-1,2)\n",
    "            mode = 'r+' if os.path.exists(f'{config.instance_path}/{config.model_name}.h5') else 'w'\n",
    "            with h5py.File(f'{config.instance_path}/{config.model_name}.h5' , mode = mode) as f:\n",
    "                for di in range(len(self.data.model_test_dates)):\n",
    "                    arr , row = self.y_pred_models[di] , self.data.sec_id \n",
    "                    arr , row = arr[np.isnan(arr).all(axis=1) == 0] , row[np.isnan(arr).all(axis=1) == 0]\n",
    "                    col = [f'{mn}.{o}' for mn,o in zip(self.test_result_model_num,self.test_result_output_type)]\n",
    "                    if str(self.data.model_test_dates[di]) in f.keys():\n",
    "                        del f[str(self.data.model_test_dates[di])]\n",
    "                    g = f.create_group(str(self.data.model_test_dates[di]))\n",
    "                    g.create_dataset('arr' , data=arr , compression='gzip')\n",
    "                    g.create_dataset('row' , data=row , compression='gzip')\n",
    "                    g.create_dataset('col' , data=col , compression='gzip')     \n",
    "  \n",
    "    def ModelResult(self):\n",
    "        # date ic writed down\n",
    "        date_step = (1 if config.process_name == 'instance' else self.data.test_step)\n",
    "        date_list = config.test_full_dates[::date_step]\n",
    "        for model_num in config.model_num_list:\n",
    "            df = pd.DataFrame({'dates' : date_list} , index = map(lambda x:f'{x[:4]}-{x[4:6]}-{x[6:]}' , date_list.astype(str)))\n",
    "            for oi , okey in enumerate(config.output_types):\n",
    "                df[f'score.{okey}'] = self.score_by_date[:,model_num*len(config.output_types) + oi]\n",
    "                df[f'cum_score.{okey}'] = np.nancumsum(self.score_by_date[:,model_num*len(config.output_types) + oi])\n",
    "            df.to_csv(config.model_params[model_num]['path'] + f'/{config.model_name}_score_by_date_{model_num}.csv')\n",
    "\n",
    "        # model ic presentation\n",
    "        add_row_key   = ['AllTimeAvg' , 'AllTimeSum' , 'Std'      , 'TValue'   , 'AnnIR']\n",
    "        add_row_fmt   = ['{:>8.4f}'   , '{:>8.2f}'   , '{:>8.4f}' , '{:>8.2f}' , '{:>8.4f}']\n",
    "        score_mean   = np.nanmean(self.score_by_date , axis = 0)\n",
    "        score_sum    = np.nansum(self.score_by_date , axis = 0) \n",
    "        score_std    = np.nanstd(self.score_by_date , axis = 0)\n",
    "        score_tvalue = score_mean / score_std * (len(self.score_by_date)**0.5) # 10 days return predicted\n",
    "        score_annir  = score_mean / score_std * ((240 / 10)**0.5) # 10 days return predicted\n",
    "        add_row_value = (score_mean , score_sum , score_std , score_tvalue , score_annir)\n",
    "        df = pd.DataFrame(np.concatenate([self.score_by_model , np.stack(add_row_value)]) , \n",
    "                          index = [str(d) for d in config.model_date_list] + add_row_key , \n",
    "                          columns = [f'{mn}.{o}' for mn,o in zip(self.test_result_model_num,self.test_result_output_type)])\n",
    "        df.to_csv(f'{config.model_base_path}/{config.model_name}_score_by_model.csv')\n",
    "        for i in range(len(add_row_key)):\n",
    "            logger.info('{: <11s}'.format(add_row_key[i]) + (add_row_fmt[i]*len(self.test_result_model_num)).format(*add_row_value[i]))\n",
    "        self.model_info['test_score_sum'] = {k:v for k,v in zip(df.columns , score_sum.tolist())}\n",
    "\n",
    "    def InstanceStart(self):\n",
    "        config.reload(config_path = f'{config.instance_path}/config_train.yaml')\n",
    "    \n",
    "    def printer(self , key):\n",
    "        \"\"\"\n",
    "        Print out status giving display conditions and looping conditions\n",
    "        \"\"\"\n",
    "        printer = [logger.info] if (config.verbosity > 2 or self.model_count < config.model_num) else [logger.debug]\n",
    "        sdout   = None\n",
    "        if key == 'model_end':\n",
    "            self.text['epoch'] = 'Ep#{:3d}'.format(self.epoch_all)\n",
    "            self.text['stat']  = 'Train{: .4f} Valid{: .4f} BestVal{: .4f}'.format(self.score_list['train'][-1],self.score_list['valid'][-1],self.score_round_best)\n",
    "            self.text['time']  = 'Cost{:5.1f}Min,{:5.1f}Sec/Ep'.format((self.tick[2]-self.tick[0])/60 , (self.tick[2]-self.tick[1])/(self.epoch_all+1))\n",
    "            sdout = self.text['model'] + '|' + self.text['round'] + ' ' + self.text['attempt'] + ' ' + \\\n",
    "                    self.text['epoch'] + ' ' + self.text['exit'] + '|' + self.text['stat'] + '|' + self.text['time']\n",
    "            printer = [logger.warning]\n",
    "        elif key == 'epoch_step':\n",
    "            self.text['trainer'] = 'loss {: .5f}, train{: .5f}, valid{: .5f}, max{: .4f}, best{: .4f}, lr{:.1e}'.format(\n",
    "                self.loss_list['train'][-1] , self.score_list['train'][-1] , self.score_list['valid'][-1] , \n",
    "                self.score_attempt_best , self.score_round_best , self.lr_list[-1])\n",
    "            if self.epoch_i % [10,5,5,3,3,1][min(config.verbosity // 2 , 5)] == 0:\n",
    "                sdout = ' '.join([self.text['attempt'],'Ep#{:3d}'.format(self.epoch_i),':', self.text['trainer']])\n",
    "        elif key == 'reset_learn_rate':\n",
    "            sdout = 'Reset learn rate and scheduler at the end of epoch {} , effective at epoch {}'.format(\n",
    "                self.epoch_i , self.epoch_i+1 , ', and will speedup2x' * config.train_params['trainer']['learn_rate']['reset']['speedup2x'])\n",
    "        elif key == 'new_attempt':\n",
    "            sdout = ' '.join([self.text['attempt'],'Epoch #{:3d}'.format(self.epoch_i),':',self.text['trainer'],', Next attempt goes!'])\n",
    "        elif key == 'new_round':\n",
    "            sdout = self.text['round'] + ' ' + self.text['exit'] + ': ' + self.text['trainer'] + ', Next round goes!'\n",
    "        elif key == 'train_dataloader':\n",
    "            sdout = ' '.join([self.text['model'],'LoadData Cost {:>6.1f}Secs'.format(self.tick[1]-self.tick[0])])  \n",
    "        else:\n",
    "            raise Exception(f'KeyError : {key}')\n",
    "        \n",
    "        for prt in printer:\n",
    "            if sdout is not None: prt(sdout)        \n",
    "            \n",
    "    def metric_calculator(self, labels , pred , key , weight = None , valid_sample = None , **penalty_kwargs):\n",
    "        \"\"\"\n",
    "        Calculate loss(with gradient), score\n",
    "        Inputs : \n",
    "            kwargs : other inputs used in calculating loss , penalty and score\n",
    "        Possible Methods :\n",
    "        loss:    pearsonr , mse , ccc\n",
    "        penalty: hidden_orthogonality , tra_ot_penalty\n",
    "        score:  pearson , spearman , mse , ccc\n",
    "        \"\"\"\n",
    "        assert key in ['train' , 'valid' , 'test'] , key\n",
    "        if labels.shape != pred.shape:\n",
    "            # if more labels than output\n",
    "            assert labels.shape[:-1] == pred.shape[:-1] , (labels.shape , pred.shape)\n",
    "            labels = labels.transpose(0,-1)[:pred.shape[-1]].transpose(0,-1)\n",
    "        if valid_sample is not None:\n",
    "            labels , pred = labels[valid_sample] , pred[valid_sample]\n",
    "            if weight is not None: weight = weight[valid_sample]\n",
    "        score_dim = lambda x:None if x is None else x.select(-1,0)\n",
    "        if key == 'train':\n",
    "            if self.param['num_output'] > 1:\n",
    "                loss = self.metric_function['loss'](labels , pred , weight , dim = 0)[:self.param['num_output']]\n",
    "                loss = self.multiloss.calculate_multi_loss(loss , self.net.get_multiloss_params())\n",
    "            else:\n",
    "                loss = self.metric_function['loss'](score_dim(labels) , score_dim(pred) , score_dim(weight))\n",
    "            for _pen_dict in self.metric_function['penalty'].values():\n",
    "                if _pen_dict['lamb'] > 0 and _pen_dict['cond']: \n",
    "                    loss = loss + _pen_dict['lamb'] * _pen_dict['func'](**penalty_kwargs)  \n",
    "            loss_item = loss.item()\n",
    "        else:\n",
    "            loss_item = loss = 0.\n",
    "        score = self.metric_function['score'][key](score_dim(labels) , score_dim(pred) , score_dim(weight)).item()\n",
    "        return {'loss' : loss , 'loss_item' : loss_item , 'score' : score}\n",
    "    \n",
    "    def _deal_nanloss(self):\n",
    "        \"\"\"\n",
    "        Deal with nan loss, life -1 and change nan_loss condition to True\n",
    "        \"\"\"\n",
    "        logger.error(f'{self.text[\"model\"]} Attempt{self.attempt_i}, epoch{self.epoch_i} got nan loss!')\n",
    "        if self.nanloss_life > 0:\n",
    "            self.nanloss_life -= 1\n",
    "            self.cond['nan_loss'] = True\n",
    "        else:\n",
    "            raise Exception('Nan loss life exhausted, possible gradient explosion/vanish!')\n",
    "    \n",
    "    def _terminate_cond(self , key , arg):\n",
    "        \"\"\"\n",
    "        Whether terminate condition meets\n",
    "        \"\"\"\n",
    "        if key == 'early_stop':\n",
    "            return self.epoch_i - self.epoch_attempt_best >= arg\n",
    "        elif key == 'max_epoch':\n",
    "            return self.epoch_i >= min(arg , config.max_epoch) - 1\n",
    "        elif key == 'train_converge':\n",
    "            return list_converge(self.loss_list['train']  , arg.get('min_epoch') , arg.get('eps'))\n",
    "        elif key == 'valid_converge':\n",
    "            return list_converge(self.score_list['valid'] , arg.get('min_epoch') , arg.get('eps'))\n",
    "        elif key == 'tv_converge':\n",
    "            return (list_converge(self.loss_list['train']  , arg.get('min_epoch') , arg.get('eps')) and \n",
    "                    list_converge(self.score_list['valid'] , arg.get('min_epoch') , arg.get('eps')))\n",
    "        else:\n",
    "            raise Exception(f'KeyError : {key}')\n",
    "    \n",
    "    def save_model(self , key = 'best'):\n",
    "        if isinstance(key , (list,tuple)):\n",
    "            [self.save_model(k) for k in key]\n",
    "        else:\n",
    "            assert key in ['best' , 'swalast' , 'swabest']\n",
    "            with trainer_timer('save_model'):\n",
    "                if key == 'best':\n",
    "                    model_state = trainer_storage.load(self.path['best'])\n",
    "                    if self.round_i < self.max_round - 1:\n",
    "                        if 'rounds' not in self.path.keys():\n",
    "                            self.path['rounds'] = ['{}/{}.round.{}.pt'.format(self.param.get('path') , self.model_date , r) for r in range(self.max_round - 1)]\n",
    "                        # self.path[f'round.{self.round_i}'] = '{}/{}.round.{}.pt'.format(self.param.get('path') , self.model_date , self.round_i)\n",
    "                        trainer_storage.save(model_state , self.path['rounds'][self.round_i])\n",
    "                    trainer_storage.save(model_state , self.path['best'] , to_disk = True)\n",
    "                else:\n",
    "                    p_exists = trainer_storage.valid_paths(self.path[f'src_model.{key}'])\n",
    "                    if len(p_exists) == 0:\n",
    "                        print(key , self.path[f'bestn'] , self.path[f'bestn_score'] , self.path[f'src_model.{key}'])\n",
    "                        raise Exception(f'Model Error')\n",
    "                    else:\n",
    "                        model = self.swa_model(p_exists)\n",
    "                        trainer_storage.save_model_state(model , self.path[key] , to_disk = True) \n",
    "    \n",
    "    def load_model(self , process , key = 'best'):\n",
    "        assert process in ['train' , 'test' , 'instance']\n",
    "        with trainer_timer('load_model'):\n",
    "            net = globals()[f'My{config.model_module}'](**self.param)\n",
    "            if process == 'train':           \n",
    "                if self.round_i > 0:\n",
    "                    model_path = self.path['rounds'][self.round_i-1]\n",
    "                elif 'transfer' in self.path.keys():\n",
    "                    model_path = self.path['transfer']\n",
    "                else:\n",
    "                    model_path = -1\n",
    "                if os.path.exists(model_path): net = trainer_storage.load_model_state(net , model_path , from_disk = True)\n",
    "                if 'training_round' in net.__dir__(): net.training_round(self.round_i)\n",
    "            else:\n",
    "                net = trainer_storage.load_model_state(net , self.path[key] , from_disk = True)\n",
    "            net = trainer_device(net)\n",
    "            self.net = net\n",
    "            # default : none modifier\n",
    "            # input : (inputs/metric/update , batch_data , self.data)\n",
    "            # output : new_inputs/new_metric/None \n",
    "            self.modifier = {'inputs': lambda x,b,d:x, 'metric': lambda x,b,d:x, 'update': lambda x,b,d:None}\n",
    "            if 'modifier_inputs' in self.net.__dir__(): self.modifier['inputs'] = lambda x,b,d:self.net.modifier_inputs(x,b,d)\n",
    "            if 'modifier_metric' in self.net.__dir__(): self.modifier['metric'] = lambda x,b,d:self.net.modifier_metric(x,b,d)\n",
    "            if 'modifier_update' in self.net.__dir__(): self.modifier['update'] = lambda x,b,d:self.net.modifier_update(x,b,d)\n",
    "    \n",
    "    def swa_model(self , model_path_list = []):\n",
    "        net = globals()[f'My{config.model_module}'](**self.param)\n",
    "        swa_net = trainer_device(AveragedModel(net))\n",
    "        for p in model_path_list:\n",
    "            swa_net.update_parameters(trainer_storage.load_model_state(net , p))\n",
    "        update_bn(self._swa_update_bn_loader(self.data.dataloaders['train']) , swa_net)\n",
    "        return swa_net.module\n",
    "    \n",
    "    def _swa_update_bn_loader(self , loader):\n",
    "        for data in loader: yield [data['x'] , data['y'] , data['w']]\n",
    "    \n",
    "    def load_optimizer(self , new_opt_kwargs = None , new_lr_kwargs = None):\n",
    "        if new_opt_kwargs is None:\n",
    "            opt_kwargs = config.train_params['trainer']['optimizer']\n",
    "        else:\n",
    "            opt_kwargs = deepcopy(config.train_params['trainer']['optimizer'])\n",
    "            opt_kwargs.update(new_opt_kwargs)\n",
    "        \n",
    "        if new_lr_kwargs is None:\n",
    "            lr_kwargs = config.train_params['trainer']['learn_rate']\n",
    "        else:\n",
    "            lr_kwargs = deepcopy(config.train_params['trainer']['learn_rate'])\n",
    "            lr_kwargs.update(new_lr_kwargs)\n",
    "\n",
    "        base_lr = lr_kwargs['base'] * lr_kwargs['ratio']['attempt'][:self.attempt_i+1][-1] * lr_kwargs['ratio']['round'][:self.round_i+1][-1]\n",
    "        if 'transfer' in self.path.keys():\n",
    "            # define param list to train with different learn rate\n",
    "            p_enc = [(p if p.dim()<=1 else nn.init.xavier_uniform_(p)) for x,p in self.net.named_parameters() if 'encoder' in x.split('.')[:3]]\n",
    "            p_dec = [p for x,p in self.net.named_parameters() if 'encoder' not in x.split('.')[:3]]\n",
    "            self.net_param_gourps = [{'params': p_dec , 'lr': base_lr , 'lr_param' : base_lr},\n",
    "                                     {'params': p_enc , 'lr': base_lr * lr_kwargs['ratio']['transfer'] , 'lr_param': base_lr * lr_kwargs['ratio']['transfer']}]\n",
    "        else:\n",
    "            self.net_param_gourps = [{'params': [p for p in self.net.parameters()] , 'lr' : base_lr , 'lr_param' : base_lr} ]\n",
    "\n",
    "        optimizer = {\n",
    "            'Adam': torch.optim.Adam ,\n",
    "            'SGD' : torch.optim.SGD ,\n",
    "        }[opt_kwargs['name']](self.net_param_gourps , **opt_kwargs['param'])\n",
    "        return optimizer\n",
    "    \n",
    "    def load_scheduler(self , new_shd_kwargs = None):\n",
    "        if new_shd_kwargs is None:\n",
    "            shd_kwargs = config.train_params['trainer']['scheduler']\n",
    "        else:\n",
    "            shd_kwargs = deepcopy(config.train_params['trainer']['scheduler'])\n",
    "            shd_kwargs.update(new_shd_kwargs)\n",
    "\n",
    "        if shd_kwargs['name'] == 'cos':\n",
    "            scheduler = lr_cosine_scheduler(self.optimizer, **shd_kwargs['param'])\n",
    "        elif shd_kwargs['name'] == 'step':\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, **shd_kwargs['param'])\n",
    "        elif shd_kwargs['name'] == 'cycle':\n",
    "            scheduler = torch.optim.lr_scheduler.CyclicLR(self.optimizer, max_lr=[pg['lr_param'] for pg in self.optimizer.param_groups],cycle_momentum=False,mode='triangular2',**shd_kwargs['param'])\n",
    "\n",
    "        return scheduler\n",
    "    \n",
    "    def reset_scheduler(self):\n",
    "        rst_kwargs = config.train_params['trainer']['learn_rate']['reset']\n",
    "        if rst_kwargs['num_reset'] <= 0 or (self.epoch_i + 1) < rst_kwargs['trigger']: return\n",
    "\n",
    "        trigger_intvl = rst_kwargs['trigger'] // 2 if rst_kwargs['speedup2x'] else rst_kwargs['trigger']\n",
    "        if (self.epoch_i + 1 - rst_kwargs['trigger']) % trigger_intvl != 0: return\n",
    "        \n",
    "        trigger_times = ((self.epoch_i + 1 - rst_kwargs['trigger']) // trigger_intvl) + 1\n",
    "        if trigger_times > rst_kwargs['num_reset']: return\n",
    "        \n",
    "        # confirm reset : change back optimizor learn rate\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr_param']  * rst_kwargs['recover_level']\n",
    "        \n",
    "        # confirm reset : reassign scheduler\n",
    "        if rst_kwargs['speedup2x']:\n",
    "            shd_kwargs = deepcopy(config.train_params['trainer']['scheduler'])\n",
    "            for k in np.intersect1d(list(shd_kwargs['param'].keys()),['step_size' , 'warmup_stage' , 'anneal_stage' , 'step_size_up' , 'step_size_down']):\n",
    "                shd_kwargs['param'][k] //= 2\n",
    "        else:\n",
    "            shd_kwargs = None\n",
    "        self.scheduler = self.load_scheduler(shd_kwargs)\n",
    "        self.printer('reset_learn_rate')\n",
    "        \n",
    "    def load_multiloss(self):\n",
    "        multiloss = None\n",
    "        if self.param['num_output'] > 1:\n",
    "            multiloss = multiloss_calculator(multi_type = config.train_params['multitask']['type'])\n",
    "            multiloss.reset_multi_type(self.param['num_output'] , **config.train_params['multitask']['param_dict'][multiloss.multi_type])\n",
    "        return multiloss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-02-03 23:53:56|MOD:587015923   |\u001b[0m: \u001b[1m\u001b[31mStart Process [Load Data]!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-02-03 23:54:11|MOD:587015923   |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Load Data]! Cost 15.2Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-02-03 23:54:11|MOD:587015923   |\u001b[0m: \u001b[1m\u001b[31mStart Process [Train Model]!\u001b[0m\n",
      "\u001b[32mLSTM_day_SHORTTEST #0 @20170103 LoadData Cost    1.6Secs\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  0.99252, train 0.00753, valid 0.02147, max 0.0215, best 0.0215, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.87464, train 0.13405, valid 0.15433, max 0.1543, best 0.1543, lr3.8e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.83297, train 0.18292, valid 0.18479, max 0.1860, best 0.1860, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.81190, train 0.20859, valid 0.20345, max 0.2037, best 0.2037, lr6.3e-04\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-02-03 23:54:25|MOD:587015923   |\u001b[0m: \u001b[1m\u001b[34mLSTM_day_SHORTTEST #0 @20170103|Round 0 FirstBite Ep# 19 Max Epoch|Train 0.2126 Valid 0.2073 BestVal 0.2074|Cost  0.2Min,  0.6Sec/Ep\u001b[0m\n",
      "\u001b[32mLSTM_day_SHORTTEST #0 @20170704 LoadData Cost    1.5Secs\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  0.99838, train 0.00166, valid-0.00089, max-0.0009, best-0.0009, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.89401, train 0.11209, valid 0.12100, max 0.1210, best 0.1210, lr3.8e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.85892, train 0.15214, valid 0.16051, max 0.1605, best 0.1605, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.82501, train 0.19247, valid 0.19224, max 0.1922, best 0.1922, lr6.3e-04\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-02-03 23:54:39|MOD:587015923   |\u001b[0m: \u001b[1m\u001b[34mLSTM_day_SHORTTEST #0 @20170704|Round 0 FirstBite Ep# 19 Max Epoch|Train 0.1986 Valid 0.1963 BestVal 0.1963|Cost  0.2Min,  0.6Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-02-03 23:54:39|MOD:587015923   |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Train Model]! Cost 0.0 Hours, 0.2 Min/model, 0.7 Sec/Epoch\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "controller = model_controller()\n",
    "controller.main_process()\n",
    "controller.ResultOutput()\n",
    "trainer_timer.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in controller.data.dataloaders['train']:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'day': {'avg': tensor([[1.0085, 1.0083, 1.0104, 1.0106, 1.6968, 1.0094],\n",
       "          [1.0097, 1.0092, 1.0109, 1.0104, 1.7055, 1.0102],\n",
       "          [1.0092, 1.0090, 1.0101, 1.0103, 1.6933, 1.0097],\n",
       "          [1.0091, 1.0084, 1.0098, 1.0097, 1.6816, 1.0092],\n",
       "          [1.0087, 1.0080, 1.0092, 1.0086, 1.6852, 1.0085],\n",
       "          [1.0077, 1.0074, 1.0096, 1.0097, 1.6630, 1.0086],\n",
       "          [1.0088, 1.0083, 1.0100, 1.0096, 1.6749, 1.0093],\n",
       "          [1.0082, 1.0080, 1.0092, 1.0095, 1.6626, 1.0088],\n",
       "          [1.0081, 1.0075, 1.0089, 1.0088, 1.6557, 1.0083],\n",
       "          [1.0078, 1.0071, 1.0083, 1.0077, 1.6499, 1.0076],\n",
       "          [1.0067, 1.0065, 1.0087, 1.0089, 1.6412, 1.0077],\n",
       "          [1.0078, 1.0074, 1.0092, 1.0088, 1.6540, 1.0085],\n",
       "          [1.0074, 1.0072, 1.0084, 1.0086, 1.6429, 1.0080],\n",
       "          [1.0073, 1.0067, 1.0081, 1.0080, 1.6295, 1.0074],\n",
       "          [1.0070, 1.0063, 1.0075, 1.0069, 1.6236, 1.0068],\n",
       "          [1.0059, 1.0057, 1.0079, 1.0081, 1.6144, 1.0068],\n",
       "          [1.0071, 1.0066, 1.0084, 1.0080, 1.6246, 1.0077],\n",
       "          [1.0066, 1.0064, 1.0076, 1.0077, 1.6191, 1.0072],\n",
       "          [1.0065, 1.0058, 1.0073, 1.0072, 1.6097, 1.0066],\n",
       "          [1.0062, 1.0055, 1.0067, 1.0061, 1.6033, 1.0060],\n",
       "          [1.0052, 1.0050, 1.0072, 1.0073, 1.5905, 1.0061],\n",
       "          [1.0064, 1.0059, 1.0077, 1.0072, 1.5941, 1.0069],\n",
       "          [1.0059, 1.0057, 1.0069, 1.0070, 1.5869, 1.0065],\n",
       "          [1.0059, 1.0053, 1.0068, 1.0066, 1.5796, 1.0061],\n",
       "          [1.0056, 1.0050, 1.0062, 1.0056, 1.5843, 1.0055],\n",
       "          [1.0047, 1.0045, 1.0068, 1.0068, 1.5651, 1.0057],\n",
       "          [1.0059, 1.0054, 1.0073, 1.0068, 1.5747, 1.0065],\n",
       "          [1.0055, 1.0053, 1.0065, 1.0067, 1.5726, 1.0061],\n",
       "          [1.0055, 1.0049, 1.0064, 1.0063, 1.5592, 1.0057],\n",
       "          [1.0052, 1.0045, 1.0058, 1.0052, 1.5556, 1.0051],\n",
       "          [1.0042, 1.0041, 1.0063, 1.0064, 1.5438, 1.0053],\n",
       "          [1.0055, 1.0050, 1.0069, 1.0064, 1.5531, 1.0061],\n",
       "          [1.0051, 1.0049, 1.0061, 1.0062, 1.5487, 1.0057],\n",
       "          [1.0050, 1.0044, 1.0059, 1.0057, 1.5320, 1.0052],\n",
       "          [1.0046, 1.0040, 1.0051, 1.0046, 1.5256, 1.0045],\n",
       "          [1.0036, 1.0035, 1.0057, 1.0058, 1.5116, 1.0046],\n",
       "          [1.0048, 1.0044, 1.0061, 1.0057, 1.5137, 1.0054],\n",
       "          [1.0043, 1.0041, 1.0054, 1.0055, 1.5056, 1.0050],\n",
       "          [1.0042, 1.0037, 1.0051, 1.0049, 1.4921, 1.0044],\n",
       "          [1.0038, 1.0032, 1.0043, 1.0038, 1.4822, 1.0037],\n",
       "          [1.0027, 1.0026, 1.0048, 1.0048, 1.4660, 1.0038],\n",
       "          [1.0038, 1.0034, 1.0052, 1.0046, 1.4652, 1.0044],\n",
       "          [1.0033, 1.0031, 1.0043, 1.0045, 1.4542, 1.0039],\n",
       "          [1.0032, 1.0027, 1.0041, 1.0040, 1.4371, 1.0034],\n",
       "          [1.0028, 1.0022, 1.0033, 1.0028, 1.4226, 1.0027],\n",
       "          [1.0017, 1.0016, 1.0037, 1.0038, 1.4119, 1.0028],\n",
       "          [1.0027, 1.0024, 1.0040, 1.0035, 1.4125, 1.0033],\n",
       "          [1.0021, 1.0021, 1.0031, 1.0033, 1.4015, 1.0028],\n",
       "          [1.0021, 1.0016, 1.0029, 1.0030, 1.3864, 1.0024],\n",
       "          [1.0018, 1.0013, 1.0022, 1.0018, 1.3759, 1.0017],\n",
       "          [1.0007, 1.0007, 1.0026, 1.0028, 1.3559, 1.0018],\n",
       "          [1.0018, 1.0015, 1.0030, 1.0027, 1.3494, 1.0024],\n",
       "          [1.0013, 1.0013, 1.0022, 1.0025, 1.3345, 1.0020],\n",
       "          [1.0012, 1.0008, 1.0020, 1.0021, 1.3190, 1.0015],\n",
       "          [1.0009, 1.0004, 1.0012, 1.0009, 1.2955, 1.0008],\n",
       "          [0.9999, 0.9999, 1.0016, 1.0020, 1.2603, 1.0009],\n",
       "          [1.0010, 1.0008, 1.0021, 1.0018, 1.2381, 1.0016],\n",
       "          [1.0005, 1.0005, 1.0011, 1.0016, 1.1991, 1.0011],\n",
       "          [1.0003, 1.0000, 1.0008, 1.0011, 1.1316, 1.0005],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]),\n",
       "  'std': tensor([[0.2527, 0.2517, 0.2523, 0.2530, 4.2304, 0.2616],\n",
       "          [0.2510, 0.2500, 0.2509, 0.2513, 4.0603, 0.2513],\n",
       "          [0.2492, 0.2483, 0.2490, 0.2498, 4.0808, 0.2490],\n",
       "          [0.2467, 0.2456, 0.2461, 0.2459, 4.0358, 0.2461],\n",
       "          [0.2436, 0.2426, 0.2429, 0.2425, 4.1018, 0.2427],\n",
       "          [0.2394, 0.2387, 0.2392, 0.2404, 3.9582, 0.2486],\n",
       "          [0.2374, 0.2368, 0.2375, 0.2383, 3.7802, 0.2382],\n",
       "          [0.2351, 0.2347, 0.2352, 0.2365, 3.5745, 0.2353],\n",
       "          [0.2325, 0.2320, 0.2326, 0.2330, 3.8854, 0.2326],\n",
       "          [0.2298, 0.2292, 0.2295, 0.2296, 3.5963, 0.2294],\n",
       "          [0.2257, 0.2254, 0.2259, 0.2274, 3.5894, 0.2258],\n",
       "          [0.2238, 0.2239, 0.2244, 0.2259, 3.8270, 0.2254],\n",
       "          [0.2221, 0.2219, 0.2224, 0.2235, 3.8636, 0.2225],\n",
       "          [0.2197, 0.2191, 0.2196, 0.2200, 3.3153, 0.2197],\n",
       "          [0.2170, 0.2163, 0.2168, 0.2169, 3.5903, 0.2167],\n",
       "          [0.2135, 0.2130, 0.2137, 0.2151, 3.7557, 0.2134],\n",
       "          [0.2119, 0.2115, 0.2121, 0.2133, 3.7047, 0.2129],\n",
       "          [0.2097, 0.2093, 0.2101, 0.2108, 3.4941, 0.2100],\n",
       "          [0.2074, 0.2066, 0.2072, 0.2076, 3.3625, 0.2071],\n",
       "          [0.2051, 0.2042, 0.2049, 0.2049, 3.5896, 0.2046],\n",
       "          [0.2020, 0.2012, 0.2023, 0.2030, 3.8066, 0.2018],\n",
       "          [0.2003, 0.1995, 0.2005, 0.2011, 3.7178, 0.2011],\n",
       "          [0.1981, 0.1975, 0.1991, 0.1992, 3.4201, 0.1985],\n",
       "          [0.1967, 0.1956, 0.1968, 0.1966, 3.2123, 0.1965],\n",
       "          [0.1940, 0.1932, 0.1943, 0.1940, 4.4779, 0.1938],\n",
       "          [0.1908, 0.1902, 0.1914, 0.1918, 3.1821, 0.1909],\n",
       "          [0.1889, 0.1883, 0.1894, 0.1901, 3.2240, 0.1899],\n",
       "          [0.1870, 0.1864, 0.1881, 0.1886, 3.0584, 0.1875],\n",
       "          [0.1855, 0.1845, 0.1858, 0.1856, 3.2100, 0.1854],\n",
       "          [0.1819, 0.1813, 0.1825, 0.1827, 3.0745, 0.1821],\n",
       "          [0.1786, 0.1782, 0.1792, 0.1802, 3.2311, 0.1789],\n",
       "          [0.1762, 0.1759, 0.1773, 0.1784, 3.0628, 0.1777],\n",
       "          [0.1744, 0.1740, 0.1759, 0.1762, 3.4691, 0.1752],\n",
       "          [0.1723, 0.1713, 0.1725, 0.1722, 3.2419, 0.1721],\n",
       "          [0.1680, 0.1674, 0.1684, 0.1685, 3.5062, 0.1680],\n",
       "          [0.1643, 0.1635, 0.1648, 0.1654, 3.0161, 0.1643],\n",
       "          [0.1609, 0.1606, 0.1620, 0.1634, 3.3694, 0.1625],\n",
       "          [0.1582, 0.1579, 0.1603, 0.1606, 2.8475, 0.1594],\n",
       "          [0.1559, 0.1549, 0.1565, 0.1561, 2.7551, 0.1561],\n",
       "          [0.1514, 0.1506, 0.1524, 0.1519, 2.9498, 0.1517],\n",
       "          [0.1471, 0.1459, 0.1475, 0.1473, 4.5080, 0.1469],\n",
       "          [0.1420, 0.1416, 0.1430, 0.1435, 2.3828, 0.1439],\n",
       "          [0.1384, 0.1378, 0.1403, 0.1403, 2.2827, 0.1398],\n",
       "          [0.1354, 0.1342, 0.1365, 0.1360, 2.3226, 0.1361],\n",
       "          [0.1302, 0.1292, 0.1316, 0.1311, 2.1734, 0.1311],\n",
       "          [0.1247, 0.1238, 0.1256, 0.1259, 2.2124, 0.1256],\n",
       "          [0.1191, 0.1189, 0.1203, 0.1207, 2.2262, 0.1219],\n",
       "          [0.1147, 0.1142, 0.1163, 0.1171, 2.2736, 0.1162],\n",
       "          [0.1105, 0.1098, 0.1122, 0.1121, 2.3479, 0.1120],\n",
       "          [0.1049, 0.1040, 0.1059, 0.1057, 2.8022, 0.1058],\n",
       "          [0.0981, 0.0974, 0.0984, 0.0997, 2.3151, 0.0992],\n",
       "          [0.0923, 0.0922, 0.0935, 0.0951, 2.3134, 0.0963],\n",
       "          [0.0868, 0.0864, 0.0886, 0.0896, 1.9287, 0.0891],\n",
       "          [0.0807, 0.0799, 0.0815, 0.0821, 1.8590, 0.0824],\n",
       "          [0.0732, 0.0720, 0.0729, 0.0736, 2.0776, 0.0743],\n",
       "          [0.0643, 0.0633, 0.0637, 0.0661, 4.3622, 0.0659],\n",
       "          [0.0563, 0.0550, 0.0565, 0.0583, 1.6378, 0.0613],\n",
       "          [0.0480, 0.0445, 0.0468, 0.0473, 1.2209, 0.0487],\n",
       "          [0.0337, 0.0296, 0.0308, 0.0324, 0.9545, 0.0349],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controller.data.norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.8815e+01, 3.9337e+01, 3.8641e+01, 3.9047e+01, 1.7895e+05, 3.8980e+01],\n",
       "        [3.8989e+01, 3.9163e+01, 3.8757e+01, 3.8989e+01, 1.2186e+05, 3.8979e+01],\n",
       "        [3.9047e+01, 3.9454e+01, 3.8757e+01, 3.9279e+01, 1.8771e+05, 3.9042e+01],\n",
       "        [3.9163e+01, 3.9221e+01, 3.8757e+01, 3.8931e+01, 1.1574e+05, 3.8956e+01],\n",
       "        [3.8989e+01, 3.9337e+01, 3.8757e+01, 3.8873e+01, 9.7713e+04, 3.9009e+01],\n",
       "        [3.8931e+01, 3.9976e+01, 3.8873e+01, 3.9744e+01, 2.8802e+05, 3.9550e+01],\n",
       "        [3.9628e+01, 4.0092e+01, 3.9337e+01, 3.9802e+01, 1.6651e+05, 3.9646e+01],\n",
       "        [3.9628e+01, 4.0846e+01, 3.9337e+01, 4.0208e+01, 3.1834e+05, 4.0143e+01],\n",
       "        [4.0208e+01, 4.1078e+01, 3.9976e+01, 4.0498e+01, 2.6503e+05, 4.0626e+01],\n",
       "        [4.0556e+01, 4.0730e+01, 3.9221e+01, 4.0324e+01, 1.9224e+05, 3.9896e+01],\n",
       "        [4.0266e+01, 4.1078e+01, 3.9860e+01, 4.0904e+01, 2.5498e+05, 4.0559e+01],\n",
       "        [4.0672e+01, 4.0730e+01, 3.9918e+01, 4.0034e+01, 1.9516e+05, 4.0321e+01],\n",
       "        [3.9744e+01, 3.9918e+01, 3.9454e+01, 3.9802e+01, 1.1087e+05, 3.9603e+01],\n",
       "        [3.9802e+01, 3.9918e+01, 3.8815e+01, 3.9395e+01, 2.2333e+05, 3.9412e+01],\n",
       "        [3.9221e+01, 3.9570e+01, 3.8873e+01, 3.9337e+01, 1.6623e+05, 3.9239e+01],\n",
       "        [3.9163e+01, 3.9918e+01, 3.8931e+01, 3.9454e+01, 1.3648e+05, 3.9584e+01],\n",
       "        [3.9570e+01, 3.9686e+01, 3.9279e+01, 3.9454e+01, 7.7982e+04, 3.9511e+01],\n",
       "        [3.9744e+01, 4.0788e+01, 3.9744e+01, 4.0440e+01, 2.3784e+05, 4.0435e+01],\n",
       "        [4.0382e+01, 4.0730e+01, 4.0092e+01, 4.0382e+01, 1.4007e+05, 4.0407e+01],\n",
       "        [4.0324e+01, 4.1078e+01, 3.9802e+01, 4.1020e+01, 2.2603e+05, 4.0568e+01],\n",
       "        [4.1136e+01, 4.1426e+01, 3.9221e+01, 3.9395e+01, 3.0510e+05, 4.0436e+01],\n",
       "        [3.9279e+01, 4.2238e+01, 3.8641e+01, 4.2122e+01, 4.5985e+05, 4.1169e+01],\n",
       "        [4.1368e+01, 4.2180e+01, 4.1020e+01, 4.1484e+01, 3.4272e+05, 4.1585e+01],\n",
       "        [4.1194e+01, 4.2703e+01, 4.1194e+01, 4.2238e+01, 3.2825e+05, 4.1961e+01],\n",
       "        [4.3225e+01, 4.3979e+01, 4.1832e+01, 4.2122e+01, 4.4125e+05, 4.2791e+01],\n",
       "        [4.2238e+01, 4.5255e+01, 4.1368e+01, 4.4269e+01, 6.4087e+05, 4.3390e+01],\n",
       "        [4.3573e+01, 4.4211e+01, 4.3225e+01, 4.3805e+01, 3.5351e+05, 4.3609e+01],\n",
       "        [4.3805e+01, 4.5139e+01, 4.3341e+01, 4.3689e+01, 3.0953e+05, 4.4007e+01],\n",
       "        [4.3283e+01, 4.3457e+01, 4.2529e+01, 4.2993e+01, 2.1191e+05, 4.2956e+01],\n",
       "        [4.2819e+01, 4.3283e+01, 4.2122e+01, 4.2645e+01, 1.9102e+05, 4.2683e+01],\n",
       "        [4.2529e+01, 4.3167e+01, 4.2180e+01, 4.3167e+01, 1.5478e+05, 4.2819e+01],\n",
       "        [4.2877e+01, 4.3515e+01, 4.2122e+01, 4.2180e+01, 1.7429e+05, 4.2592e+01],\n",
       "        [4.2238e+01, 4.2471e+01, 4.1716e+01, 4.2354e+01, 1.4002e+05, 4.2097e+01],\n",
       "        [4.2064e+01, 4.2238e+01, 4.1658e+01, 4.1716e+01, 1.1217e+05, 4.1912e+01],\n",
       "        [4.1542e+01, 4.2122e+01, 4.1020e+01, 4.1542e+01, 1.5543e+05, 4.1556e+01],\n",
       "        [4.1600e+01, 4.3341e+01, 4.1484e+01, 4.2935e+01, 2.8910e+05, 4.2817e+01],\n",
       "        [4.2761e+01, 4.4037e+01, 4.2645e+01, 4.3225e+01, 3.1093e+05, 4.3325e+01],\n",
       "        [4.2993e+01, 4.4501e+01, 4.2761e+01, 4.3979e+01, 3.8790e+05, 4.3667e+01],\n",
       "        [4.3457e+01, 4.3689e+01, 4.2935e+01, 4.3283e+01, 2.8163e+05, 4.3257e+01],\n",
       "        [4.3225e+01, 4.3515e+01, 4.2761e+01, 4.3457e+01, 1.7143e+05, 4.3126e+01]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v['x'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
