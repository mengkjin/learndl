{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'basic' from partially initialized module 'src.model' (most likely due to a circular import) (d:\\Coding\\learndl\\learndl\\src\\model\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodernTCN\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModernTCNEmbed\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Layer\n\u001b[0;32m      5\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \n",
      "File \u001b[1;32md:\\Coding\\learndl\\learndl\\src\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     util , func , factor , boost , model , data\n\u001b[0;32m      3\u001b[0m )\n",
      "File \u001b[1;32md:\\Coding\\learndl\\learndl\\src\\model\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     atf , attention , basic , cnn , model , rnn , tra   \n\u001b[0;32m      3\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'basic' from partially initialized module 'src.model' (most likely due to a circular import) (d:\\Coding\\learndl\\learndl\\src\\model\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.model.modernTCN import ModernTCNEmbed\n",
    "from src import Layer\n",
    "batch_size = 2 \n",
    "seq_len = 30\n",
    "patch_len = 3\n",
    "stride = 2\n",
    "nvars = 6\n",
    "mask_ratio = 0.4\n",
    "d_model = 16\n",
    "predict_steps = 1\n",
    "\n",
    "num_patch = (max(seq_len, patch_len)-patch_len) // stride + 2 # 15\n",
    "\n",
    "x = torch.rand(batch_size , seq_len , nvars)\n",
    "y = torch.rand(batch_size , predict_steps)\n",
    "x.shape , y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 15, 16])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = ModernTCNEmbed(nvars , d_model , patch_len=patch_len , stride=stride, shared=True)\n",
    "embed(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MtcnTSMixer(nn.Module):\n",
    "    '''\n",
    "    in : [bs x nvars x d_model x num_patch]\n",
    "    out: [bs x nvars * d_model x num_patch]\n",
    "    '''\n",
    "    def __init__(self, nvars, d_model, kernel_size = 3):\n",
    "        super().__init__()\n",
    "        self.nvars   = nvars\n",
    "        self.d_model = d_model\n",
    "        self.dw_conv = nn.Conv1d(\n",
    "            in_channels     = nvars * d_model, \n",
    "            out_channels    = nvars * d_model, \n",
    "            kernel_size     = kernel_size,\n",
    "            groups          = nvars * d_model,\n",
    "            padding         = 'same'\n",
    "        )\n",
    "        self.bn = nn.BatchNorm1d(nvars * d_model)\n",
    "        \n",
    "    def forward(self , x):\n",
    "        '''\n",
    "        in : [bs x nvars x d_model x num_patch]\n",
    "        out: [bs x nvars * d_model x num_patch]\n",
    "        '''\n",
    "        bs = x.shape[0]\n",
    "        x = x.reshape(bs,self.nvars*self.d_model,-1)    # [bs x nvars * d_model x num_patch]\n",
    "        x = self.dw_conv(x)                             # [bs x nvars * d_model x num_patch]\n",
    "        x = self.bn(x)                                  # [bs x nvars * d_model x num_patch]\n",
    "        #x = x.reshape(bs,self.nvars,self.d_model,-1)    # [bs x nvars x d_model x num_patch]\n",
    "        return x\n",
    "\n",
    "class MtcnFeatureMixer(nn.Module):\n",
    "    '''\n",
    "    in : [bs x nvars * d_model x num_patch]\n",
    "    out: [bs x nvars x d_model x num_patch]\n",
    "    '''\n",
    "    def __init__(self, nvars, d_model, kernel_size = 1 , activation = 'gelu'):\n",
    "        super().__init__()\n",
    "        self.nvars   = nvars\n",
    "        self.d_model = d_model\n",
    "        self.act     = Layer.Act.get_activation_fn(activation)\n",
    "        self.pw_con_up = nn.Conv1d(\n",
    "            in_channels     = nvars * d_model, \n",
    "            out_channels    = kernel_size * nvars * d_model, \n",
    "            kernel_size     = 1,\n",
    "            groups          = nvars\n",
    "        )\n",
    "        self.pw_con_down = nn.Conv1d(\n",
    "            in_channels     = kernel_size * nvars * d_model, \n",
    "            out_channels    = nvars * d_model, \n",
    "            kernel_size     = 1 ,\n",
    "            groups          = nvars\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        in : [bs x nvars * d_model x num_patch]\n",
    "        out: [bs x nvars x d_model x num_patch]\n",
    "        '''\n",
    "        bs = x.shape[0]\n",
    "        x = self.pw_con_up(x)                           # [bs x kernel_size * nvars * d_model x num_patch]\n",
    "        x = self.act(x)                                 # [bs x kernel_size * nvars * d_model x num_patch]\n",
    "        x = self.pw_con_down(x)                         # [bs x nvars * d_model x num_patch]\n",
    "        x = x.reshape(bs,self.nvars,self.d_model,-1)    # [bs x nvars x d_model x num_patch]\n",
    "        return x # out shape :[bs, nvars, d_model, num_patch] \n",
    "    \n",
    "    \n",
    "class MtcnChannelMixer(nn.Module):\n",
    "    '''\n",
    "    in : [bs x nvars x d_model x num_patch]\n",
    "    out: [bs x nvars x d_model x num_patch]\n",
    "    '''\n",
    "    def __init__(self, nvars, d_model, kernel_size = 1 , activation = 'gelu'):\n",
    "        super().__init__()\n",
    "        self.nvars   = nvars\n",
    "        self.d_model = d_model\n",
    "        self.act     = Layer.Act.get_activation_fn(activation)\n",
    "        self.pw_con_up = nn.Conv1d(\n",
    "            in_channels     = nvars * d_model, \n",
    "            out_channels    = kernel_size * nvars * d_model, \n",
    "            kernel_size     = 1,\n",
    "            groups          = d_model\n",
    "        )\n",
    "        self.pw_con_down = nn.Conv1d(\n",
    "            in_channels     = kernel_size * nvars * d_model, \n",
    "            out_channels    = nvars * d_model, \n",
    "            kernel_size     = 1,\n",
    "            groups          = d_model\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        in : [bs x nvars x d_model x num_patch]\n",
    "        out: [bs x nvars x d_model x num_patch]\n",
    "        ''' \n",
    "        bs = x.shape[0]\n",
    "        x = x.permute(0,2,1,3)                                      # [bs x d_model x nvars x num_patch]\n",
    "        x = x.reshape(bs,self.nvars*self.d_model,-1)                # [bs x d_model * nvars x num_patch]\n",
    "        x = self.pw_con_up(x)                                       # [bs x kernel_size * d_model * nvars x num_patch]\n",
    "        x = self.act(x)                                             # [bs x kernel_size * d_model * nvars x num_patch]\n",
    "        x = self.pw_con_down(x)                                     # [bs x d_model * nvars x num_patch]\n",
    "        x = x.reshape(bs,self.nvars,self.d_model,-1)                # [bs x d_model x nvars x num_patch]\n",
    "        x = x.permute(0,2,1,3)                                      # [bs x nvars x d_model x num_patch]\n",
    "        return x  \n",
    "\n",
    "class ModernTCNBlock(nn.Module):\n",
    "    '''\n",
    "    in : [bs x nvars x num_patch x d_model]\n",
    "    out: [bs x nvars x num_patch x d_model]\n",
    "    '''\n",
    "    def __init__(self, nvars, d_model, kernel_size, expansion_factor = 1,channel_mixer = True):\n",
    "        super().__init__()\n",
    "        self.ts_mixer = MtcnTSMixer(nvars, d_model, kernel_size)\n",
    "        self.feature_mixer = MtcnFeatureMixer(nvars, d_model, expansion_factor)\n",
    "        self.channel_mixer = MtcnChannelMixer(nvars, d_model, expansion_factor) if channel_mixer else nn.Sequential()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        in : [bs x nvars x num_patch x d_model]\n",
    "        out: [bs x nvars x num_patch x d_model]\n",
    "        '''\n",
    "        res = x\n",
    "        x = x.permute(0,1,3,2)      # [bs x nvars x d_model x num_patch]\n",
    "        x = self.ts_mixer(x)        # [bs x nvars x d_model x num_patch]\n",
    "        x = self.feature_mixer(x)   # [bs x nvars x d_model x num_patch]\n",
    "        x = self.channel_mixer(x)   # [bs x nvars x d_model x num_patch]\n",
    "        x = x.permute(0,1,3,2)      # [bs x nvars x num_patch x d_model]\n",
    "\n",
    "        return res + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1,train,valid loader shuffle\n",
    "# 2,tar extract and record all dates\n",
    "# 3,seperate model config yaml, and use train yaml to load them\n",
    "# 4,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModernTCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 nvars :int ,\n",
    "                 seq_len :int,\n",
    "                 d_model = 16,\n",
    "                 patch_len = 8,\n",
    "                 stride = 4 ,\n",
    "                 kernel_size = 3 ,\n",
    "                 expansion_factor = 1,\n",
    "                 num_layers = 1,\n",
    "                 label_idx = 1,\n",
    "                 factormean = True,\n",
    "                 num_MLP = 1,\n",
    "                 gamma = 0,\n",
    "                 predict_steps:int = 1,\n",
    "                 channel_mixer = True,\n",
    "                 shared_embedding=True, shared_head = False, \n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        '''\n",
    "        self, \n",
    "        nvars : int , \n",
    "        seq_len: int , \n",
    "        patch_len:int, \n",
    "        stride:int, \n",
    "        d_model:int=32, \n",
    "        shared_embedding=True, shared_head = False, \n",
    "        revin:bool=True,n_layers:int=3, n_heads=16, d_ff:int=256, \n",
    "        norm:str='BatchNorm', attn_dropout:float=0., dropout:float=0., act:str='gelu', \n",
    "        res_attention:bool=True, pre_norm:bool=False, store_attn:bool=False,\n",
    "        pe:str='zeros', learn_pe:bool=True, head_dropout = 0, predict_steps:int = 1,\n",
    "        head_type = 'prediction', verbose:bool=False, **kwargs\n",
    "        '''\n",
    "\n",
    "\n",
    "        if stride is None: stride = patch_len // 2\n",
    "        num_patch = seq_len // patch_len\n",
    "        self.embed = ModernTCNEmbed(nvars , d_model , patch_len=patch_len , stride=stride, shared=shared_embedding)\n",
    "        \n",
    "        \n",
    "        layers = [ModernTCNBlock(nvars=nvars,\n",
    "                                 d_model=d_model,\n",
    "                                 kernel_size=kernel_size,\n",
    "                                 num_patch =num_patch,\n",
    "                                 expansion_factor=expansion_factor,\n",
    "                                 channel_mixer=channel_mixer\n",
    "                                 ) for _ in range(num_layers)\n",
    "                  ]\n",
    "        self.tcn_encoder = nn.Sequential(*layers)\n",
    "\n",
    "        # predict head\n",
    "        self.ts_linear = nn.Linear(d_model*num_patch,d_model)   \n",
    "        \n",
    "        # task specific setting\n",
    "        self.label_idx = label_idx\n",
    "        \n",
    "        # predict layer\n",
    "        self.predict_layer = nn.Sequential(\n",
    "            nn.Linear(nvars*d_model, 50)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        x_emb = self.embed(inputs)  # [bs, nvars, seq_len] -> [bs, nvars, d_model, num_patch]\n",
    "        x_out = self.tcn_encoder(x_emb)  # [bs, nvars, d_model, num_patch]\n",
    "        x_out = rearrange(x_out, 'bs nvars d n -> bs nvars (d n)')\n",
    "        \n",
    "        x_out = F.gelu(self.ts_linear(x_out))  # [bs, nvars, d_model]\n",
    "        x_out = rearrange(x_out, 'bs nvars d -> bs (nvars d)') # [bs, nvars*d_model]\n",
    "        return x_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
