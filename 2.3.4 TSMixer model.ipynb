{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L \n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torchmetrics\n",
    "import random\n",
    "import glob\n",
    "import math\n",
    "from einops import rearrange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    '''\n",
    "    PatchTST位置编码\n",
    "    num_patches: 序列长度\n",
    "    num_input_channels: 输入通道数\n",
    "    d_model: hidden state维度\n",
    "    use_cls_token: 是在开头添加一个cls token\n",
    "    positional_encoding_type: 位置编码类型\n",
    "    \n",
    "    input : [B  M  N  d_model]\n",
    "    output: [B  M  N  d_model] / [B  M  N+1 d_model]\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                seq_len,\n",
    "                d_model,\n",
    "                positional_dropout = 0,\n",
    "                use_cls_token = False,\n",
    "                num_input_channels = 1,\n",
    "                positional_encoding_type = 'sincos' # 可选random（随机可学习）/ sincos（固定）\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.use_cls_token = use_cls_token\n",
    "        self.num_input_channels = num_input_channels\n",
    "        \n",
    "        if use_cls_token:\n",
    "            self.cls_token = nn.Parameter(torch.zeros(1, 1, 1, d_model))  # cls_token: [1 , num_input_channels , 1 , d_model]\n",
    "            \n",
    "        # 位置编码矩阵: [seq_len, d_model]\n",
    "        self.position_enc = self._init_pe(positional_encoding_type,seq_len,d_model)\n",
    "        # Positional dropout\n",
    "        self.positional_dropout = (\n",
    "            nn.Dropout(positional_dropout) if positional_dropout > 0 else nn.Identity()\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_pe(positional_encoding_type, seq_len, d_model):\n",
    "        if positional_encoding_type == \"random\":\n",
    "            position_enc = nn.Parameter(torch.randn(seq_len, d_model), requires_grad=True)\n",
    "        elif positional_encoding_type == \"sincos\":\n",
    "            position_enc = torch.zeros(seq_len,d_model)\n",
    "            position = torch.arange(0, seq_len).unsqueeze(1)\n",
    "            div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "            position_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "            position_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "            position_enc = position_enc - position_enc.mean()\n",
    "            position_enc = position_enc / (position_enc.std() * 10)\n",
    "            position_enc = nn.Parameter(position_enc, requires_grad=False)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"{positional_encoding_type} is not a valid positional encoder. Available types are 'random' and 'sincos'.\"\n",
    "            )\n",
    "        return position_enc\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.use_cls_token:\n",
    "            # PE矩阵形状为(N+1,D)\n",
    "            inputs = self.positional_dropout(inputs + self.position_enc[1:, :])\n",
    "            cls_token = self.cls_token + self.position_enc[:1, :]\n",
    "            cls_tokens = cls_token.expand(inputs.shape[0], self.num_input_channels, -1, -1)\n",
    "            hidden_state = torch.cat((cls_tokens, inputs), dim=2)\n",
    "        else:\n",
    "            # PE矩阵形状为(N,D)\n",
    "            hidden_state = self.positional_dropout(inputs + self.position_enc)\n",
    "        return hidden_state\n",
    "    \n",
    "\n",
    "class TSTPatchEmbed(torch.nn.Module):\n",
    "    '''\n",
    "    输入: [B L M]\n",
    "    (padding)unfold patch + linear embedding + 位置编码(可选)\n",
    "    输出: B M N d_model ->  [B*M N d_model]\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                seq_len,\n",
    "                d_model,\n",
    "                do_padding = 'end', # padding方法，如果为None则不进行padding\n",
    "                P = None,\n",
    "                S = None,\n",
    "                M = 6, # 变量数\n",
    "                emb_dropout = 0,\n",
    "                do_pe = True,       # 是否加入位置编码\n",
    "                pe_type = 'sincos',\n",
    "                ):\n",
    "        super().__init__()\n",
    "        assert do_padding in ['end',None]\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.P = P\n",
    "        self.S = S\n",
    "        \n",
    "        # 计算patch_number\n",
    "        patch_num = int((seq_len - P) / S + 1)\n",
    "        # padding\n",
    "        self.do_padding = do_padding\n",
    "        \n",
    "        if do_padding == \"end\":\n",
    "            self.padding_patch_layer = torch.nn.ReplicationPad1d((0, S))\n",
    "            patch_num += 1\n",
    "        else:\n",
    "            # 不进行padding时，需要保证时序排在【最后的一个窗口】正好能形成一组\n",
    "            new_sequence_length = self.P + self.S * (patch_num - 1)\n",
    "            self.sequence_start = seq_len - new_sequence_length\n",
    "        \n",
    "        # linear embedding layer\n",
    "        self.emb = torch.nn.Linear(P, d_model)\n",
    "        \n",
    "        # 位置编码\n",
    "        self.do_pe = do_pe\n",
    "        if self.do_pe:\n",
    "            self.pe_enc = positional_encoding(pe = pe_type\n",
    "                                            , q_len = patch_num\n",
    "                                            , d_model = d_model\n",
    "                                            )\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        z = rearrange(inputs, 'B L M -> B M L')  # [B L M] -> [B M L]\n",
    "        \n",
    "        if self.do_padding=='end': \n",
    "            z = self.padding_patch_layer(z)\n",
    "        else:\n",
    "            z = z[:, :, self.sequence_start :]  \n",
    "        \n",
    "        z = z.unfold(dimension=-1, size=self.P, step=self.S) # z: [bs x M x N x P]\n",
    "        x_emb = self.emb(z)\n",
    "        x_emb = rearrange(x_emb,\"B M N d_model -> (B M) N d_model\")  # 合并B和M的维度 -> 把每个变量视为了一个样本\n",
    "        \n",
    "        if self.do_pe:\n",
    "            x_emb = self.emb_dropout(x_emb + self.pe_enc)   # 加入位置编码,输出维度: (BM , N ,d_model)\n",
    "        else:\n",
    "            x_emb = self.emb_dropout(x_emb)\n",
    "        return x_emb  # [BM, N, d_model]\n",
    "\n",
    "class MixerNormLayer(nn.Module):\n",
    "    '''\n",
    "    Batch / Layer Norm\n",
    "    输入维度: (B, M, N, D)\n",
    "    输出维度: (B, M, N, D)\n",
    "    如果是batchnorm,会先合并B和M的维度,然后把D转到倒数第二个维度，再进行batchnorm\n",
    "    '''\n",
    "    def __init__(self, norm_type,d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_type = norm_type\n",
    "        \n",
    "        if \"batch\" in norm_type.lower():\n",
    "            self.norm = nn.BatchNorm1d(d_model) # 默认格式为(N,C) 或 (N,C,L)\n",
    "        else:\n",
    "            self.norm = nn.LayerNorm(d_model)  # 默认对最后一个维度进行LayerNorm\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        if \"batch\" in self.norm_type.lower():\n",
    "            # 将数据转为N C L 的格式\n",
    "            B = inputs.shape[0]\n",
    "            inputs = rearrange(inputs, \"B M N D -> (B M) N D\")\n",
    "            inputs = inputs.transpose(1, 2)  # [BM, D, N])\n",
    "            inputs = self.norm(inputs)\n",
    "            inputs = inputs.transpose(1, 2)  # [BM, N, D])\n",
    "            output = rearrange(inputs, \"(B M) N D -> B M N D\", B=B)\n",
    "        else:\n",
    "            output = self.norm(inputs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IC LOSS\n",
    "class ICLoss(nn.Module):\n",
    "    def __init__(self, gamma=0):\n",
    "        super(ICLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred_ = torch.mean(y_pred, dim=1).unsqueeze(1)\n",
    "        y_pred_demean = y_pred_ - y_pred_.mean(dim=0, keepdim=True)\n",
    "        y_true_demean = y_true - y_true.mean(dim=0, keepdim=True)\n",
    "        cos_sim = F.cosine_similarity(y_pred_demean, y_true_demean, dim=0)\n",
    "        loss1 = 1 - cos_sim.mean()\n",
    "        \n",
    "        if self.gamma > 0:\n",
    "            F_inv = torch.linalg.inv(torch.matmul(y_pred_demean.T, y_pred_demean))\n",
    "            penalty = torch.trace(F_inv)\n",
    "            loss1 = loss1 + self.gamma * penalty\n",
    "        return loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    门控ATTN插件\n",
    "    对输入数据的最后一个维度进行加权\n",
    "    输入输出维度相同\n",
    "    \"\"\"\n",
    "    def __init__(self, in_size: int, out_size: int):\n",
    "        super().__init__()\n",
    "        self.attn_layer = nn.Linear(in_size, out_size)\n",
    "        self.attn_softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        attn_weight = self.attn_softmax(self.attn_layer(inputs))\n",
    "        inputs = inputs * attn_weight\n",
    "        return inputs\n",
    " \n",
    "\n",
    "class MixerMLP(nn.Module):\n",
    "    '''\n",
    "    在三个提取器中使用的MLP Mixer\n",
    "    类似FFN，逆瓶颈结构，最终的输出维度和输出维度一样\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, in_features, out_features, expansion_factor=1,dropout=0):\n",
    "        super().__init__()\n",
    "        num_hidden = in_features * expansion_factor\n",
    "        self.fc1 = nn.Linear(in_features, num_hidden)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(num_hidden, out_features)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        inputs = self.dropout1(nn.functional.gelu(self.fc1(inputs)))\n",
    "        inputs = self.fc2(inputs)\n",
    "        inputs = self.dropout2(inputs)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "class TsMixerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    inter patch 信息抽取（每个通道、学习每一个特征维度的时间N交互）\n",
    "    输入维度: B M N D\n",
    "    输出维度: B M N D\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_patches,\n",
    "                 d_model,\n",
    "                 dropout,\n",
    "                 expansion_factor,\n",
    "                 gated_attn=True,\n",
    "                 norm_type='batch',\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm = MixerNormLayer(norm_type,d_model)\n",
    "        self.gated_attn = gated_attn\n",
    "        self.mlp = MixerMLP(\n",
    "                in_features=num_patches,\n",
    "                out_features=num_patches,\n",
    "                expansion_factor = expansion_factor,\n",
    "                dropout = dropout\n",
    "        )\n",
    "        if gated_attn:\n",
    "            self.gating_block = GatedAttention(in_size=num_patches, out_size=num_patches)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        residual = hidden_state\n",
    "        \n",
    "        hidden_state = self.norm(hidden_state)\n",
    "        hidden_state = hidden_state.transpose(2, 3)\n",
    "        hidden_state = self.mlp(hidden_state)\n",
    "\n",
    "        if self.gated_attn:\n",
    "            hidden_state = self.gating_block(hidden_state)\n",
    "\n",
    "        # Transpose back\n",
    "        hidden_state = hidden_state.transpose(2, 3)\n",
    "        out = hidden_state + residual\n",
    "        return out\n",
    "    \n",
    "class FeatureMixerBlock(nn.Module):\n",
    "    '''\n",
    "    输入维度:batch_size, m, num_patches, d_model\n",
    "    输出维度:batch_size, m, num_patches, d_model\n",
    "    '''\n",
    "    def __init__(self\n",
    "                 , d_model\n",
    "                 ,expansion_factor\n",
    "                 ,dropout\n",
    "                 ,gated_attn=True\n",
    "                 ,norm_type = 'batch'\n",
    "                 \n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm = MixerNormLayer(norm_type,d_model)\n",
    "        self.gated_attn = gated_attn\n",
    "        self.mlp = MixerMLP(\n",
    "                in_features=d_model,\n",
    "                out_features=d_model,\n",
    "                expansion_factor = expansion_factor,\n",
    "                dropout = dropout\n",
    "        )\n",
    "        if gated_attn:\n",
    "            self.gating_block = GatedAttention(in_size=d_model, out_size=d_model)\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        residual = hidden\n",
    "        hidden = self.norm(hidden)\n",
    "        hidden = self.mlp(hidden)\n",
    "\n",
    "        if self.gated_attn:\n",
    "            hidden = self.gating_block(hidden)\n",
    "\n",
    "        out = hidden + residual\n",
    "        return out\n",
    "\n",
    "class ChannelMixerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    输入维度\n",
    "    输出维度\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 norm_type,\n",
    "                 num_input_channels,\n",
    "                 expansion_factor,\n",
    "                 dropout,\n",
    "                 gated_attn=True,\n",
    "                 ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm = MixerNormLayer(norm_type,d_model)\n",
    "        self.gated_attn = gated_attn\n",
    "        self.mlp = MixerMLP(\n",
    "                in_features=num_input_channels,\n",
    "                out_features=num_input_channels,\n",
    "                expansion_factor = expansion_factor,\n",
    "                dropout = dropout\n",
    "        )\n",
    "        if gated_attn:\n",
    "            self.gating_block = GatedAttention(\n",
    "                in_size=num_input_channels, out_size=num_input_channels\n",
    "            )\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        residual = inputs\n",
    "        inputs = self.norm(inputs)\n",
    "      \n",
    "        inputs = inputs.permute(0, 3, 2, 1)\n",
    "        if self.gated_attn:\n",
    "            inputs = self.gating_block(inputs)\n",
    "\n",
    "        inputs = self.mlp(inputs)\n",
    "        inputs = inputs.permute(0, 3, 2, 1)\n",
    "        out = inputs + residual\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TSMixerFactorNetV2(L.LightningModule):\n",
    "    def __init__(self,\n",
    "                 input_dim = 6,\n",
    "                 d_model = 50,\n",
    "                 P = 4,\n",
    "                 S = 2,\n",
    "                 seq_len = 30,\n",
    "                 norm_type = 'batch',\n",
    "                 label_idx = 1,\n",
    "                 gamma = 0,\n",
    "                 channel_mixer = True,\n",
    "                 gated_attn = True,\n",
    "                 use_cls = False,\n",
    "                 pe_type = 'sincos',\n",
    "                 do_padding = 'end',\n",
    "                 expansion_factor = 1,\n",
    "                 dropout = 0\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.P = P          # patch 窗口大小\n",
    "        self.S = S          # patch 步长\n",
    "        self.use_cls = use_cls\n",
    "        \n",
    "        self.N = int((seq_len - P) / S + 1)\n",
    "        self.M = input_dim\n",
    "        self.patch_encoder = TSTPatchEmbed(\n",
    "                            seq_len = seq_len,\n",
    "                            P = P,\n",
    "                            S = S,\n",
    "                            d_model = d_model,\n",
    "                            do_padding = do_padding,\n",
    "                            do_pe = False\n",
    "                            )\n",
    "        if do_padding == 'end':\n",
    "            self.N += 1\n",
    "        if use_cls:\n",
    "            self.N += 1\n",
    "            \n",
    "        self.pe_enc = PositionalEncoding(seq_len = self.N,\n",
    "                            num_input_channels = input_dim,\n",
    "                            d_model = d_model,\n",
    "                            use_cls_token = use_cls,\n",
    "                            positional_encoding_type = pe_type\n",
    "                            )\n",
    "\n",
    "        # -------backbone-------\n",
    "        ts_mixer = TsMixerBlock(num_patches = self.N,\n",
    "                        d_model = d_model,\n",
    "                        gated_attn = gated_attn,\n",
    "                        norm_type = norm_type,\n",
    "                        dropout = dropout,\n",
    "                        expansion_factor = expansion_factor\n",
    "                    )\n",
    "        \n",
    "        feature_mixer = FeatureMixerBlock(d_model=d_model,\n",
    "                        gated_attn = gated_attn,\n",
    "                        norm_type = norm_type,\n",
    "                        dropout = dropout,\n",
    "                        expansion_factor = expansion_factor\n",
    "                    )\n",
    "        \n",
    "        mixer_list = [ts_mixer,feature_mixer]\n",
    "        \n",
    "        if channel_mixer:\n",
    "            channel_mixer = ChannelMixerBlock(d_model=d_model,\n",
    "                            norm_type=norm_type,\n",
    "                            num_input_channels = self.M,\n",
    "                            gated_attn = gated_attn,\n",
    "                            dropout = dropout,\n",
    "                            expansion_factor = expansion_factor\n",
    "                    )\n",
    "            mixer_list.append(channel_mixer)\n",
    "        \n",
    "        self.mixer_backbone = nn.ModuleList(mixer_list)\n",
    "        \n",
    "        # -------head-------\n",
    "        self.ts_linear = nn.Linear(self.d_model * self.N, self.d_model)   \n",
    "        \n",
    "        self.label_idx = label_idx\n",
    "        # predict layer\n",
    "        \n",
    "        self.predict_layer = nn.Sequential(\n",
    "            nn.Linear(self.M * self.d_model, 50)\n",
    "        )\n",
    "        self.loss_fn = ICLoss(gamma)\n",
    "       \n",
    "\n",
    "    def forward(self,inputs):\n",
    "        z = self.patch_encoder(inputs)  # BM N D\n",
    "        \n",
    "        z = rearrange(z,'(b m) n d -> b m n d',m = self.M)\n",
    "        z = self.pe_enc(z) # 加入位置编码\n",
    "        \n",
    "        for mixer in self.mixer_backbone:\n",
    "            z = mixer(z)\n",
    "        \n",
    "        if self.use_cls:\n",
    "            x_out = z[:,:,0,:] # b m n d -> b m d\n",
    "            x_out = rearrange(x_out, 'b m d -> b (m d)')\n",
    "        else:\n",
    "            z = rearrange(z, 'b m n d -> b m (n d)')\n",
    "            x_out = F.gelu(self.ts_linear(z)) # [B, M, predict_len]\n",
    "            x_out = rearrange(x_out, 'b m d -> b (m d)') # [B, M * predict_len]\n",
    "        return x_out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs = batch[0].squeeze(0)\n",
    "        x_out = self.forward(inputs)\n",
    "        y_pred = self.predict_layer(x_out)\n",
    "        \n",
    "        labels  = batch[1].squeeze(0)[:,self.label_idx].reshape(-1,1)\n",
    "        loss = self.loss_fn(y_pred,labels)\n",
    "        self.log(\"train_loss\",loss, on_epoch=True,prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        inputs = batch[0].squeeze(0)\n",
    "        ids = batch[1].squeeze(0)[:,0]\n",
    "        x_out = self.forward(inputs)\n",
    "        y_pred = self.predict_layer(x_out)\n",
    "        return torch.cat((y_pred,ids.unsqueeze(1)),1).cpu().numpy()\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        inputs = batch[0].squeeze(0)\n",
    "        x_out = self.forward(inputs)\n",
    "        y_pred = self.predict_layer(x_out)\n",
    "        labels  = batch[1].squeeze(0)[:,self.label_idx].reshape(-1,1)\n",
    "        loss = self.loss_fn(y_pred,labels)\n",
    "        self.log(\"val_loss\",loss,prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"input_dim\": 6,\n",
    "    \"d_model\": 16,\n",
    "    \"P\": 4,\n",
    "    \"S\": 2,\n",
    "    \"dropout\": 0.5,\n",
    "    \"use_cls\": False,\n",
    "    \"expansion_factor\": 2,\n",
    "}\n",
    "   \n",
    "model = TSMixerFactorNetV2(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape: B L M\n",
    "x_in  = torch.rand(1024, 30, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 96])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调用forward模块的结果\n",
    "# 用CLS时 输出维度为 B M*D\n",
    "x_out = model(x_in)\n",
    "x_out.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 6\n",
    "D = 16\n",
    "ts_linear = nn.Linear(M*D,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 50])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_final = ts_linear(x_out)\n",
    "x_final.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
