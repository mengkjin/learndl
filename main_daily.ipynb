{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hidden.1.swalast.20230606'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "#data = Path('d:\\\\Coding\\\\learndl\\\\learndl\\\\src\\\\configs\\\\fmp\\\\default.yaml')\n",
    "#list(os.walk(data))\n",
    "directory = Path('./data/ModelHidden')\n",
    "[Path(dirpath).joinpath(filename) for dirpath, _, filenames in os.walk(directory) for filename in filenames][-1].stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "p = Path('d:/Coding/learndl/learndl/src/configs/fmp/default.yaml')\n",
    "q = Path('d:/Coding/learndl/learndl/src/configs/fmp/default.yaml')\n",
    "p.match(f'*default*.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.boost import GeneralBooster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train':                  Amount_20D_Avg  Amount_240D_Avg  Asset_LR  Asset_LR_Gr  \\\n",
       " date     secid                                                            \n",
       " 20211029 1             0.875544         0.875703  0.375270     0.775000   \n",
       "          2             0.993958         0.994373  0.994009     0.516299   \n",
       "          4             0.755437         0.861125  0.301701     0.438971   \n",
       "          5             0.181489         0.158951  0.389648     0.067157   \n",
       "          6             0.166022         0.183120  0.246585     0.946078   \n",
       " ...                         ...              ...       ...          ...   \n",
       " 20230831 871970        0.010362         0.010567  0.030890     0.458824   \n",
       "          871981        0.028687         0.030800  0.077007     0.230937   \n",
       "          872925        0.029014         0.025629  0.018490     0.662309   \n",
       "          873169        0.025742         0.055755  0.021101     0.489107   \n",
       "          873223        0.029668         0.074978  0.037198     0.037473   \n",
       " \n",
       "                  Asset_LR_Ln  Bias_20D   Bias_5D  Bias_60D     BP_LR  \\\n",
       " date     secid                                                         \n",
       " 20211029 1          0.375270  0.775375  0.175523  0.776047  0.124611   \n",
       "          2          0.994009  0.039633  0.054220  0.183057  0.579679   \n",
       "          4          0.301701  0.079507  0.163982  0.112707  0.666906   \n",
       "          5          0.389648  0.840744  0.742486  0.784810  0.654086   \n",
       "          6          0.246585  0.404906  0.738278  0.309153  0.500120   \n",
       " ...                      ...       ...       ...       ...       ...   \n",
       " 20230831 871970     0.030890  0.113983  0.214348  0.177692  0.928975   \n",
       "          871981     0.077007  0.173320  0.052036  0.347692  0.810746   \n",
       "          872925     0.018490  0.023560  0.001089  0.063736  0.593213   \n",
       "          873169     0.021101  0.057155  0.129980  0.192967  0.979117   \n",
       "          873223     0.037198  0.108421  0.051600  0.225495  0.959321   \n",
       " \n",
       "                  BP_LR_Tangible  ...  TargetReturn  ThreeCosts2Revenue_TTM  \\\n",
       " date     secid                   ...                                         \n",
       " 20211029 1             0.124611  ...      0.826789                0.500124   \n",
       "          2             0.579679  ...      0.845155                0.154084   \n",
       "          4             0.158160  ...           NaN                0.369554   \n",
       "          5             0.808172  ...           NaN                0.755941   \n",
       "          6             0.532231  ...           NaN                0.476856   \n",
       " ...                         ...  ...           ...                     ...   \n",
       " 20230831 871970        0.928975  ...           NaN                0.867278   \n",
       "          871981        0.836198  ...           NaN                0.555058   \n",
       "          872925        0.666957  ...           NaN                0.375336   \n",
       "          873169        0.985860  ...           NaN                0.358102   \n",
       "          873223        0.966717  ...           NaN                0.691137   \n",
       " \n",
       "                  Turnover_20D_Avg  Turnover_240D_Avg  Turnover_Close_20D_Corr  \\\n",
       " date     secid                                                                  \n",
       " 20211029 1               0.824915           0.624808                 0.075278   \n",
       "          2               0.817545           0.642711                 0.087482   \n",
       "          4               0.825520           0.909207                 0.655631   \n",
       "          5               0.115273           0.229284                 0.181489   \n",
       "          6               0.387989           0.468031                 0.579749   \n",
       " ...                           ...                ...                      ...   \n",
       " 20230831 871970          0.032068           0.052158                 0.302029   \n",
       "          871981          0.021924           0.037995                 0.154123   \n",
       "          872925          0.039485           0.053058                 0.092932   \n",
       "          873169          0.025742           0.230216                 0.092059   \n",
       "          873223          0.037522           0.379946                 0.065227   \n",
       " \n",
       "                  Turnover_Close_5D_Corr  Turnover_Close_60D_Corr  \\\n",
       " date     secid                                                     \n",
       " 20211029 1                     0.924742                 0.074732   \n",
       "          2                     0.246213                 0.087634   \n",
       "          4                     0.334215                 0.836173   \n",
       "          5                     0.094253                 0.040409   \n",
       "          6                     0.452392                 0.626826   \n",
       " ...                                 ...                      ...   \n",
       " 20230831 871970                0.438711                 0.344286   \n",
       "          871981                0.239713                 0.606923   \n",
       "          872925                0.476159                 0.707473   \n",
       "          873169                0.129980                 0.003956   \n",
       "          873223                0.200305                 0.221758   \n",
       " \n",
       "                  Vol_20D_240D_Avg  Vol_20D_CV     label  \n",
       " date     secid                                           \n",
       " 20211029 1               0.875353    0.775375 -0.105641  \n",
       "          2               0.896384    0.642943  0.002746  \n",
       "          4               0.463452    0.420735  0.120905  \n",
       "          5               0.326494    0.555341  0.000000  \n",
       "          6               0.389459    0.674239 -0.016787  \n",
       " ...                           ...         ...       ...  \n",
       " 20230831 871970          0.521143    0.863765 -0.047478  \n",
       "          871981          0.038012    0.834097 -0.063205  \n",
       "          872925          0.567701    0.949825  0.350190  \n",
       "          873169          0.011134    0.954188 -0.008114  \n",
       "          873223          0.004948    0.805192 -0.014599  \n",
       " \n",
       " [101371 rows x 170 columns],\n",
       " 'valid':                  Amount_20D_Avg  Amount_240D_Avg  Asset_LR  Asset_LR_Gr  \\\n",
       " date     secid                                                            \n",
       " 20230928 1             0.775615         0.925224  0.374919     0.224880   \n",
       "          2             0.935066         0.991256  0.991430     0.268378   \n",
       "          4             0.760714         0.468834  0.046865     0.002066   \n",
       "          5             0.077877         0.072197  0.423085     0.685950   \n",
       "          6             0.657494         0.731390  0.286939     0.842105   \n",
       " ...                         ...              ...       ...          ...   \n",
       "          871970        0.010224         0.010538  0.031026     0.479556   \n",
       "          871981        0.034805         0.031726  0.076589     0.230100   \n",
       "          872925        0.039374         0.031166  0.018117     0.659091   \n",
       "          873169        0.003807         0.050897  0.021046     0.475642   \n",
       "          873223        0.025125         0.054709  0.037101     0.041975   \n",
       " \n",
       "                  Asset_LR_Ln  Bias_20D   Bias_5D  Bias_60D     BP_LR  \\\n",
       " date     secid                                                         \n",
       " 20230928 1          0.374919  0.124429  0.825043  0.074536  0.325125   \n",
       "          2          0.991430  0.306069  0.509666  0.657268  0.749946   \n",
       "          4          0.046865  0.411246  0.061034  0.140874  0.082013   \n",
       "          5          0.423085  0.894061  0.832537  0.966776  0.832285   \n",
       "          6          0.286939  0.379269  0.620873  0.231913  0.472337   \n",
       " ...                      ...       ...       ...       ...       ...   \n",
       "          871970     0.031026  0.091364  0.479800  0.113989  0.928835   \n",
       "          871981     0.076589  0.121057  0.873805  0.085683  0.873725   \n",
       "          872925     0.018117  0.993909  0.994027  0.981202  0.452484   \n",
       "          873169     0.021046  0.173700  0.338076  0.185574  0.979171   \n",
       "          873223     0.037101  0.169132  0.245004  0.208634  0.959427   \n",
       " \n",
       "                  BP_LR_Tangible  ...  TargetReturn  ThreeCosts2Revenue_TTM  \\\n",
       " date     secid                   ...                                         \n",
       " 20230928 1             0.325125  ...      0.874470                0.500112   \n",
       "          2             0.787481  ...      0.500265                0.120844   \n",
       "          4             0.001953  ...           NaN                0.832403   \n",
       "          5             0.914949  ...           NaN                0.851149   \n",
       "          6             0.527880  ...           NaN                0.657331   \n",
       " ...                         ...  ...           ...                     ...   \n",
       "          871970        0.928835  ...           NaN                0.867663   \n",
       "          871981        0.886309  ...           NaN                0.571747   \n",
       "          872925        0.500108  ...           NaN                0.367998   \n",
       "          873169        0.985897  ...           NaN                0.356728   \n",
       "          873223        0.963116  ...           NaN                0.692256   \n",
       " \n",
       "                  Turnover_20D_Avg  Turnover_240D_Avg  Turnover_Close_20D_Corr  \\\n",
       " date     secid                                                                  \n",
       " 20230928 1               0.775615           0.825000                 0.224603   \n",
       "          2               0.361540           0.416480                 0.713183   \n",
       "          4               0.775615           0.630045                 0.353491   \n",
       "          5               0.271481           0.148655                 0.472482   \n",
       "          6               0.805960           0.953139                 0.953557   \n",
       " ...                           ...                ...                      ...   \n",
       "          871970          0.030890           0.052466                 0.275615   \n",
       "          871981          0.065151           0.031726                 0.540570   \n",
       "          872925          0.231020           0.053139                 0.204699   \n",
       "          873169          0.009463           0.113453                 0.122362   \n",
       "          873223          0.033609           0.153587                 0.554601   \n",
       " \n",
       "                  Turnover_Close_5D_Corr  Turnover_Close_60D_Corr  \\\n",
       " date     secid                                                     \n",
       " 20230928 1                     0.975777                 0.675410   \n",
       "          2                     0.601890                 0.620328   \n",
       "          4                     0.597002                 0.593333   \n",
       "          5                     0.286118                 0.173552   \n",
       "          6                     0.528134                 0.991475   \n",
       " ...                                 ...                      ...   \n",
       "          871970                0.071475                 0.051475   \n",
       "          871981                0.070823                 0.293552   \n",
       "          872925                0.907886                 0.496393   \n",
       "          873169                0.940474                 0.081530   \n",
       "          873223                0.604388                 0.200219   \n",
       " \n",
       "                  Vol_20D_240D_Avg  Vol_20D_CV     label  \n",
       " date     secid                                           \n",
       " 20230928 1               0.024798    0.525234 -0.066071  \n",
       "          2               0.268402    0.694148 -0.133792  \n",
       "          4               0.842908    0.532739  0.082346  \n",
       "          5               0.630162    0.812486 -0.020690  \n",
       "          6               0.176167    0.953557 -0.048458  \n",
       " ...                           ...         ...       ...  \n",
       "          871970          0.839991    0.989993 -0.006231  \n",
       "          871981          0.705117    0.948662 -0.145382  \n",
       "          872925          0.993043    0.928432 -0.117978  \n",
       "          873169          0.000449    0.948336 -0.067485  \n",
       "          873223          0.065305    0.869589 -0.085185  \n",
       " \n",
       " [4609 rows x 170 columns],\n",
       " 'test':                  Amount_20D_Avg  Amount_240D_Avg  Asset_LR  Asset_LR_Gr  \\\n",
       " date     secid                                                            \n",
       " 20231031 1             0.726094         0.925343  0.375496     0.223964   \n",
       "          2             0.972259         0.991340  0.991120     0.360337   \n",
       "          4             0.963265         0.644734  0.059890     0.001936   \n",
       "          5             0.116455         0.123432  0.453240     0.665699   \n",
       "          6             0.435334         0.750050  0.306065     0.786585   \n",
       " ...                         ...              ...       ...          ...   \n",
       "          873339        0.036073         0.037627  0.107595     0.768002   \n",
       "          873527        0.009468         0.009954  0.067448     0.875629   \n",
       "          873576        0.055955              NaN  0.099849     0.964189   \n",
       "          873593        0.761693              NaN  0.151332     0.948703   \n",
       "          873665        0.212270              NaN  0.102399     0.912698   \n",
       " \n",
       "                  Asset_LR_Ln  Bias_20D   Bias_5D  Bias_60D     BP_LR  \\\n",
       " date     secid                                                         \n",
       " 20231031 1          0.375496  0.224673  0.824664  0.074529  0.474400   \n",
       "          2          0.991120  0.083128  0.102421  0.213783  0.768751   \n",
       "          4          0.059890  0.741905  0.786079  0.772892  0.048744   \n",
       "          5          0.453240  0.193524  0.365992  0.691034  0.866238   \n",
       "          6          0.306065  0.509373  0.397674  0.324005  0.490270   \n",
       " ...                      ...       ...       ...       ...       ...   \n",
       "          873339     0.107595  0.321814  0.440893  0.226156  0.868317   \n",
       "          873527     0.067448  0.144291  0.163798  0.240529  0.952012   \n",
       "          873576     0.099849  0.988260  0.966427  0.766324  0.188929   \n",
       "          873593     0.151332  0.006154  0.927747  0.000381  0.122804   \n",
       "          873665     0.102399  0.546298  0.857859       NaN  0.942188   \n",
       " \n",
       "                  BP_LR_Tangible  ...  TargetReturn  ThreeCosts2Revenue_TTM  \\\n",
       " date     secid                   ...                                         \n",
       " 20231031 1             0.474400  ...      0.816628                0.500097   \n",
       "          2             0.805970  ...      0.634642                0.176738   \n",
       "          4             0.001700  ...           NaN                0.838079   \n",
       "          5             0.935953  ...           NaN                0.887662   \n",
       "          6             0.490270  ...           NaN                0.657854   \n",
       " ...                         ...  ...           ...                     ...   \n",
       "          873339        0.868317  ...           NaN                0.297308   \n",
       "          873527        0.952012  ...           NaN                0.278520   \n",
       "          873576        0.277725  ...           NaN                0.077087   \n",
       "          873593        0.136596  ...      0.997229                0.705210   \n",
       "          873665        0.962592  ...           NaN                0.576022   \n",
       " \n",
       "                  Turnover_20D_Avg  Turnover_240D_Avg  Turnover_Close_20D_Corr  \\\n",
       " date     secid                                                                  \n",
       " 20231031 1               0.824560           0.824806                 0.124598   \n",
       "          2               0.472354           0.416783                 0.064571   \n",
       "          4               0.928612           0.734023                 0.967714   \n",
       "          5               0.237455           0.169819                 0.486650   \n",
       "          6               0.657641           0.935198                 0.306287   \n",
       " ...                           ...                ...                      ...   \n",
       "          873339          0.012308           0.037627                 0.155084   \n",
       "          873527          0.009468           0.009954                 0.106609   \n",
       "          873576          0.300227                NaN                 0.900303   \n",
       "          873593          0.816701                NaN                 0.029729   \n",
       "          873665          0.933725                NaN                 0.982579   \n",
       " \n",
       "                  Turnover_Close_5D_Corr  Turnover_Close_60D_Corr  \\\n",
       " date     secid                                                     \n",
       " 20231031 1                     0.624929                 0.575576   \n",
       "          2                     0.119822                 0.324005   \n",
       "          4                     0.951390                 0.864078   \n",
       "          5                     0.031398                 0.204074   \n",
       "          6                     0.750142                 0.953931   \n",
       " ...                                 ...                      ...   \n",
       "          873339                0.321260                 0.132115   \n",
       "          873527                0.606204                 0.470683   \n",
       "          873576                0.966427                 0.900628   \n",
       "          873593                0.740496                 0.051399   \n",
       "          873665                0.881502                      NaN   \n",
       " \n",
       "                  Vol_20D_240D_Avg  Vol_20D_CV     label  \n",
       " date     secid                                           \n",
       " 20231031 1               0.025209    0.375308 -0.074570  \n",
       "          2               0.471802    0.509373  0.009709  \n",
       "          4               0.952272    0.565423  0.000000  \n",
       "          5               0.638501    0.814618 -0.014085  \n",
       "          6               0.008868    0.213975  0.071759  \n",
       " ...                           ...         ...       ...  \n",
       "          873339          0.012754    0.559364  0.328084  \n",
       "          873527          0.068553    0.779019  0.288026  \n",
       "          873576               NaN    0.878621  0.435461  \n",
       "          873593               NaN    0.641166  0.880506  \n",
       "          873665               NaN    0.999432  0.567663  \n",
       " \n",
       " [5293 rows x 170 columns]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = GeneralBooster.df_input()\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 1, Est-Err: 0.4939, F_name: OCFA_Resd\n",
      "Round: 2, Est-Err: 0.4960, F_name: Amount_20D_Avg\n",
      "Round: 3, Est-Err: 0.4986, F_name: CL\n",
      "Round: 4, Est-Err: 0.4992, F_name: OCFA_Para\n",
      "Round: 5, Est-Err: 0.4992, F_name: BuyMinusSell_5D_Vol\n",
      "Round: 6, Est-Err: 0.4985, F_name: Rating_R3M\n",
      "Round: 7, Est-Err: 0.4995, F_name: Equity_LR_Gr\n",
      "Round: 8, Est-Err: 0.4996, F_name: Num_Emp\n",
      "Round: 9, Est-Err: 0.4997, F_name: BuyMinusSell_5D_Amount\n",
      "Round: 10, Est-Err: 0.4990, F_name: OCFA_Resd\n",
      "Round: 11, Est-Err: 0.4997, F_name: EPS_FY0_R1M\n",
      "Round: 12, Est-Err: 0.4998, F_name: CloseTrend_240D_Reg\n",
      "Round: 13, Est-Err: 0.4998, F_name: BuyMinusSell_5D_Vol\n",
      "Round: 14, Est-Err: 0.4997, F_name: Bias_5D\n",
      "Round: 15, Est-Err: 0.4998, F_name: ILLIQ_240D\n",
      "Round: 16, Est-Err: 0.4998, F_name: NetProfitDeducted_SQ_YoY\n",
      "Round: 17, Est-Err: 0.4998, F_name: MarketCap\n",
      "Round: 18, Est-Err: 0.4998, F_name: BuyMinusSell_5D_Amount\n",
      "Round: 19, Est-Err: 0.4997, F_name: FloatCap\n",
      "Round: 20, Est-Err: 0.4998, F_name: IVR_20D\n",
      "Round: 21, Est-Err: 0.4998, F_name: Vol_20D_240D_Avg\n",
      "Round: 22, Est-Err: 0.4999, F_name: ROE_Fwd12M\n",
      "Round: 23, Est-Err: 0.4999, F_name: TargetReturn\n",
      "Round: 24, Est-Err: 0.4999, F_name: BuyMinusSell_5D_Vol\n",
      "Round: 25, Est-Err: 0.4998, F_name: ROE_Fwd12M_R3M\n",
      "Round: 26, Est-Err: 0.4999, F_name: Rating_R1M\n",
      "Round: 27, Est-Err: 0.4999, F_name: EPS_FY0_R3M\n",
      "Round: 28, Est-Err: 0.4999, F_name: OCF_SQ_Acc\n",
      "Round: 29, Est-Err: 0.4999, F_name: Num_EPS_FY0\n",
      "Round: 30, Est-Err: 0.4999, F_name: Turnover_Close_5D_Corr\n"
     ]
    }
   ],
   "source": [
    "model1 = GeneralBooster('lgbm',**inputs).fit()\n",
    "model2 = GeneralBooster('ada',**inputs).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = GeneralBooster('ada',**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8755, 0.8757, 0.3753,  ..., 0.0747, 0.8754, 0.7754],\n",
       "        [0.9211, 0.8758, 0.3747,  ..., 0.1753, 0.9739, 0.8681],\n",
       "        [0.8757, 0.8762, 0.3748,  ..., 0.0756, 0.9757, 0.5760],\n",
       "        ...,\n",
       "        [0.0353, 0.0854, 0.0412,  ..., 0.3083, 0.0114, 0.9901],\n",
       "        [0.0379, 0.0791, 0.0408,  ..., 0.4471, 0.0391, 0.9704],\n",
       "        [0.0297, 0.0750, 0.0372,  ..., 0.2218, 0.0049, 0.8052]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.booster.data['train'].X()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = model2.booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.algo.boost import BoosterInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.algo.boost.model.ada import StrongLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train = self.data['train']\n",
    "valid = self.data['valid']\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() and self.cuda else 'cpu')\n",
    "train_data = BoosterInput.concat([train , valid])\n",
    "x , y , w = train_data.XYW(device)\n",
    "\n",
    "x = self.input_transform(x)\n",
    "y = self.label_transform(y)\n",
    "\n",
    "idx = (y != 0).squeeze()\n",
    "x , y , w = x[idx] , y[idx] , w[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4966, 0.4983, 0.4999,  ..., 0.4998, 0.4989, 0.4997],\n",
      "        [0.4966, 0.4983, 0.4999,  ..., 0.4998, 0.4989, 0.4997],\n",
      "        [0.4966, 0.4983, 0.4999,  ..., 0.4998, 0.4989, 0.4997],\n",
      "        ...,\n",
      "        [0.4966, 0.4983, 0.4999,  ..., 0.4998, 0.4989, 0.4997],\n",
      "        [0.4966, 0.4983, 0.4999,  ..., 0.4998, 0.4989, 0.4997],\n",
      "        [0.4966, 0.4983, 0.4999,  ..., 0.4998, 0.4989, 0.4997]])\n",
      "tensor(87)\n",
      "tensor([-0.1078, -0.1299, -0.1532, -0.1492, -0.0975, -0.0926, -0.2004, -0.1015,\n",
      "        -0.1528, -0.1478, -0.0877, -0.1673, -0.2075, -0.1547, -0.1217, -0.2157,\n",
      "        -0.1828, -0.1799, -0.2018, -0.1841])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to Tensor.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel : StrongLearner \u001b[38;5;241m=\u001b[39m StrongLearner(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_param)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilence\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding\\learndl\\learndl\\src\\algo\\boost\\model\\ada.py:100\u001b[0m, in \u001b[0;36mStrongLearner.fit\u001b[1;34m(self, x, y, w, silence, feature)\u001b[0m\n\u001b[0;32m     98\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_weight(w , y , y_pred)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silence \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m--> 100\u001b[0m     txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRound: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Est-Err: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearner\u001b[38;5;241m.\u001b[39mmin_feat_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \n\u001b[0;32m    102\u001b[0m         txt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, F_idx: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearner\u001b[38;5;241m.\u001b[39mfeat_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jinmeng\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:966\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[1;34m(self, format_spec)\u001b[0m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[1;32m--> 966\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__format__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_spec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported format string passed to Tensor.__format__"
     ]
    }
   ],
   "source": [
    "\n",
    "self.model : StrongLearner = StrongLearner(**self.train_param)\n",
    "self.model.fit(x , y , w , silence = False , feature = train_data.feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4939)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.model[0].feat_losses.nan_to_num(1).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = train_data.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = self.model.weak_learners[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.bin_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = learner\n",
    "weight = w\n",
    "X = x\n",
    "from torch import Tensor\n",
    "\n",
    "if weight is None: weight = torch.ones_like(y) / len(y)\n",
    "assert isinstance(X , Tensor) and isinstance(y , Tensor) and isinstance(weight , Tensor) , (X , y , weight)\n",
    "assert torch.all(X < self.n_bins) , X.max()\n",
    "assert not torch.is_floating_point(X) , X\n",
    "self.n_feat = X.shape[-1]\n",
    "\n",
    "pos_wgt , neg_wgt = (weight * (y > 0))[:,None] , (weight * (y < 0))[:,None]\n",
    "pos_imp = torch.zeros(self.n_feat , self.n_bins + 1).to(y)\n",
    "neg_imp = pos_imp * 0.\n",
    "\n",
    "for ibin in range(self.n_bins + 1):\n",
    "    where = X == ibin - 1\n",
    "    pos_imp[:,ibin] = (pos_wgt * where).sum(dim = 0)\n",
    "    neg_imp[:,ibin] = (neg_wgt * where).sum(dim = 0)\n",
    "\n",
    "feat_imp = (pos_imp + neg_imp).sum(-1,keepdim=True)\n",
    "nan_ratio = (pos_imp + neg_imp)[:,0]\n",
    "pos_imp , neg_imp = pos_imp / feat_imp , neg_imp / feat_imp\n",
    "\n",
    "self.feat_losses = torch.sqrt(pos_imp * neg_imp).sum(1).nan_to_num(torch.inf).where(nan_ratio <= 0.8 , torch.inf)\n",
    "self.feat_idx = self.feat_losses.argmin()\n",
    "self.bin_predictions = 0.5 * torch.log((pos_imp[self.feat_idx] + self.EPS) / (neg_imp[self.feat_idx] + self.EPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 45],\n",
       "        [ 46],\n",
       "        [159]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((pos_imp + neg_imp)[:,0] > 0.80).argwhere()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OCFA_Resd'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature[87]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0378, -0.0013, -0.0016, -0.0019, -0.0019, -0.0012, -0.0012, -0.0025,\n",
       "        -0.0013, -0.0019, -0.0018, -0.0011, -0.0021, -0.0026, -0.0020, -0.0016,\n",
       "        -0.0026, -0.0022, -0.0022, -0.0025, -0.0024])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_imp[87] - neg_imp[87]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(87)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.feat_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0054, -0.0063, -0.0075, -0.0077, -0.0047, -0.0047, -0.0100, -0.0051,\n",
       "        -0.0076, -0.0073, -0.0042, -0.0083, -0.0101, -0.0079, -0.0062, -0.0104,\n",
       "        -0.0089, -0.0087, -0.0101, -0.0094])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_imp[self.feat_idx] - neg_imp[self.feat_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibin = 0\n",
    "where = X == ibin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(70586)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X[:,45] == -1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 1, Est-Err: nan, F_name: FreeFloatCap_Ln\n",
      "Round: 2, Est-Err: nan, F_name: FreeFloatCap_Ln\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i , learner \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mweak_learners):  \u001b[38;5;66;03m# 使用 tqdm 显示进度条\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(x)\n\u001b[0;32m      3\u001b[0m     w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mupdate_weight(w , y , y_pred)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n",
      "File \u001b[1;32md:\\Coding\\learndl\\learndl\\src\\algo\\boost\\model\\ada.py:158\u001b[0m, in \u001b[0;36mWeakLearner.fit\u001b[1;34m(self, X, y, weight)\u001b[0m\n\u001b[0;32m    156\u001b[0m     where \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m==\u001b[39m ibin\n\u001b[0;32m    157\u001b[0m     pos_imp[:,ibin] \u001b[38;5;241m=\u001b[39m (pos_wgt \u001b[38;5;241m*\u001b[39m where)\u001b[38;5;241m.\u001b[39msum(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 158\u001b[0m     neg_imp[:,ibin] \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mneg_wgt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m feat_imp \u001b[38;5;241m=\u001b[39m (pos_imp \u001b[38;5;241m+\u001b[39m neg_imp)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    161\u001b[0m pos_imp , neg_imp \u001b[38;5;241m=\u001b[39m pos_imp \u001b[38;5;241m/\u001b[39m feat_imp , neg_imp \u001b[38;5;241m/\u001b[39m feat_imp\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i , learner in enumerate(self.model.weak_learners):  # 使用 tqdm 显示进度条\n",
    "    y_pred = learner.fit(x , y , w).predict(x)\n",
    "    w = self.model.update_weight(w , y , y_pred)\n",
    "    if i % 1 == 0: \n",
    "        txt = f'Round: {i+1}, Est-Err: {learner.min_feat_loss:.4f}'\n",
    "        if feature is None: \n",
    "            txt += f', F_idx: {learner.feat_idx}'\n",
    "        else:\n",
    "            txt += f', F_name: {feature[learner.feat_idx]}'\n",
    "        print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lgbm"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update Files\n",
      "Fri Aug 23 01:09:37 2024 : Updated ~ DB_information\\calendar.feather Done! Cost 0.02 Secs\n",
      "Fri Aug 23 01:09:37 2024 : Updated ~ DB_information\\description.feather Done! Cost 0.04 Secs\n",
      "Fri Aug 23 01:09:37 2024 : Updated ~ DB_information\\st.feather Done! Cost 0.02 Secs\n",
      "Fri Aug 23 01:09:40 2024 : Updated ~ DB_information\\industry.feather Done! Cost 2.56 Secs\n",
      "Fri Aug 23 01:09:45 2024 : Updated ~ DB_information\\concepts.feather Done! Cost 5.18 Secs\n",
      "Fri Aug 23 01:09:46 2024 : Updated ~ DB_models\\longcl_exp\\2024\\longcl_exp.20240821.feather Done! Cost 0.11 Secs\n",
      "Fri Aug 23 01:09:46 2024 : Updated ~ DB_models\\risk_exp\\2024\\risk_exp.20240822.feather Done! Cost 0.06 Secs\n",
      "Fri Aug 23 01:09:46 2024 : Updated ~ DB_models\\risk_cov\\2024\\risk_cov.20240822.feather Done! Cost 0.03 Secs\n",
      "Fri Aug 23 01:09:46 2024 : Updated ~ DB_models\\risk_spec\\2024\\risk_spec.20240822.feather Done! Cost 0.02 Secs\n",
      "Fri Aug 23 01:09:48 2024 : Updated ~ DB_trade\\day\\2024\\day.20240822.feather Done! Cost 0.05 Secs\n",
      "Fri Aug 23 01:09:48 2024 : Updated ~ DB_trade\\5day\\2024\\5day.20240822.feather Done! Cost 0.23 Secs\n",
      "Fri Aug 23 01:09:49 2024 : Updated ~ DB_trade\\10day\\2024\\10day.20240822.feather Done! Cost 0.46 Secs\n",
      "Fri Aug 23 01:09:50 2024 : Updated ~ DB_trade\\20day\\2024\\20day.20240822.feather Done! Cost 0.86 Secs\n",
      "Fri Aug 23 01:09:53 2024 : Updated ~ DB_trade\\min\\2024\\min.20240822.feather Done! Cost 3.10 Secs\n",
      "Fri Aug 23 01:09:53 2024 : Updated ~ DB_trade\\5min\\2024\\5min.20240822.feather Done! Cost 0.32 Secs\n",
      "Fri Aug 23 01:09:53 2024 : Updated ~ DB_trade\\10min\\2024\\10min.20240822.feather Done! Cost 0.27 Secs\n",
      "Fri Aug 23 01:09:54 2024 : Updated ~ DB_trade\\15min\\2024\\15min.20240822.feather Done! Cost 0.27 Secs\n",
      "Fri Aug 23 01:09:54 2024 : Updated ~ DB_trade\\30min\\2024\\30min.20240822.feather Done! Cost 0.25 Secs\n",
      "Fri Aug 23 01:09:54 2024 : Updated ~ DB_trade\\60min\\2024\\60min.20240822.feather Done! Cost 0.25 Secs\n",
      "Fri Aug 23 01:09:56 2024 : Updated ~ DB_labels\\ret20_lag\\2024\\ret20_lag.20240724.feather Done! Cost 0.70 Secs\n",
      "Fri Aug 23 01:09:57 2024 : Updated ~ DB_labels\\ret20\\2024\\ret20.20240725.feather Done! Cost 0.68 Secs\n",
      "Fri Aug 23 01:10:05 2024 : Updated ~ DB_labels\\ret10_lag\\2024\\ret10_lag.20240807.feather Done! Cost 0.51 Secs\n",
      "Fri Aug 23 01:10:07 2024 : Updated ~ DB_labels\\ret10\\2024\\ret10.20240808.feather Done! Cost 0.63 Secs\n",
      "Fri Aug 23 01:10:14 2024 : Updated ~ DB_labels\\ret5_lag\\2024\\ret5_lag.20240814.feather Done! Cost 0.48 Secs\n",
      "Fri Aug 23 01:10:16 2024 : Updated ~ DB_labels\\ret5\\2024\\ret5.20240815.feather Done! Cost 0.47 Secs\n",
      "Fri Aug 23 01:10:30 2024 : download since!\n",
      "Connection and Factor preparation finished!\n",
      "Fri Aug 23 01:10:30 2024 : sellside/dongfang.hfq_chars from 20240822 to 20240823, total 1 periods\n",
      "Start sellside/dongfang.hfq_chars:20240822-20240823 \n",
      "Done sellside/dongfang.hfq_chars:20240822-20240823, cost 2.2 Secs\n",
      "Fri Aug 23 01:10:32 2024 : sellside/dongfang.l2_chars from 20240821 to 20240823, total 1 periods\n",
      "Start sellside/dongfang.l2_chars:20240821-20240823 \n",
      "Done sellside/dongfang.l2_chars:20240821-20240823, cost 33.5 Secs\n",
      "Fri Aug 23 01:11:06 2024 : sellside/dongfang.ms_chars from 20240822 to 20240823, total 1 periods\n",
      "Start sellside/dongfang.ms_chars:20240822-20240823 \n",
      "Done sellside/dongfang.ms_chars:20240822-20240823, cost 47.1 Secs\n",
      "Fri Aug 23 01:11:53 2024 : sellside/dongfang.order_flow from 20240822 to 20240823, total 1 periods\n",
      "Start sellside/dongfang.order_flow:20240822-20240823 \n",
      "Done sellside/dongfang.order_flow:20240822-20240823, cost 0.8 Secs\n",
      "Fri Aug 23 01:11:54 2024 : sellside/dongfang.gp from 20240821 to 20240823, total 1 periods\n",
      "Start sellside/dongfang.gp:20240821-20240823 \n",
      "Done sellside/dongfang.gp:20240821-20240823, cost 1.8 Secs\n",
      "Fri Aug 23 01:11:55 2024 : sellside/dongfang.tra from 20240821 to 20240823, total 1 periods\n",
      "Start sellside/dongfang.tra:20240821-20240823 \n",
      "Done sellside/dongfang.tra:20240821-20240823, cost 1.4 Secs\n",
      "Fri Aug 23 01:11:57 2024 : sellside/dongfang.hist from 20240821 to 20240823, total 1 periods\n",
      "Start sellside/dongfang.hist:20240821-20240823 \n",
      "Done sellside/dongfang.hist:20240821-20240823, cost 1.2 Secs\n",
      "Fri Aug 23 01:11:58 2024 : sellside/dongfang.scores_v0 from 20240822 to 20240823, total 1 periods\n",
      "Start sellside/dongfang.scores_v0:20240822-20240823 \n",
      "Done sellside/dongfang.scores_v0:20240822-20240823, cost 1.5 Secs\n",
      "Fri Aug 23 01:12:00 2024 : sellside/dongfang.factorvae from 20240821 to 20240823, total 1 periods\n",
      "Start sellside/dongfang.factorvae:20240821-20240823 \n",
      "Done sellside/dongfang.factorvae:20240821-20240823, cost 1.2 Secs\n",
      "Fri Aug 23 01:12:01 2024 : sellside/huatai.dl_factors from 20240821 to 20240823, total 1 periods\n",
      "Start sellside/huatai.dl_factors:20240821-20240823 \n",
      "Done sellside/huatai.dl_factors:20240821-20240823, cost 9.8 Secs\n",
      "Fri Aug 23 01:12:11 2024 : All Updates Done! Cost 153.62 Secs\n",
      "--------------------------------------------------------------------------------\n",
      "Calendar Updating information_ts/calendar at 20240823\n",
      "Description Updating information_ts/description at 20240823\n",
      "SWIndustry Updating information_ts/industry at 20240823\n",
      "ChangeName Updating information_ts/change_name at 20240823\n",
      "THSConcept Already Updated at 20240731\n",
      "DailyValuation Updating trade_ts/day_val at 20240822\n",
      "DailyValuation Updating trade_ts/day_val at 20240823\n",
      "DailyQuote Updating trade_ts/day at 20240822\n",
      "DailyQuote Updating trade_ts/day at 20240823\n",
      "FinaIndicator Updating financial_ts/indicator at 20231231\n",
      "FinaIndicator Updating financial_ts/indicator at 20240331\n",
      "FinaIndicator Updating financial_ts/indicator at 20240630\n",
      "Finish exposure update at date 20240822\n",
      "Finish risk update at date 20240822\n",
      "--------------------------------------------------------------------------------\n",
      "predict is True , Data Processing start!\n",
      "6 datas :['y', 'day', '30m', 'style', 'indus', 'week']\n",
      "y blocks loading start!\n",
      " --> labels blocks reading [ret10_lag] DataBase...... finished! Cost 0.64 secs\n",
      " --> labels blocks reading [ret20_lag] DataBase...... finished! Cost 0.72 secs\n",
      " --> labels blocks merging (2)...... finished! Cost 0.15 secs\n",
      " --> models blocks reading [risk_exp] DataBase...... finished! Cost 2.77 secs\n",
      "y blocks loading finished! Cost 4.61 secs\n",
      "y blocks process...... finished! Cost 4.34 secs\n",
      "y blocks masking...... finished! Cost 0.09 secs\n",
      "y blocks saving ...... finished! Cost 0.12 secs\n",
      "y blocks norming...... finished! Cost 0.00 secs\n",
      "y finished! Cost 9.26 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "day blocks loading start!\n",
      " --> trade blocks reading [day] DataBase...... finished! Cost 0.98 secs\n",
      "day blocks loading finished! Cost 1.01 secs\n",
      "day blocks process...... finished! Cost 0.19 secs\n",
      "day blocks masking...... finished! Cost 0.08 secs\n",
      "day blocks saving ...... finished! Cost 0.13 secs\n",
      "day blocks norming...... finished! Cost 0.00 secs\n",
      "day finished! Cost 1.52 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "30m blocks loading start!\n",
      " --> trade blocks reading [30min] DataBase...... finished! Cost 5.41 secs\n",
      " --> trade blocks reading [day] DataBase...... finished! Cost 0.84 secs\n",
      "30m blocks loading finished! Cost 6.30 secs\n",
      "30m blocks process...... finished! Cost 1.63 secs\n",
      "30m blocks masking...... finished! Cost 0.15 secs\n",
      "30m blocks saving ...... finished! Cost 1.80 secs\n",
      "30m blocks norming...... finished! Cost 0.00 secs\n",
      "30m finished! Cost 10.00 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "style blocks loading start!\n",
      " --> models blocks reading [risk_exp] DataBase...... finished! Cost 1.57 secs\n",
      "style blocks loading finished! Cost 1.61 secs\n",
      "style blocks process...... finished! Cost 0.00 secs\n",
      "style blocks masking...... finished! Cost 0.10 secs\n",
      "style blocks saving ...... finished! Cost 0.23 secs\n",
      "style blocks norming...... finished! Cost 0.00 secs\n",
      "style finished! Cost 2.08 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "indus blocks loading start!\n",
      " --> models blocks reading [risk_exp] DataBase...... finished! Cost 2.64 secs\n",
      "indus blocks loading finished! Cost 2.65 secs\n",
      "indus blocks process...... finished! Cost 0.00 secs\n",
      "indus blocks masking...... finished! Cost 0.13 secs\n",
      "indus blocks saving ...... finished! Cost 1.31 secs\n",
      "indus blocks norming...... finished! Cost 0.00 secs\n",
      "indus finished! Cost 4.24 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "week blocks loading start!\n",
      " --> trade blocks reading [day] DataBase...... finished! Cost 1.84 secs\n",
      "week blocks loading finished! Cost 1.86 secs\n",
      "week blocks process...... finished! Cost 1.60 secs\n",
      "week blocks masking...... finished! Cost 0.20 secs\n",
      "week blocks saving ...... finished! Cost 2.41 secs\n",
      "week blocks norming...... finished! Cost 0.00 secs\n",
      "week finished! Cost 6.20 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "Data Processing Finished! Cost 33.29 Seconds\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'avg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m DataAPI\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m      4\u001b[0m DataAPI\u001b[38;5;241m.\u001b[39mprepare_predict_data()\n\u001b[1;32m----> 5\u001b[0m \u001b[43mPredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_factors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding\\learndl\\learndl\\src\\model\\api\\predictor.py:43\u001b[0m, in \u001b[0;36mPredictor.update_factors\u001b[1;34m(cls, silent)\u001b[0m\n\u001b[0;32m     41\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(model)\n\u001b[0;32m     42\u001b[0m CONF\u001b[38;5;241m.\u001b[39mSILENT \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[43mmd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdeploy()\n\u001b[0;32m     44\u001b[0m CONF\u001b[38;5;241m.\u001b[39mSILENT \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinish model [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] predicting!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Coding\\learndl\\learndl\\src\\model\\api\\predictor.py:61\u001b[0m, in \u001b[0;36mPredictor.get_df\u001b[1;34m(self, start_dt, end_dt)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_df\u001b[39m(\u001b[38;5;28mself\u001b[39m , start_dt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m , end_dt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20991231\u001b[39m):\n\u001b[0;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''save recent prediction to self.df'''\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_dt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart_dt\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_dt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mend_dt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\Coding\\learndl\\learndl\\src\\model\\api\\predictor.py:87\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[1;34m(self, start_dt, end_dt)\u001b[0m\n\u001b[0;32m     84\u001b[0m model_dates  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_model\u001b[38;5;241m.\u001b[39mmodel_dates \n\u001b[0;32m     85\u001b[0m start_dt     \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(start_dt , \u001b[38;5;28mint\u001b[39m(date_offset(\u001b[38;5;28mmin\u001b[39m(model_dates) ,\u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m---> 87\u001b[0m data_mod \u001b[38;5;241m=\u001b[39m \u001b[43mNetDataModule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mboth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstart_dt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoday\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     89\u001b[0m end_dt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(end_dt , \u001b[38;5;28mmax\u001b[39m(data_mod\u001b[38;5;241m.\u001b[39mtest_full_dates))\n\u001b[0;32m     90\u001b[0m pred_dates \u001b[38;5;241m=\u001b[39m GetData\u001b[38;5;241m.\u001b[39mtrade_dates(start_dt , end_dt)\n",
      "File \u001b[1;32md:\\Coding\\learndl\\learndl\\src\\model\\trainer\\trainers\\basic.py:40\u001b[0m, in \u001b[0;36mDataModule.load_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatas \u001b[38;5;241m=\u001b[39m \u001b[43mModuleData\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_type_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.labels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mfit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mupdate_data_param(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatas\u001b[38;5;241m.\u001b[39mx)\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatas\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] , \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mmax_num_output)\n",
      "File \u001b[1;32md:\\Coding\\learndl\\learndl\\src\\data\\core.py:450\u001b[0m, in \u001b[0;36mModuleData.load\u001b[1;34m(cls, data_type_list, y_labels, fit, predict, dtype, save_upon_loading)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mload_datas(data_type_list , y_labels , \u001b[38;5;28;01mFalse\u001b[39;00m , dtype , save_upon_loading)\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fit:\n\u001b[1;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_datas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_type_list\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_labels\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_upon_loading\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    452\u001b[0m     hist_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mload_datas(data_type_list , y_labels , \u001b[38;5;28;01mFalse\u001b[39;00m , dtype , save_upon_loading)\n",
      "File \u001b[1;32md:\\Coding\\learndl\\learndl\\src\\data\\core.py:491\u001b[0m, in \u001b[0;36mModuleData.load_datas\u001b[1;34m(cls, data_type_list, y_labels, predict, dtype, save_upon_loading)\u001b[0m\n\u001b[0;32m    488\u001b[0m data_type_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;241m*\u001b[39mdata_type_list]\n\u001b[0;32m    490\u001b[0m blocks \u001b[38;5;241m=\u001b[39m DataBlock\u001b[38;5;241m.\u001b[39mload_keys(data_type_list, predict , alias_search\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,dtype \u001b[38;5;241m=\u001b[39m dtype)\n\u001b[1;32m--> 491\u001b[0m norms  \u001b[38;5;241m=\u001b[39m \u001b[43mDataBlockNorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_type_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malias_search\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m y : DataBlock \u001b[38;5;241m=\u001b[39m blocks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    494\u001b[0m x : \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m,DataBlock] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mabbr(key):blocks[i] \u001b[38;5;28;01mfor\u001b[39;00m i,key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_type_list) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m}\n",
      "File \u001b[1;32md:\\Coding\\learndl\\learndl\\src\\data\\core.py:417\u001b[0m, in \u001b[0;36mDataBlockNorm.load_keys\u001b[1;34m(cls, keys, predict, alias_search, dtype)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_keys\u001b[39m(\u001b[38;5;28mcls\u001b[39m , keys : \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] , predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m , alias_search \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m , dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(keys , \u001b[38;5;28mlist\u001b[39m): keys \u001b[38;5;241m=\u001b[39m [keys]\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malias_search\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32md:\\Coding\\learndl\\learndl\\src\\data\\core.py:417\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_keys\u001b[39m(\u001b[38;5;28mcls\u001b[39m , keys : \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] , predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m , alias_search \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m , dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(keys , \u001b[38;5;28mlist\u001b[39m): keys \u001b[38;5;241m=\u001b[39m [keys]\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malias_search\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys]\n",
      "File \u001b[1;32md:\\Coding\\learndl\\learndl\\src\\data\\core.py:412\u001b[0m, in \u001b[0;36mDataBlockNorm.load_key\u001b[1;34m(cls, key, predict, alias_search, dtype)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_key\u001b[39m(\u001b[38;5;28mcls\u001b[39m , key : \u001b[38;5;28mstr\u001b[39m , predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m , alias_search \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m , dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    411\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_path(key , predict , alias_search)\n\u001b[1;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding\\learndl\\learndl\\src\\data\\core.py:401\u001b[0m, in \u001b[0;36mDataBlockNorm.load_path\u001b[1;34m(cls, path, dtype)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    400\u001b[0m data \u001b[38;5;241m=\u001b[39m DataBlock\u001b[38;5;241m.\u001b[39mload_dict(path)\n\u001b[1;32m--> 401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mavg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m , data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m'\u001b[39m] , dtype)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'avg'"
     ]
    }
   ],
   "source": [
    "from src.api import DataAPI , Predictor\n",
    "\n",
    "DataAPI.update()\n",
    "DataAPI.prepare_predict_data()\n",
    "Predictor.update_factors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data.tushare.download.info import Calendar , pro\n",
    "renamer = {'cal_date' : 'calendar' ,'is_open'  : 'trade'}\n",
    "fields = None\n",
    "df = pro.trade_cal(exchange='SSE').rename(columns=renamer)\n",
    "#df = df.sort_values('calendar').reset_index(drop = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish model [gru_day] predicting!\n",
      "Finish model [gruRTN_day] predicting!\n",
      "Finish model [gru_avg] predicting!\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# DataAPI.prepare_predict_data()\n",
    "Predictor.update_factors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Coding\\learndl\\learndl\\data\\HistNorm\\X_day.pt\n",
      "Finish model [gru_day] predicting!\n",
      "d:\\Coding\\learndl\\learndl\\data\\HistNorm\\X_day.pt\n",
      "Finish model [gruRTN_day] predicting!\n",
      "d:\\Coding\\learndl\\learndl\\data\\HistNorm\\X_day.pt\n",
      "Finish model [gru_avg] predicting!\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from src.api import DataAPI , Predictor\n",
    "# DataAPI.prepare_predict_data()\n",
    "Predictor.update_factors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  2 DataBlocks ..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... cost 0.22 secs\n",
      "Align 2 DataBlocks ...... cost 0.21 secs\n",
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n",
      "x shape is torch.Size([5064, 30, 6])\n",
      "y shape is torch.Size([5064, 1])\n",
      "Test Forward Success\n",
      "metrics :  Metrics.MetricOutput(loss=tensor(1.0688, grad_fn=<AddBackward0>), score=-0.015420470386743546, loss_item=1.068833589553833, penalty=0.0, losses=tensor(1.0688, grad_fn=<ExpBackward0>))\n",
      "Test Metrics Success\n"
     ]
    }
   ],
   "source": [
    "from src.api import ModelTestor\n",
    "ModelTestor.new('ts_mixer').try_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
