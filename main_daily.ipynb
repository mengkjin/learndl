{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import GetData\n",
    "daily_returns = GetData.daily_returns(20240101,20240602)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import DataBlock\n",
    "from typing import Literal\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "def nday_fut_ret(daily_returns : DataBlock , nday : int = 10 , lag : Literal[1,2] = 2 , step : int = 5 , retain_shape = False ,\n",
    "                 auto_rename = True):\n",
    "    assert lag > 0 , f'lag must be positive : {lag}'\n",
    "    block = deepcopy(daily_returns).as_tensor()\n",
    "    block.values = F.pad(block.values[:,lag:] , (0,0,0,0,0,lag),value=torch.nan)\n",
    "\n",
    "    new_value = block.values.unfold(1 , nday , step).exp().prod(dim = -1) - 1\n",
    "    new_date  = block.date[::step][:new_value.shape[1]]\n",
    "    secid     = block.secid\n",
    "    feature   = block.feature\n",
    "\n",
    "    if auto_rename: feature = [f'{nday}day_{feat}_ret_lag{lag-1}' for feat in feature]\n",
    "    new_block = DataBlock(new_value , secid , new_date , feature)\n",
    "    if retain_shape: new_block.align_date(block.date)\n",
    "    return new_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = nday_fut_ret(daily_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>10day_close_ret_lag1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>secid</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>20240102</th>\n",
       "      <td>0.003814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20240109</th>\n",
       "      <td>0.026858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20240116</th>\n",
       "      <td>0.025671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20240123</th>\n",
       "      <td>0.031273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20240130</th>\n",
       "      <td>0.158645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">689009</th>\n",
       "      <th>20240412</th>\n",
       "      <td>0.284709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20240419</th>\n",
       "      <td>0.287006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20240426</th>\n",
       "      <td>0.093814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20240508</th>\n",
       "      <td>0.039070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20240515</th>\n",
       "      <td>-0.063290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92232 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 10day_close_ret_lag1\n",
       "secid  date                          \n",
       "1      20240102              0.003814\n",
       "       20240109              0.026858\n",
       "       20240116              0.025671\n",
       "       20240123              0.031273\n",
       "       20240130              0.158645\n",
       "...                               ...\n",
       "689009 20240412              0.284709\n",
       "       20240419              0.287006\n",
       "       20240426              0.093814\n",
       "       20240508              0.039070\n",
       "       20240515             -0.063290\n",
       "\n",
       "[92232 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "block.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.arange(6).reshape(3,2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'tile'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtile\u001b[49m(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'tile'"
     ]
    }
   ],
   "source": [
    "np.arange(10).tile(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'by' is an invalid keyword argument for repeat()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m block\u001b[38;5;241m.\u001b[39msecid\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m2\u001b[39m) , \u001b[43mblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'by' is an invalid keyword argument for repeat()"
     ]
    }
   ],
   "source": [
    "block.secid.repeat(2) , block.date.repeat(1,by=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0146,     nan,     nan,     nan,     nan, -0.0019,     nan,     nan,\n",
       "             nan,     nan,  0.0179,     nan,     nan,     nan,     nan,  0.0582,\n",
       "             nan,     nan,     nan,     nan,  0.1431,     nan,     nan,     nan,\n",
       "             nan,  0.0895,     nan,     nan,     nan,     nan, -0.0428,     nan,\n",
       "             nan,     nan,     nan, -0.0146,     nan,     nan,     nan,     nan,\n",
       "          0.0128,     nan,     nan,     nan,     nan,  0.0205,     nan,     nan,\n",
       "             nan,     nan,  0.0014,     nan,     nan,     nan,     nan, -0.0443,\n",
       "             nan,     nan,     nan,     nan,  0.0233,     nan,     nan,     nan,\n",
       "             nan,  0.0549,     nan,     nan,     nan,     nan,  0.0043,     nan,\n",
       "             nan,     nan,     nan,  0.0222,     nan,     nan,     nan,     nan,\n",
       "          0.0785,     nan,     nan,     nan,     nan,  0.0410,     nan,     nan,\n",
       "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "             nan,     nan],\n",
       "        [-0.0527,     nan,     nan,     nan,     nan, -0.0695,     nan,     nan,\n",
       "             nan,     nan,  0.0228,     nan,     nan,     nan,     nan,  0.0403,\n",
       "             nan,     nan,     nan,     nan,  0.0469,     nan,     nan,     nan,\n",
       "             nan,  0.0470,     nan,     nan,     nan,     nan, -0.0845,     nan,\n",
       "             nan,     nan,     nan, -0.0202,     nan,     nan,     nan,     nan,\n",
       "          0.0062,     nan,     nan,     nan,     nan, -0.0486,     nan,     nan,\n",
       "             nan,     nan, -0.1223,     nan,     nan,     nan,     nan, -0.2249,\n",
       "             nan,     nan,     nan,     nan, -0.1788,     nan,     nan,     nan,\n",
       "             nan, -0.0302,     nan,     nan,     nan,     nan,  0.0824,     nan,\n",
       "             nan,     nan,     nan,  0.1336,     nan,     nan,     nan,     nan,\n",
       "          0.3165,     nan,     nan,     nan,     nan,  0.1208,     nan,     nan,\n",
       "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "             nan,     nan],\n",
       "        [-0.0516,     nan,     nan,     nan,     nan, -0.1481,     nan,     nan,\n",
       "             nan,     nan, -0.0907,     nan,     nan,     nan,     nan, -0.2961,\n",
       "             nan,     nan,     nan,     nan, -0.1965,     nan,     nan,     nan,\n",
       "             nan,  0.3432,     nan,     nan,     nan,     nan,  0.3111,     nan,\n",
       "             nan,     nan,     nan,  0.1431,     nan,     nan,     nan,     nan,\n",
       "          0.0984,     nan,     nan,     nan,     nan,  0.0678,     nan,     nan,\n",
       "             nan,     nan, -0.1011,     nan,     nan,     nan,     nan, -0.1482,\n",
       "             nan,     nan,     nan,     nan, -0.2332,     nan,     nan,     nan,\n",
       "             nan, -0.1094,     nan,     nan,     nan,     nan,  0.1049,     nan,\n",
       "             nan,     nan,     nan,  0.0907,     nan,     nan,     nan,     nan,\n",
       "          0.0501,     nan,     nan,     nan,     nan, -0.0836,     nan,     nan,\n",
       "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "             nan,     nan],\n",
       "        [-0.0399,     nan,     nan,     nan,     nan, -0.0046,     nan,     nan,\n",
       "             nan,     nan, -0.0709,     nan,     nan,     nan,     nan, -0.2609,\n",
       "             nan,     nan,     nan,     nan, -0.2639,     nan,     nan,     nan,\n",
       "             nan,  0.2092,     nan,     nan,     nan,     nan,  0.1662,     nan,\n",
       "             nan,     nan,     nan, -0.0929,     nan,     nan,     nan,     nan,\n",
       "          0.0000,     nan,     nan,     nan,     nan,  0.0000,     nan,     nan,\n",
       "             nan,     nan,  0.0000,     nan,     nan,     nan,     nan,  0.0000,\n",
       "             nan,     nan,     nan,     nan,  0.0000,     nan,     nan,     nan,\n",
       "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "             nan,     nan],\n",
       "        [-0.0032,     nan,     nan,     nan,     nan, -0.0382,     nan,     nan,\n",
       "             nan,     nan,  0.0147,     nan,     nan,     nan,     nan, -0.1082,\n",
       "             nan,     nan,     nan,     nan, -0.0885,     nan,     nan,     nan,\n",
       "             nan,  0.0766,     nan,     nan,     nan,     nan, -0.0227,     nan,\n",
       "             nan,     nan,     nan,  0.0044,     nan,     nan,     nan,     nan,\n",
       "          0.0137,     nan,     nan,     nan,     nan, -0.0389,     nan,     nan,\n",
       "             nan,     nan, -0.0287,     nan,     nan,     nan,     nan, -0.0402,\n",
       "             nan,     nan,     nan,     nan, -0.0909,     nan,     nan,     nan,\n",
       "             nan, -0.0006,     nan,     nan,     nan,     nan,  0.0837,     nan,\n",
       "             nan,     nan,     nan,  0.0829,     nan,     nan,     nan,     nan,\n",
       "          0.1256,     nan,     nan,     nan,     nan, -0.0060,     nan,     nan,\n",
       "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "             nan,     nan],\n",
       "        [-0.0504,     nan,     nan,     nan,     nan, -0.1756,     nan,     nan,\n",
       "             nan,     nan,  0.0342,     nan,     nan,     nan,     nan,  0.0287,\n",
       "             nan,     nan,     nan,     nan, -0.0841,     nan,     nan,     nan,\n",
       "             nan,  0.0091,     nan,     nan,     nan,     nan, -0.0243,     nan,\n",
       "             nan,     nan,     nan,  0.0911,     nan,     nan,     nan,     nan,\n",
       "          0.0862,     nan,     nan,     nan,     nan,  0.0081,     nan,     nan,\n",
       "             nan,     nan,  0.0085,     nan,     nan,     nan,     nan,  0.0133,\n",
       "             nan,     nan,     nan,     nan, -0.2345,     nan,     nan,     nan,\n",
       "             nan, -0.0864,     nan,     nan,     nan,     nan,  0.1998,     nan,\n",
       "             nan,     nan,     nan,  0.0521,     nan,     nan,     nan,     nan,\n",
       "          0.1224,     nan,     nan,     nan,     nan,  0.1902,     nan,     nan,\n",
       "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "             nan,     nan],\n",
       "        [-0.0292,     nan,     nan,     nan,     nan, -0.0678,     nan,     nan,\n",
       "             nan,     nan, -0.0449,     nan,     nan,     nan,     nan, -0.0561,\n",
       "             nan,     nan,     nan,     nan, -0.0371,     nan,     nan,     nan,\n",
       "             nan,  0.0461,     nan,     nan,     nan,     nan,  0.0104,     nan,\n",
       "             nan,     nan,     nan,  0.2925,     nan,     nan,     nan,     nan,\n",
       "          0.3007,     nan,     nan,     nan,     nan, -0.1121,     nan,     nan,\n",
       "             nan,     nan, -0.1105,     nan,     nan,     nan,     nan,  0.0105,\n",
       "             nan,     nan,     nan,     nan, -0.0485,     nan,     nan,     nan,\n",
       "             nan, -0.0578,     nan,     nan,     nan,     nan, -0.0154,     nan,\n",
       "             nan,     nan,     nan,  0.0067,     nan,     nan,     nan,     nan,\n",
       "          0.0051,     nan,     nan,     nan,     nan, -0.0486,     nan,     nan,\n",
       "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "             nan,     nan],\n",
       "        [ 0.0012,     nan,     nan,     nan,     nan, -0.0176,     nan,     nan,\n",
       "             nan,     nan, -0.0108,     nan,     nan,     nan,     nan, -0.0198,\n",
       "             nan,     nan,     nan,     nan,  0.0348,     nan,     nan,     nan,\n",
       "             nan,  0.0655,     nan,     nan,     nan,     nan, -0.0251,     nan,\n",
       "             nan,     nan,     nan, -0.0157,     nan,     nan,     nan,     nan,\n",
       "         -0.0157,     nan,     nan,     nan,     nan, -0.0902,     nan,     nan,\n",
       "             nan,     nan, -0.0267,     nan,     nan,     nan,     nan,  0.0099,\n",
       "             nan,     nan,     nan,     nan, -0.0566,     nan,     nan,     nan,\n",
       "             nan, -0.0006,     nan,     nan,     nan,     nan,  0.0283,     nan,\n",
       "             nan,     nan,     nan, -0.0093,     nan,     nan,     nan,     nan,\n",
       "         -0.0190,     nan,     nan,     nan,     nan, -0.0397,     nan,     nan,\n",
       "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "             nan,     nan],\n",
       "        [-0.0614,     nan,     nan,     nan,     nan, -0.1980,     nan,     nan,\n",
       "             nan,     nan, -0.1079,     nan,     nan,     nan,     nan, -0.2115,\n",
       "             nan,     nan,     nan,     nan, -0.1578,     nan,     nan,     nan,\n",
       "             nan,  0.1450,     nan,     nan,     nan,     nan,  0.0958,     nan,\n",
       "             nan,     nan,     nan,  0.0823,     nan,     nan,     nan,     nan,\n",
       "          0.0438,     nan,     nan,     nan,     nan, -0.0642,     nan,     nan,\n",
       "             nan,     nan, -0.0367,     nan,     nan,     nan,     nan, -0.0371,\n",
       "             nan,     nan,     nan,     nan, -0.2029,     nan,     nan,     nan,\n",
       "             nan, -0.1095,     nan,     nan,     nan,     nan,  0.0668,     nan,\n",
       "             nan,     nan,     nan,  0.0500,     nan,     nan,     nan,     nan,\n",
       "          0.0023,     nan,     nan,     nan,     nan, -0.0688,     nan,     nan,\n",
       "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "             nan,     nan],\n",
       "        [ 0.0142,     nan,     nan,     nan,     nan,  0.0162,     nan,     nan,\n",
       "             nan,     nan,  0.0366,     nan,     nan,     nan,     nan, -0.0985,\n",
       "             nan,     nan,     nan,     nan, -0.0799,     nan,     nan,     nan,\n",
       "             nan,  0.0419,     nan,     nan,     nan,     nan, -0.0281,     nan,\n",
       "             nan,     nan,     nan,  0.0159,     nan,     nan,     nan,     nan,\n",
       "          0.0191,     nan,     nan,     nan,     nan, -0.0020,     nan,     nan,\n",
       "             nan,     nan,  0.0199,     nan,     nan,     nan,     nan, -0.0149,\n",
       "             nan,     nan,     nan,     nan, -0.0506,     nan,     nan,     nan,\n",
       "             nan,  0.0194,     nan,     nan,     nan,     nan,  0.0354,     nan,\n",
       "             nan,     nan,     nan,  0.0298,     nan,     nan,     nan,     nan,\n",
       "          0.1095,     nan,     nan,     nan,     nan, -0.0113,     nan,     nan,\n",
       "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "             nan,     nan]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_block.values[:10,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from .stats_utils.ic_perf import calc_ic\n",
    "from .stats_utils.grouping import calc_group_perf\n",
    "\n",
    "\n",
    "def calc_decay_pnl(factor_val_df, ret_type, yend_ed, bm_index_nm, ret_range_type, price_type, ic_type, lag_num):\n",
    "    decay_pnl_df = []\n",
    "    for lag in range(lag_num + 1):\n",
    "        decay_pnl = calc_ic(factor_val_df, ret_type, yend_ed, bm_index_nm, ret_range_type, price_type, ic_type, lag)\n",
    "        decay_pnl = decay_pnl.stack().rename(\"ic\").reset_index(drop=False)\n",
    "        decay_pnl[\"lag_type\"] = \"lag{0}\".format(lag)\n",
    "        decay_pnl_df.append(decay_pnl)\n",
    "    decay_pnl_df = pd.concat(decay_pnl_df, axis=0)\n",
    "    #\n",
    "    ic_mean = decay_pnl_df.groupby([\"factor_name\", \"lag_type\"])[\"ic\"].mean().rename(\"ic_mean\").reset_index(drop=False)\n",
    "    return ic_mean\n",
    "\n",
    "\n",
    "def calc_decay_grp_perf(factor_val_df, ret_type, freq_type, yend_ed, grp_bm_index, group_nums, ret_range_type,\n",
    "                        price_type, lag_num):\n",
    "    decay_grp_perf = []\n",
    "    for lag in range(lag_num + 1):\n",
    "        grp_perf = calc_group_perf(factor_val_df, ret_type, yend_ed, grp_bm_index, group_nums, ret_range_type,\n",
    "                                   price_type, lag)\n",
    "        grp_perf[\"lag_type\"] = \"lag{0}\".format(lag)\n",
    "        decay_grp_perf.append(grp_perf)\n",
    "    decay_grp_perf = pd.concat(decay_grp_perf, axis=0)\n",
    "    #\n",
    "    freq_days_dict = {\"day\": 245, \"week\": 50, \"month\": 12}\n",
    "    assert freq_type in freq_days_dict.keys()\n",
    "    decay_grp_ret_mean = decay_grp_perf.groupby([\"factor_name\", \"group\", \"lag_type\"])[\"group_ret\"].mean() * freq_days_dict[freq_type]\n",
    "    deacy_grp_rert_std = decay_grp_perf.groupby([\"factor_name\", \"group\", \"lag_type\"])[\"group_ret\"].std() * \\\n",
    "                       np.sqrt(freq_days_dict[freq_type])\n",
    "    decay_grp_ret_ir = decay_grp_ret_mean / deacy_grp_rert_std\n",
    "    rtn = pd.concat(\n",
    "        (\n",
    "            decay_grp_ret_mean.rename(\"decay_grp_ret\"),\n",
    "            decay_grp_ret_ir.rename(\"decay_grp_ir\")\n",
    "         ),\n",
    "        axis=1, sort=True).rename_axis(\"stats_name\", axis=\"columns\").stack().rename(\"stats_value\").reset_index(drop=False)\n",
    "    return rtn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update Files\n",
      "Tue Jun  4 23:42:09 2024 : Updated ~ data\\DataBase\\DB_information\\calendar.feather Done! Cost 0.03 Secs\n",
      "Tue Jun  4 23:42:09 2024 : Updated ~ data\\DataBase\\DB_information\\description.feather Done! Cost 0.04 Secs\n",
      "Tue Jun  4 23:42:09 2024 : Updated ~ data\\DataBase\\DB_information\\st.feather Done! Cost 0.02 Secs\n",
      "Tue Jun  4 23:42:12 2024 : Updated ~ data\\DataBase\\DB_information\\industry.feather Done! Cost 2.40 Secs\n",
      "Tue Jun  4 23:42:16 2024 : Updated ~ data\\DataBase\\DB_information\\concepts.feather Done! Cost 4.45 Secs\n",
      "Tue Jun  4 23:42:17 2024 : Updated ~ data\\DataBase\\DB_models\\longcl_exp\\2024\\longcl_exp.20240603.feather Done! Cost 0.09 Secs\n",
      "Tue Jun  4 23:42:17 2024 : Updated ~ data\\DataBase\\DB_models\\risk_exp\\2024\\risk_exp.20240604.feather Done! Cost 0.05 Secs\n",
      "Tue Jun  4 23:42:19 2024 : Updated ~ data\\DataBase\\DB_trade\\day\\2024\\day.20240604.feather Done! Cost 0.05 Secs\n",
      "Tue Jun  4 23:42:19 2024 : Updated ~ data\\DataBase\\DB_trade\\5day\\2024\\5day.20240604.feather Done! Cost 0.24 Secs\n",
      "Tue Jun  4 23:42:19 2024 : Updated ~ data\\DataBase\\DB_trade\\10day\\2024\\10day.20240604.feather Done! Cost 0.43 Secs\n",
      "Tue Jun  4 23:42:20 2024 : Updated ~ data\\DataBase\\DB_trade\\20day\\2024\\20day.20240604.feather Done! Cost 0.82 Secs\n",
      "Tue Jun  4 23:42:23 2024 : Updated ~ data\\DataBase\\DB_trade\\min\\2024\\min.20240604.feather Done! Cost 3.21 Secs\n",
      "Tue Jun  4 23:42:24 2024 : Updated ~ data\\DataBase\\DB_trade\\5min\\2024\\5min.20240604.feather Done! Cost 0.34 Secs\n",
      "Tue Jun  4 23:42:24 2024 : Updated ~ data\\DataBase\\DB_trade\\10min\\2024\\10min.20240604.feather Done! Cost 0.30 Secs\n",
      "Tue Jun  4 23:42:24 2024 : Updated ~ data\\DataBase\\DB_trade\\15min\\2024\\15min.20240604.feather Done! Cost 0.27 Secs\n",
      "Tue Jun  4 23:42:24 2024 : Updated ~ data\\DataBase\\DB_trade\\30min\\2024\\30min.20240604.feather Done! Cost 0.26 Secs\n",
      "Tue Jun  4 23:42:25 2024 : Updated ~ data\\DataBase\\DB_trade\\60min\\2024\\60min.20240604.feather Done! Cost 0.23 Secs\n",
      "Tue Jun  4 23:42:27 2024 : Updated ~ data\\DataBase\\DB_labels\\ret20_lag\\2024\\ret20_lag.20240506.feather Done! Cost 0.55 Secs\n",
      "Tue Jun  4 23:42:27 2024 : Updated ~ data\\DataBase\\DB_labels\\ret20\\2024\\ret20.20240507.feather Done! Cost 0.29 Secs\n",
      "Tue Jun  4 23:42:29 2024 : Updated ~ data\\DataBase\\DB_labels\\ret10_lag\\2024\\ret10_lag.20240520.feather Done! Cost 0.20 Secs\n",
      "Tue Jun  4 23:42:30 2024 : Updated ~ data\\DataBase\\DB_labels\\ret10\\2024\\ret10.20240521.feather Done! Cost 0.20 Secs\n",
      "Tue Jun  4 23:42:32 2024 : Updated ~ data\\DataBase\\DB_labels\\ret5_lag\\2024\\ret5_lag.20240527.feather Done! Cost 0.19 Secs\n",
      "Tue Jun  4 23:42:33 2024 : Updated ~ data\\DataBase\\DB_labels\\ret5\\2024\\ret5.20240528.feather Done! Cost 0.24 Secs\n",
      "Tue Jun  4 23:42:36 2024 : download since!\n",
      "Connection and Factor preparation finished!\n",
      "Tue Jun  4 23:42:36 2024 : sellside/haitong.hf_factors from 20240604 to 20240604, total 1 periods\n",
      "Start sellside/haitong.hf_factors:20240604-20240604 \n",
      "Done sellside/haitong.hf_factors:20240604-20240604, cost 5.8 Secs\n",
      "Tue Jun  4 23:42:42 2024 : sellside/haitong.dl_factors from 20240604 to 20240604, total 1 periods\n",
      "Start sellside/haitong.dl_factors:20240604-20240604 \n",
      "Done sellside/haitong.dl_factors:20240604-20240604, cost 93.0 Secs\n",
      "Tue Jun  4 23:44:15 2024 : sellside/dongfang.hfq_chars from 20240604 to 20240604, total 1 periods\n",
      "Start sellside/dongfang.hfq_chars:20240604-20240604 \n",
      "Done sellside/dongfang.hfq_chars:20240604-20240604, cost 2.8 Secs\n",
      "Tue Jun  4 23:44:18 2024 : sellside/dongfang.l2_chars from 20240604 to 20240604, total 1 periods\n",
      "Start sellside/dongfang.l2_chars:20240604-20240604 \n",
      "Done sellside/dongfang.l2_chars:20240604-20240604, cost 76.9 Secs\n",
      "Tue Jun  4 23:45:35 2024 : sellside/dongfang.ms_chars from 20240604 to 20240604, total 1 periods\n",
      "Start sellside/dongfang.ms_chars:20240604-20240604 \n",
      "Done sellside/dongfang.ms_chars:20240604-20240604, cost 44.8 Secs\n",
      "Tue Jun  4 23:46:20 2024 : sellside/dongfang.order_flow from 20240604 to 20240604, total 1 periods\n",
      "Start sellside/dongfang.order_flow:20240604-20240604 \n",
      "Done sellside/dongfang.order_flow:20240604-20240604, cost 0.8 Secs\n",
      "Tue Jun  4 23:46:21 2024 : sellside/dongfang.gp from 20240604 to 20240604, total 1 periods\n",
      "Start sellside/dongfang.gp:20240604-20240604 \n",
      "Done sellside/dongfang.gp:20240604-20240604, cost 2.8 Secs\n",
      "Tue Jun  4 23:46:23 2024 : sellside/dongfang.tra from 20240604 to 20240604, total 1 periods\n",
      "Start sellside/dongfang.tra:20240604-20240604 \n",
      "Done sellside/dongfang.tra:20240604-20240604, cost 1.2 Secs\n",
      "Tue Jun  4 23:46:25 2024 : sellside/dongfang.hist from 20240604 to 20240604, total 1 periods\n",
      "Start sellside/dongfang.hist:20240604-20240604 \n",
      "Done sellside/dongfang.hist:20240604-20240604, cost 1.1 Secs\n",
      "Tue Jun  4 23:46:26 2024 : sellside/dongfang.scores_v0 from 20240604 to 20240604, total 1 periods\n",
      "Start sellside/dongfang.scores_v0:20240604-20240604 \n",
      "Done sellside/dongfang.scores_v0:20240604-20240604, cost 1.5 Secs\n",
      "Tue Jun  4 23:46:27 2024 : sellside/dongfang.factorvae from 20240604 to 20240604, total 1 periods\n",
      "Start sellside/dongfang.factorvae:20240604-20240604 \n",
      "Done sellside/dongfang.factorvae:20240604-20240604, cost 1.8 Secs\n",
      "Tue Jun  4 23:46:29 2024 : sellside/kaiyuan.positive from 20240601 to 20240604, total 1 periods\n",
      "Start sellside/kaiyuan.positive:20240601-20240604 \n",
      "Done sellside/kaiyuan.positive:20240601-20240604, cost 3.4 Secs\n",
      "Tue Jun  4 23:46:32 2024 : sellside/kaiyuan.negative from 20240601 to 20240604, total 1 periods\n",
      "Start sellside/kaiyuan.negative:20240601-20240604 \n",
      "Done sellside/kaiyuan.negative:20240601-20240604, cost 1.9 Secs\n",
      "Tue Jun  4 23:46:34 2024 : sellside/huatai.dl_factors from 20240604 to 20240604, total 1 periods\n",
      "Start sellside/huatai.dl_factors:20240604-20240604 \n",
      "Done sellside/huatai.dl_factors:20240604-20240604, cost 2.3 Secs\n",
      "Tue Jun  4 23:46:37 2024 : All Updates Done! Cost 267.54 Secs\n",
      "predict is True , Data Processing start!\n",
      "4 datas :['y', 'day', '30m', 'risk']\n",
      "y blocks loading start!\n",
      " --> labels blocks reading [ret10_lag] DataBase...... finished! Cost 0.89 secs\n",
      " --> labels blocks reading [ret20_lag] DataBase...... finished! Cost 0.86 secs\n",
      " --> labels blocks merging (2)...... finished! Cost 0.09 secs\n",
      " --> models blocks reading [risk_exp] DataBase...... finished! Cost 3.10 secs\n",
      "y blocks loading finished! Cost 5.22 secs\n",
      "y blocks process...... finished! Cost 4.11 secs\n",
      "y blocks masking...... finished! Cost 0.08 secs\n",
      "y blocks saving ...... finished! Cost 0.13 secs\n",
      "y blocks norming...... finished! Cost 0.00 secs\n",
      "y finished! Cost 9.84 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "day blocks loading start!\n",
      " --> trade blocks reading [day] DataBase...... finished! Cost 1.32 secs\n",
      "day blocks loading finished! Cost 1.35 secs\n",
      "day blocks process...... finished! Cost 0.12 secs\n",
      "day blocks masking...... finished! Cost 0.07 secs\n",
      "day blocks saving ...... finished! Cost 0.12 secs\n",
      "day blocks norming...... finished! Cost 0.00 secs\n",
      "day finished! Cost 1.79 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "30m blocks loading start!\n",
      " --> trade blocks reading [30min] DataBase...... finished! Cost 5.88 secs\n",
      " --> trade blocks reading [day] DataBase...... finished! Cost 1.00 secs\n",
      "30m blocks loading finished! Cost 6.92 secs\n",
      "30m blocks process...... finished! Cost 1.41 secs\n",
      "30m blocks masking...... finished! Cost 0.14 secs\n",
      "30m blocks saving ...... finished! Cost 1.70 secs\n",
      "30m blocks norming...... finished! Cost 0.00 secs\n",
      "30m finished! Cost 10.26 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "risk blocks loading start!\n",
      " --> models blocks reading [risk_exp] DataBase...... finished! Cost 1.31 secs\n",
      "risk blocks loading finished! Cost 1.35 secs\n",
      "risk blocks process...... finished! Cost 0.00 secs\n",
      "risk blocks masking...... finished! Cost 0.08 secs\n",
      "risk blocks saving ...... finished! Cost 0.19 secs\n",
      "risk blocks norming...... finished! Cost 0.00 secs\n",
      "risk finished! Cost 1.70 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "Data Processing Finished! Cost 23.58 Seconds\n",
      "Finish model [gru_day] predicting!\n",
      "Finish model [gruRTN_day] predicting!\n",
      "Finish model [gruRES_day] predicting!\n"
     ]
    }
   ],
   "source": [
    "from src import API\n",
    "\n",
    "API.DataAPI.update()\n",
    "API.DataAPI.prepare_train_data()\n",
    "API.Predictor.update_factors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  2 DataBlocks...... finished! Cost 0.11 secs\n",
      "Align 2 DataBlocks...... finished! Cost 0.17 secs\n",
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n",
      "20231201 new\n",
      "Load  2 DataBlocks...... finished! Cost 0.03 secs\n",
      "Align 2 DataBlocks...... finished! Cost 0.15 secs\n",
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n",
      "20231201 new\n",
      "Load  2 DataBlocks...... finished! Cost 0.03 secs\n",
      "Align 2 DataBlocks...... finished! Cost 0.17 secs\n",
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n",
      "20231201 new\n"
     ]
    }
   ],
   "source": [
    "from src import API\n",
    "API.Predictor.update_factors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  2 DataBlocks ..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... cost 0.22 secs\n",
      "Align 2 DataBlocks ...... cost 0.21 secs\n",
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n",
      "x shape is torch.Size([5064, 30, 6])\n",
      "y shape is torch.Size([5064, 1])\n",
      "Test Forward Success\n",
      "metrics :  Metrics.MetricOutput(loss=tensor(1.0688, grad_fn=<AddBackward0>), score=-0.015420470386743546, loss_item=1.068833589553833, penalty=0.0, losses=tensor(1.0688, grad_fn=<ExpBackward0>))\n",
      "Test Metrics Success\n"
     ]
    }
   ],
   "source": [
    "from src import API\n",
    "API.ModelTestor.new('ts_mixer').try_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
