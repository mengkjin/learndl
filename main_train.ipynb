{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 2.1.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available() , torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device name: NVIDIA GeForce RTX 4090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[44m24-07-03 23:28:59|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-07-03 23:28:59|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Wed Jul  3 23:28:59 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Process Queue : Data + Fit + Test\n",
      "--Model_name is set to gru_avg!\n",
      "Callback : ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True) , reset optimizer on some epoch (can speedup scheduler)\n",
      "Callback : CallbackTimer(verbosity=2) , record time cost of callback hooks\n",
      "Callback : EarlyStoppage(patience=20) , stop fitting when validation score cease to improve\n",
      "Callback : ValidationConverge(patience=5,eps=1e-05) , stop fitting when valid_score converge\n",
      "Callback : EarlyExitRetrain(earliest=10,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100, 1]) , retrain with new lr if fitting stopped too early\n",
      "Callback : NanLossRetrain(max_attempt=4) , retrain if fitting encounters nan loss\n",
      "Callback : BatchDisplay(verbosity=2) , display batch progress bar\n",
      "Callback : StatusDisplay(verbosity=2) , display epoch and event information\n",
      "Callback : DetailedAlphaAnalysis(which_alpha=avg) , record and concat each model to Alpha model instance\n",
      "{'random_seed': None,\n",
      " 'model_name': 'gru_avg',\n",
      " 'model_module': 'gru',\n",
      " 'model_data_type': 'day',\n",
      " 'model_types': ['best', 'swalast', 'swabest'],\n",
      " 'labels': ['std_lag1_10'],\n",
      " 'beg_date': 20170103,\n",
      " 'end_date': 99991231,\n",
      " 'sample_method': 'sequential',\n",
      " 'shuffle_option': 'epoch',\n",
      " 'lgbm_ensembler': False}\n",
      "{'hidden_dim': [32, 32, 32, 32, 32],\n",
      " 'seqlens': [{'day': 30, '30m': 30}],\n",
      " 'dropout': [0.1],\n",
      " 'enc_in': [True],\n",
      " 'enc_att': [False],\n",
      " 'rnn_type': ['gru'],\n",
      " 'rnn_att': [False],\n",
      " 'rnn_layers': [2],\n",
      " 'dec_mlp_layers': [2],\n",
      " 'num_output': [1],\n",
      " 'which_output': [0],\n",
      " 'kernel_size': [3],\n",
      " 'hidden_as_factor': [True],\n",
      " 'ordered_param_group': [False]}\n",
      "try using /home/mengkjin/Workspace/learndl/data/DataSet/day.20240605.pt , success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-07-03 23:29:01|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 2.0 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-07-03 23:29:01|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Wed Jul  3 23:29:01 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/mengkjin/Workspace/learndl/src/nn_model/util/logger.py\", line 66, in format\n",
      "    return super(_LevelColorFormatter, self).format(record)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1053, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 737, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 524, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 418, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 758, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 426, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_9336/4184826398.py\", line 3, in <module>\n",
      "    app.go()\n",
      "  File \"/home/mengkjin/Workspace/learndl/src/nn_model/trainer/trainer.py\", line 56, in go\n",
      "    self.main_process()\n",
      "  File \"/home/mengkjin/Workspace/learndl/src/nn_model/classes/mod.py\", line 328, in main_process\n",
      "    getattr(self , f'stage_{self.stage}')()\n",
      "  File \"/home/mengkjin/Workspace/learndl/src/nn_model/classes/mod.py\", line 344, in stage_fit\n",
      "    self.logger.warn('First Iterance:' , self.status.model_date , self.status.model_num)\n",
      "Message: 'First Iterance:'\n",
      "Arguments: (20170103, 0)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/mengkjin/Workspace/learndl/src/nn_model/util/logger.py\", line 53, in format\n",
      "    return super(_LevelFormatter, self).format(record)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1053, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 737, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 524, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 418, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 758, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 426, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/mengkjin/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_9336/4184826398.py\", line 3, in <module>\n",
      "    app.go()\n",
      "  File \"/home/mengkjin/Workspace/learndl/src/nn_model/trainer/trainer.py\", line 56, in go\n",
      "    self.main_process()\n",
      "  File \"/home/mengkjin/Workspace/learndl/src/nn_model/classes/mod.py\", line 328, in main_process\n",
      "    getattr(self , f'stage_{self.stage}')()\n",
      "  File \"/home/mengkjin/Workspace/learndl/src/nn_model/classes/mod.py\", line 344, in stage_fit\n",
      "    self.logger.warn('First Iterance:' , self.status.model_date , self.status.model_num)\n",
      "Message: 'First Iterance:'\n",
      "Arguments: (20170103, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score function of [spearman] calculated and success!\n",
      "loss function of [pearson] calculated and success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mFirstBite Ep#  0 : loss  0.97885, train 0.02796, valid 0.05140, best 0.0514, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.89901, train 0.11966, valid 0.13956, best 0.1396, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.86792, train 0.14827, valid 0.16522, best 0.1652, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.85358, train 0.16197, valid 0.16680, best 0.1683, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.85236, train 0.16388, valid 0.16205, best 0.1726, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.84282, train 0.17297, valid 0.17001, best 0.1729, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 30 : loss  0.84098, train 0.17429, valid 0.17264, best 0.1729, lr1.6e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 35 : loss  0.84011, train 0.17540, valid 0.17477, best 0.1748, lr3.1e-04\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 39 , effective at epoch 40\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 40 : loss  0.83665, train 0.17805, valid 0.17023, best 0.1748, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 45 : loss  0.85370, train 0.16153, valid 0.15868, best 0.1748, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 50 : loss  0.83725, train 0.17896, valid 0.16318, best 0.1748, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 55 : loss  0.82953, train 0.18582, valid 0.16635, best 0.1748, lr1.0e-07\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-03 23:33:29|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #0 @20170103|FirstBite Ep# 56 EarlyStop|Train 0.1858 Valid 0.1663 BestVal 0.1663|Cost  4.4Min,  4.6Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  0.98563, train 0.01928, valid 0.04375, best 0.0438, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.90076, train 0.11377, valid 0.11805, best 0.1280, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.87346, train 0.14186, valid 0.13178, best 0.1458, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.85787, train 0.15807, valid 0.16092, best 0.1609, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.85685, train 0.15846, valid 0.15018, best 0.1609, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.84795, train 0.16794, valid 0.16090, best 0.1614, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 30 : loss  0.84591, train 0.16977, valid 0.15718, best 0.1614, lr1.6e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 35 : loss  0.84432, train 0.17186, valid 0.15792, best 0.1614, lr3.1e-04\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 39 , effective at epoch 40\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 40 : loss  0.84135, train 0.17466, valid 0.16204, best 0.1635, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 45 : loss  0.85566, train 0.16041, valid 0.16762, best 0.1676, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 50 : loss  0.84121, train 0.17539, valid 0.16146, best 0.1676, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 55 : loss  0.83423, train 0.18128, valid 0.16417, best 0.1695, lr1.0e-07\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 59 , effective at epoch 60\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 60 : loss  0.82947, train 0.18614, valid 0.15767, best 0.1695, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 65 : loss  0.84415, train 0.17198, valid 0.15305, best 0.1695, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 70 : loss  0.82933, train 0.18563, valid 0.15713, best 0.1695, lr1.9e-03\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-03 23:38:54|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #1 @20170103|FirstBite Ep# 72 EarlyStop|Train 0.1840 Valid 0.1586 BestVal 0.1586|Cost  5.4Min,  4.5Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  1.00489, train-0.00436, valid-0.01119, best-0.0112, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.89143, train 0.12389, valid 0.12877, best 0.1413, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.86838, train 0.14702, valid 0.13340, best 0.1433, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.85656, train 0.15906, valid 0.15447, best 0.1561, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.85844, train 0.15719, valid 0.15065, best 0.1587, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.84655, train 0.16948, valid 0.15960, best 0.1596, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 30 : loss  0.84468, train 0.17087, valid 0.16238, best 0.1624, lr1.6e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 35 : loss  0.84345, train 0.17202, valid 0.16570, best 0.1657, lr3.1e-04\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 39 , effective at epoch 40\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 40 : loss  0.83988, train 0.17663, valid 0.16341, best 0.1657, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 45 : loss  0.85715, train 0.15812, valid 0.12874, best 0.1657, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 50 : loss  0.83979, train 0.17531, valid 0.16411, best 0.1657, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 55 : loss  0.83120, train 0.18342, valid 0.15549, best 0.1657, lr1.0e-07\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-03 23:43:17|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #2 @20170103|FirstBite Ep# 56 EarlyStop|Train 0.1834 Valid 0.1555 BestVal 0.1555|Cost  4.4Min,  4.6Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  1.00186, train-0.00094, valid-0.00233, best-0.0023, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.89802, train 0.12154, valid 0.14788, best 0.1479, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.87282, train 0.14304, valid 0.13928, best 0.1513, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.85760, train 0.15706, valid 0.15782, best 0.1656, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.85679, train 0.15818, valid 0.16109, best 0.1656, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.84841, train 0.16631, valid 0.15861, best 0.1656, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 30 : loss  0.84799, train 0.16713, valid 0.15734, best 0.1656, lr1.6e-04\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-03 23:45:57|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #3 @20170103|FirstBite Ep# 34 EarlyStop|Train 0.1694 Valid 0.1637 BestVal 0.1637|Cost  2.7Min,  4.6Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  1.00173, train-0.00068, valid 0.00720, best 0.0072, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.89356, train 0.12244, valid 0.14759, best 0.1476, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.86952, train 0.14663, valid 0.14909, best 0.1553, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.85625, train 0.15977, valid 0.16601, best 0.1660, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.85386, train 0.16202, valid 0.15827, best 0.1660, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.84495, train 0.17099, valid 0.16512, best 0.1667, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 30 : loss  0.84378, train 0.17202, valid 0.17218, best 0.1722, lr1.6e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 35 : loss  0.84153, train 0.17486, valid 0.17086, best 0.1722, lr3.1e-04\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 39 , effective at epoch 40\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 40 : loss  0.83893, train 0.17876, valid 0.17131, best 0.1722, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 45 : loss  0.85361, train 0.16309, valid 0.11714, best 0.1722, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 50 : loss  0.83945, train 0.17755, valid 0.16455, best 0.1722, lr1.9e-03\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-03 23:49:15|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #4 @20170103|FirstBite Ep# 51 EarlyStop|Train 0.1776 Valid 0.1646 BestVal 0.1646|Cost  3.3Min,  3.8Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-03 23:56:05|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #0 @20170704|FirstBite Ep# 86 EarlyStop|Train 0.2002 Valid 0.1595 BestVal 0.1595|Cost  6.8Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-03 23:59:15|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #1 @20170704|FirstBite Ep# 40 EarlyStop|Train 0.1726 Valid 0.1531 BestVal 0.1531|Cost  3.2Min,  4.6Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 00:05:28|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #2 @20170704|FirstBite Ep# 80 EarlyStop|Train 0.1875 Valid 0.1487 BestVal 0.1487|Cost  6.2Min,  4.6Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 00:08:59|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #3 @20170704|FirstBite Ep# 50 EarlyStop|Train 0.1720 Valid 0.1609 BestVal 0.1609|Cost  3.5Min,  4.1Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 00:12:17|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #4 @20170704|FirstBite Ep# 42 EarlyStop|Train 0.1697 Valid 0.1346 BestVal 0.1346|Cost  3.3Min,  4.6Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 00:15:41|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #0 @20171226|FirstBite Ep# 42 EarlyStop|Train 0.1661 Valid 0.1479 BestVal 0.1479|Cost  3.3Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 00:19:26|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #1 @20171226|FirstBite Ep# 49 EarlyStop|Train 0.1763 Valid 0.1490 BestVal 0.1490|Cost  3.7Min,  4.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 00:24:42|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #2 @20171226|FirstBite Ep# 66 EarlyStop|Train 0.1713 Valid 0.1373 BestVal 0.1373|Cost  5.3Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 00:29:44|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #3 @20171226|FirstBite Ep# 64 EarlyStop|Train 0.1747 Valid 0.1645 BestVal 0.1645|Cost  5.0Min,  4.6Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 00:34:10|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #4 @20171226|FirstBite Ep# 56 EarlyStop|Train 0.1840 Valid 0.1517 BestVal 0.1517|Cost  4.4Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 00:39:50|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #0 @20180627|FirstBite Ep# 71 EarlyStop|Train 0.1881 Valid 0.1562 BestVal 0.1562|Cost  5.6Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 00:46:14|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #1 @20180627|FirstBite Ep# 80 EarlyStop|Train 0.1919 Valid 0.1523 BestVal 0.1523|Cost  6.4Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 00:51:38|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #2 @20180627|FirstBite Ep# 69 EarlyStop|Train 0.1888 Valid 0.1476 BestVal 0.1476|Cost  5.4Min,  4.6Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 00:58:02|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #3 @20180627|FirstBite Ep# 80 EarlyStop|Train 0.1931 Valid 0.1528 BestVal 0.1528|Cost  6.4Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 01:04:22|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #4 @20180627|FirstBite Ep# 83 EarlyStop|Train 0.1941 Valid 0.1495 BestVal 0.1495|Cost  6.3Min,  4.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 01:09:38|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #0 @20181220|FirstBite Ep# 68 EarlyStop|Train 0.1869 Valid 0.1462 BestVal 0.1462|Cost  5.2Min,  4.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 01:14:08|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #1 @20181220|FirstBite Ep# 57 EarlyStop|Train 0.1900 Valid 0.1498 BestVal 0.1498|Cost  4.5Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 01:18:02|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #2 @20181220|FirstBite Ep# 49 EarlyStop|Train 0.1803 Valid 0.1420 BestVal 0.1420|Cost  3.9Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 01:24:53|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #3 @20181220|FirstBite Ep# 86 EarlyStop|Train 0.2000 Valid 0.1482 BestVal 0.1482|Cost  6.8Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 01:30:19|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #4 @20181220|FirstBite Ep# 68 EarlyStop|Train 0.1871 Valid 0.1488 BestVal 0.1488|Cost  5.4Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 01:34:13|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #0 @20190624|FirstBite Ep# 48 EarlyStop|Train 0.1773 Valid 0.1433 BestVal 0.1433|Cost  3.8Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 01:37:56|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #1 @20190624|FirstBite Ep# 50 EarlyStop|Train 0.1827 Valid 0.1398 BestVal 0.1398|Cost  3.7Min,  4.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 01:41:24|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #2 @20190624|FirstBite Ep# 43 EarlyStop|Train 0.1640 Valid 0.1342 BestVal 0.1342|Cost  3.5Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 01:45:24|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #3 @20190624|FirstBite Ep# 51 EarlyStop|Train 0.1793 Valid 0.1388 BestVal 0.1388|Cost  4.0Min,  4.6Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 01:49:10|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #4 @20190624|FirstBite Ep# 48 EarlyStop|Train 0.1788 Valid 0.1327 BestVal 0.1327|Cost  3.8Min,  4.6Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 01:52:29|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #0 @20191217|FirstBite Ep# 44 EarlyStop|Train 0.1625 Valid 0.1178 BestVal 0.1178|Cost  3.3Min,  4.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 01:56:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #1 @20191217|FirstBite Ep# 51 EarlyStop|Train 0.1792 Valid 0.1286 BestVal 0.1286|Cost  4.0Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 02:01:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #2 @20191217|FirstBite Ep# 63 EarlyStop|Train 0.1780 Valid 0.1233 BestVal 0.1233|Cost  5.0Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 02:05:31|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #3 @20191217|FirstBite Ep# 50 EarlyStop|Train 0.1835 Valid 0.1282 BestVal 0.1282|Cost  4.0Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 02:10:03|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #4 @20191217|FirstBite Ep# 57 EarlyStop|Train 0.1817 Valid 0.1211 BestVal 0.1211|Cost  4.5Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 02:13:56|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #0 @20200617|FirstBite Ep# 48 EarlyStop|Train 0.1791 Valid 0.1091 BestVal 0.1091|Cost  3.8Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 02:16:41|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #1 @20200617|FirstBite Ep# 35 EarlyStop|Train 0.1763 Valid 0.1159 BestVal 0.1159|Cost  2.7Min,  4.6Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 02:19:52|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #2 @20200617|FirstBite Ep# 41 EarlyStop|Train 0.1822 Valid 0.1212 BestVal 0.1212|Cost  3.2Min,  4.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 02:24:45|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #3 @20200617|FirstBite Ep# 63 EarlyStop|Train 0.1791 Valid 0.1149 BestVal 0.1149|Cost  4.9Min,  4.6Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 02:27:55|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #4 @20200617|FirstBite Ep# 41 EarlyStop|Train 0.1806 Valid 0.1168 BestVal 0.1168|Cost  3.2Min,  4.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 02:32:28|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #0 @20201214|FirstBite Ep# 57 EarlyStop|Train 0.1852 Valid 0.0988 BestVal 0.0988|Cost  4.5Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 02:37:43|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #1 @20201214|FirstBite Ep# 66 EarlyStop|Train 0.1778 Valid 0.0888 BestVal 0.0888|Cost  5.2Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 02:42:57|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #2 @20201214|FirstBite Ep# 66 EarlyStop|Train 0.1755 Valid 0.0942 BestVal 0.0942|Cost  5.2Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 02:46:14|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #3 @20201214|FirstBite Ep# 41 EarlyStop|Train 0.1783 Valid 0.0998 BestVal 0.0998|Cost  3.3Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 02:50:03|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #4 @20201214|FirstBite Ep# 48 EarlyStop|Train 0.1759 Valid 0.1029 BestVal 0.1029|Cost  3.8Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 02:55:46|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #0 @20210615|FirstBite Ep# 71 EarlyStop|Train 0.1842 Valid 0.0936 BestVal 0.0936|Cost  5.7Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 02:59:03|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #1 @20210615|FirstBite Ep# 41 EarlyStop|Train 0.1744 Valid 0.0994 BestVal 0.0994|Cost  3.3Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 03:04:23|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #2 @20210615|FirstBite Ep# 67 EarlyStop|Train 0.1783 Valid 0.1026 BestVal 0.1026|Cost  5.3Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 03:10:52|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #3 @20210615|FirstBite Ep# 82 EarlyStop|Train 0.1901 Valid 0.1020 BestVal 0.1020|Cost  6.5Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 03:15:34|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #4 @20210615|FirstBite Ep# 65 EarlyStop|Train 0.1680 Valid 0.0994 BestVal 0.0994|Cost  4.7Min,  4.3Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 03:22:06|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #0 @20211209|FirstBite Ep# 80 EarlyStop|Train 0.1839 Valid 0.0909 BestVal 0.0909|Cost  6.5Min,  4.8Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 03:27:31|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #1 @20211209|FirstBite Ep# 68 EarlyStop|Train 0.1770 Valid 0.0899 BestVal 0.0899|Cost  5.4Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 03:31:12|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #2 @20211209|FirstBite Ep# 46 EarlyStop|Train 0.1584 Valid 0.0838 BestVal 0.0838|Cost  3.7Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 03:37:05|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #3 @20211209|FirstBite Ep# 74 EarlyStop|Train 0.1778 Valid 0.0887 BestVal 0.0887|Cost  5.9Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 03:44:28|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #4 @20211209|FirstBite Ep# 92 EarlyStop|Train 0.1896 Valid 0.0899 BestVal 0.0899|Cost  7.4Min,  4.8Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 03:48:25|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #0 @20220613|FirstBite Ep# 48 EarlyStop|Train 0.1639 Valid 0.0889 BestVal 0.0889|Cost  3.9Min,  4.8Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 03:51:46|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #1 @20220613|FirstBite Ep# 41 EarlyStop|Train 0.1663 Valid 0.0968 BestVal 0.0968|Cost  3.4Min,  4.8Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 03:55:17|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #2 @20220613|FirstBite Ep# 43 EarlyStop|Train 0.1556 Valid 0.0884 BestVal 0.0884|Cost  3.5Min,  4.8Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 04:08:00|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #3 @20220613|Retrain#4 Ep#155 EarlyStop|Train 0.0250 Valid 0.0292 BestVal 0.0944|Cost 12.7Min,  4.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 04:11:17|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #4 @20220613|FirstBite Ep# 40 EarlyStop|Train 0.1670 Valid 0.0945 BestVal 0.0945|Cost  3.3Min,  4.8Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 04:16:53|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #0 @20221206|FirstBite Ep# 72 EarlyStop|Train 0.1671 Valid 0.0840 BestVal 0.0840|Cost  5.5Min,  4.6Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 04:19:29|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #1 @20221206|FirstBite Ep# 34 EarlyStop|Train 0.1574 Valid 0.0891 BestVal 0.0891|Cost  2.6Min,  4.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 04:26:20|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #2 @20221206|FirstBite Ep# 84 EarlyStop|Train 0.1797 Valid 0.0890 BestVal 0.0890|Cost  6.8Min,  4.8Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 04:29:37|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #3 @20221206|FirstBite Ep# 40 EarlyStop|Train 0.1642 Valid 0.0924 BestVal 0.0924|Cost  3.3Min,  4.8Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 04:32:14|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #4 @20221206|FirstBite Ep# 32 EarlyStop|Train 0.1598 Valid 0.0914 BestVal 0.0914|Cost  2.6Min,  4.8Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 04:35:40|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #0 @20230606|FirstBite Ep# 41 EarlyStop|Train 0.1620 Valid 0.0938 BestVal 0.0938|Cost  3.4Min,  4.8Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 04:41:06|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #1 @20230606|FirstBite Ep# 66 EarlyStop|Train 0.1532 Valid 0.0717 BestVal 0.0717|Cost  5.4Min,  4.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 04:44:23|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #2 @20230606|FirstBite Ep# 40 EarlyStop|Train 0.1559 Valid 0.0944 BestVal 0.0944|Cost  3.3Min,  4.8Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 04:47:00|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #3 @20230606|FirstBite Ep# 32 EarlyStop|Train 0.1575 Valid 0.0979 BestVal 0.0979|Cost  2.6Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 04:50:28|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #4 @20230606|FirstBite Ep# 42 EarlyStop|Train 0.1545 Valid 0.0914 BestVal 0.0914|Cost  3.5Min,  4.8Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 05:00:45|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #0 @20231201|Retrain#4 Ep#125 EarlyStop|Train 0.0319 Valid 0.0369 BestVal 0.1088|Cost 10.2Min,  4.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 05:10:01|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #1 @20231201|Retrain#4 Ep#120 EarlyStop|Train 0.0274 Valid 0.0341 BestVal 0.1031|Cost  9.3Min,  4.6Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 05:12:37|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #2 @20231201|FirstBite Ep# 32 EarlyStop|Train 0.1530 Valid 0.0866 BestVal 0.0866|Cost  2.6Min,  4.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 05:15:47|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #3 @20231201|FirstBite Ep# 41 EarlyStop|Train 0.1529 Valid 0.0901 BestVal 0.0901|Cost  3.2Min,  4.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 05:24:58|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #4 @20231201|Retrain#4 Ep#120 EarlyStop|Train 0.0390 Valid 0.0486 BestVal 0.1048|Cost  9.2Min,  4.6Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 05:27:14|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #0 @20240604|FirstBite Ep# 33 EarlyStop|Train 0.1536 Valid 0.1080 BestVal 0.1080|Cost  2.2Min,  3.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 05:29:46|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #1 @20240604|FirstBite Ep# 32 EarlyStop|Train 0.1507 Valid 0.1003 BestVal 0.1003|Cost  2.5Min,  4.6Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 05:40:21|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #2 @20240604|Retrain#4 Ep#130 EarlyStop|Train 0.0520 Valid 0.0552 BestVal 0.1113|Cost 10.6Min,  4.8Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 05:43:03|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #3 @20240604|FirstBite Ep# 33 EarlyStop|Train 0.1496 Valid 0.0975 BestVal 0.0975|Cost  2.7Min,  4.8Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 05:53:38|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_avg #4 @20240604|Retrain#4 Ep#130 EarlyStop|Train 0.0476 Valid 0.0466 BestVal 0.1139|Cost 10.6Min,  4.8Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-07-04 05:53:38|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Fit], Cost 6.4 Hours, 4.8 Min/model, 4.8 Sec/Epoch\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-07-04 05:53:38|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Thu Jul  4 05:53:38 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 05:55:08|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mTesting Mean Score(spearman):\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">gru.0</th>\n",
       "      <th colspan=\"3\" halign=\"left\">gru.1</th>\n",
       "      <th colspan=\"3\" halign=\"left\">gru.2</th>\n",
       "      <th colspan=\"3\" halign=\"left\">gru.3</th>\n",
       "      <th colspan=\"3\" halign=\"left\">gru.4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>best</th>\n",
       "      <th>swalast</th>\n",
       "      <th>swabest</th>\n",
       "      <th>best</th>\n",
       "      <th>swalast</th>\n",
       "      <th>swabest</th>\n",
       "      <th>best</th>\n",
       "      <th>swalast</th>\n",
       "      <th>swabest</th>\n",
       "      <th>best</th>\n",
       "      <th>swalast</th>\n",
       "      <th>swabest</th>\n",
       "      <th>best</th>\n",
       "      <th>swalast</th>\n",
       "      <th>swabest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20170103</th>\n",
       "      <td>0.1529</td>\n",
       "      <td>0.1564</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.1619</td>\n",
       "      <td>0.1583</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.1519</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>0.1504</td>\n",
       "      <td>0.1476</td>\n",
       "      <td>0.1319</td>\n",
       "      <td>0.1426</td>\n",
       "      <td>0.1489</td>\n",
       "      <td>0.1518</td>\n",
       "      <td>0.1532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20170704</th>\n",
       "      <td>0.1572</td>\n",
       "      <td>0.1570</td>\n",
       "      <td>0.1592</td>\n",
       "      <td>0.1473</td>\n",
       "      <td>0.1491</td>\n",
       "      <td>0.1504</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.1568</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.1524</td>\n",
       "      <td>0.1513</td>\n",
       "      <td>0.1520</td>\n",
       "      <td>0.1519</td>\n",
       "      <td>0.1481</td>\n",
       "      <td>0.1527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20171226</th>\n",
       "      <td>0.1369</td>\n",
       "      <td>0.1455</td>\n",
       "      <td>0.1452</td>\n",
       "      <td>0.1448</td>\n",
       "      <td>0.1464</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>0.1327</td>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.1425</td>\n",
       "      <td>0.1300</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>0.1424</td>\n",
       "      <td>0.1473</td>\n",
       "      <td>0.1488</td>\n",
       "      <td>0.1477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180627</th>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.1351</td>\n",
       "      <td>0.1443</td>\n",
       "      <td>0.1371</td>\n",
       "      <td>0.1337</td>\n",
       "      <td>0.1398</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>0.1419</td>\n",
       "      <td>0.1387</td>\n",
       "      <td>0.1364</td>\n",
       "      <td>0.1401</td>\n",
       "      <td>0.1391</td>\n",
       "      <td>0.1345</td>\n",
       "      <td>0.1386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181220</th>\n",
       "      <td>0.1167</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>0.1175</td>\n",
       "      <td>0.1113</td>\n",
       "      <td>0.1145</td>\n",
       "      <td>0.1111</td>\n",
       "      <td>0.1069</td>\n",
       "      <td>0.1047</td>\n",
       "      <td>0.1092</td>\n",
       "      <td>0.1187</td>\n",
       "      <td>0.1141</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.1089</td>\n",
       "      <td>0.1119</td>\n",
       "      <td>0.1126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190624</th>\n",
       "      <td>0.1005</td>\n",
       "      <td>0.1013</td>\n",
       "      <td>0.1003</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>0.1007</td>\n",
       "      <td>0.1004</td>\n",
       "      <td>0.1042</td>\n",
       "      <td>0.1058</td>\n",
       "      <td>0.1052</td>\n",
       "      <td>0.1028</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.1011</td>\n",
       "      <td>0.1024</td>\n",
       "      <td>0.1002</td>\n",
       "      <td>0.1019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20191217</th>\n",
       "      <td>0.1042</td>\n",
       "      <td>0.1046</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.1029</td>\n",
       "      <td>0.1040</td>\n",
       "      <td>0.1014</td>\n",
       "      <td>0.1061</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.1047</td>\n",
       "      <td>0.1013</td>\n",
       "      <td>0.1029</td>\n",
       "      <td>0.1024</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.1011</td>\n",
       "      <td>0.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20200617</th>\n",
       "      <td>0.0980</td>\n",
       "      <td>0.0973</td>\n",
       "      <td>0.0991</td>\n",
       "      <td>0.0965</td>\n",
       "      <td>0.0960</td>\n",
       "      <td>0.1005</td>\n",
       "      <td>0.0996</td>\n",
       "      <td>0.0983</td>\n",
       "      <td>0.1007</td>\n",
       "      <td>0.1003</td>\n",
       "      <td>0.0973</td>\n",
       "      <td>0.1020</td>\n",
       "      <td>0.1037</td>\n",
       "      <td>0.0956</td>\n",
       "      <td>0.1019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201214</th>\n",
       "      <td>0.0941</td>\n",
       "      <td>0.0928</td>\n",
       "      <td>0.0932</td>\n",
       "      <td>0.0934</td>\n",
       "      <td>0.0945</td>\n",
       "      <td>0.0952</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.0947</td>\n",
       "      <td>0.0961</td>\n",
       "      <td>0.0952</td>\n",
       "      <td>0.0939</td>\n",
       "      <td>0.0949</td>\n",
       "      <td>0.0950</td>\n",
       "      <td>0.0927</td>\n",
       "      <td>0.0955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20210615</th>\n",
       "      <td>0.0776</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.0779</td>\n",
       "      <td>0.0747</td>\n",
       "      <td>0.0735</td>\n",
       "      <td>0.0749</td>\n",
       "      <td>0.0774</td>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0780</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>0.0770</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0846</td>\n",
       "      <td>0.0808</td>\n",
       "      <td>0.0788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211209</th>\n",
       "      <td>0.1068</td>\n",
       "      <td>0.1068</td>\n",
       "      <td>0.1081</td>\n",
       "      <td>0.1117</td>\n",
       "      <td>0.1058</td>\n",
       "      <td>0.1116</td>\n",
       "      <td>0.1046</td>\n",
       "      <td>0.1025</td>\n",
       "      <td>0.1037</td>\n",
       "      <td>0.1109</td>\n",
       "      <td>0.1024</td>\n",
       "      <td>0.1040</td>\n",
       "      <td>0.1128</td>\n",
       "      <td>0.1043</td>\n",
       "      <td>0.1084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20220613</th>\n",
       "      <td>0.1061</td>\n",
       "      <td>0.0998</td>\n",
       "      <td>0.1013</td>\n",
       "      <td>0.1084</td>\n",
       "      <td>0.1015</td>\n",
       "      <td>0.1044</td>\n",
       "      <td>0.1014</td>\n",
       "      <td>0.1009</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1041</td>\n",
       "      <td>0.1038</td>\n",
       "      <td>0.0999</td>\n",
       "      <td>0.1122</td>\n",
       "      <td>0.1027</td>\n",
       "      <td>0.1075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20221206</th>\n",
       "      <td>0.0870</td>\n",
       "      <td>0.0773</td>\n",
       "      <td>0.0792</td>\n",
       "      <td>0.0841</td>\n",
       "      <td>0.0724</td>\n",
       "      <td>0.0771</td>\n",
       "      <td>0.0820</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>0.0825</td>\n",
       "      <td>0.0820</td>\n",
       "      <td>0.0761</td>\n",
       "      <td>0.0808</td>\n",
       "      <td>0.0866</td>\n",
       "      <td>0.0779</td>\n",
       "      <td>0.0806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20230606</th>\n",
       "      <td>0.1208</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1169</td>\n",
       "      <td>0.1153</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.1114</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1212</td>\n",
       "      <td>0.1188</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.1197</td>\n",
       "      <td>0.1127</td>\n",
       "      <td>0.1076</td>\n",
       "      <td>0.1138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20231201</th>\n",
       "      <td>0.1166</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>0.1168</td>\n",
       "      <td>0.1155</td>\n",
       "      <td>0.1120</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.1173</td>\n",
       "      <td>0.1139</td>\n",
       "      <td>0.1206</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1137</td>\n",
       "      <td>0.1162</td>\n",
       "      <td>0.1036</td>\n",
       "      <td>0.1128</td>\n",
       "      <td>0.1115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20240604</th>\n",
       "      <td>0.0752</td>\n",
       "      <td>0.0558</td>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0888</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.0644</td>\n",
       "      <td>-0.0322</td>\n",
       "      <td>-0.0208</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0547</td>\n",
       "      <td>0.0661</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.0234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg</th>\n",
       "      <td>0.1145</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>0.1144</td>\n",
       "      <td>0.1137</td>\n",
       "      <td>0.1112</td>\n",
       "      <td>0.1127</td>\n",
       "      <td>0.1126</td>\n",
       "      <td>0.1109</td>\n",
       "      <td>0.1138</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>0.1106</td>\n",
       "      <td>0.1125</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>0.1108</td>\n",
       "      <td>0.1130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sum</th>\n",
       "      <td>207.4776</td>\n",
       "      <td>204.6613</td>\n",
       "      <td>207.3737</td>\n",
       "      <td>206.0608</td>\n",
       "      <td>201.4749</td>\n",
       "      <td>204.2738</td>\n",
       "      <td>204.0893</td>\n",
       "      <td>200.9066</td>\n",
       "      <td>206.2645</td>\n",
       "      <td>204.5169</td>\n",
       "      <td>200.3760</td>\n",
       "      <td>203.8209</td>\n",
       "      <td>205.2130</td>\n",
       "      <td>200.7310</td>\n",
       "      <td>204.8324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Std</th>\n",
       "      <td>0.0724</td>\n",
       "      <td>0.0701</td>\n",
       "      <td>0.0717</td>\n",
       "      <td>0.0710</td>\n",
       "      <td>0.0696</td>\n",
       "      <td>0.0705</td>\n",
       "      <td>0.0727</td>\n",
       "      <td>0.0711</td>\n",
       "      <td>0.0719</td>\n",
       "      <td>0.0710</td>\n",
       "      <td>0.0690</td>\n",
       "      <td>0.0706</td>\n",
       "      <td>0.0728</td>\n",
       "      <td>0.0702</td>\n",
       "      <td>0.0714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>67.3041</td>\n",
       "      <td>68.6079</td>\n",
       "      <td>67.9188</td>\n",
       "      <td>68.1832</td>\n",
       "      <td>68.0483</td>\n",
       "      <td>68.0498</td>\n",
       "      <td>65.9179</td>\n",
       "      <td>66.3388</td>\n",
       "      <td>67.3846</td>\n",
       "      <td>67.7030</td>\n",
       "      <td>68.1914</td>\n",
       "      <td>67.8367</td>\n",
       "      <td>66.2153</td>\n",
       "      <td>67.1566</td>\n",
       "      <td>67.4314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IR</th>\n",
       "      <td>7.7458</td>\n",
       "      <td>7.8959</td>\n",
       "      <td>7.8166</td>\n",
       "      <td>7.8470</td>\n",
       "      <td>7.8315</td>\n",
       "      <td>7.8317</td>\n",
       "      <td>7.5863</td>\n",
       "      <td>7.6347</td>\n",
       "      <td>7.7551</td>\n",
       "      <td>7.7917</td>\n",
       "      <td>7.8480</td>\n",
       "      <td>7.8071</td>\n",
       "      <td>7.6205</td>\n",
       "      <td>7.7289</td>\n",
       "      <td>7.7605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             gru.0                         gru.1                      \\\n",
       "              best   swalast   swabest      best   swalast   swabest   \n",
       "20170103    0.1529    0.1564    0.1582    0.1619    0.1583    0.1582   \n",
       "20170704    0.1572    0.1570    0.1592    0.1473    0.1491    0.1504   \n",
       "20171226    0.1369    0.1455    0.1452    0.1448    0.1464    0.1454   \n",
       "20180627    0.1460    0.1351    0.1443    0.1371    0.1337    0.1398   \n",
       "20181220    0.1167    0.1177    0.1175    0.1113    0.1145    0.1111   \n",
       "20190624    0.1005    0.1013    0.1003    0.1032    0.1007    0.1004   \n",
       "20191217    0.1042    0.1046    0.1057    0.1029    0.1040    0.1014   \n",
       "20200617    0.0980    0.0973    0.0991    0.0965    0.0960    0.1005   \n",
       "20201214    0.0941    0.0928    0.0932    0.0934    0.0945    0.0952   \n",
       "20210615    0.0776    0.0743    0.0779    0.0747    0.0735    0.0749   \n",
       "20211209    0.1068    0.1068    0.1081    0.1117    0.1058    0.1116   \n",
       "20220613    0.1061    0.0998    0.1013    0.1084    0.1015    0.1044   \n",
       "20221206    0.0870    0.0773    0.0792    0.0841    0.0724    0.0771   \n",
       "20230606    0.1208    0.1099    0.1169    0.1153    0.1100    0.1114   \n",
       "20231201    0.1166    0.1242    0.1168    0.1155    0.1120    0.1140   \n",
       "20240604    0.0752    0.0558    0.0522    0.0888    0.0649    0.0644   \n",
       "Avg         0.1145    0.1129    0.1144    0.1137    0.1112    0.1127   \n",
       "Sum       207.4776  204.6613  207.3737  206.0608  201.4749  204.2738   \n",
       "Std         0.0724    0.0701    0.0717    0.0710    0.0696    0.0705   \n",
       "T          67.3041   68.6079   67.9188   68.1832   68.0483   68.0498   \n",
       "IR          7.7458    7.8959    7.8166    7.8470    7.8315    7.8317   \n",
       "\n",
       "             gru.2                         gru.3                      \\\n",
       "              best   swalast   swabest      best   swalast   swabest   \n",
       "20170103    0.1519    0.1500    0.1504    0.1476    0.1319    0.1426   \n",
       "20170704    0.1566    0.1568    0.1579    0.1524    0.1513    0.1520   \n",
       "20171226    0.1327    0.1453    0.1425    0.1300    0.1441    0.1424   \n",
       "20180627    0.1460    0.1379    0.1419    0.1387    0.1364    0.1401   \n",
       "20181220    0.1069    0.1047    0.1092    0.1187    0.1141    0.1183   \n",
       "20190624    0.1042    0.1058    0.1052    0.1028    0.1010    0.1011   \n",
       "20191217    0.1061    0.1080    0.1047    0.1013    0.1029    0.1024   \n",
       "20200617    0.0996    0.0983    0.1007    0.1003    0.0973    0.1020   \n",
       "20201214    0.0963    0.0947    0.0961    0.0952    0.0939    0.0949   \n",
       "20210615    0.0774    0.0762    0.0780    0.0789    0.0770    0.0760   \n",
       "20211209    0.1046    0.1025    0.1037    0.1109    0.1024    0.1040   \n",
       "20220613    0.1014    0.1009    0.1035    0.1041    0.1038    0.0999   \n",
       "20221206    0.0820    0.0713    0.0825    0.0820    0.0761    0.0808   \n",
       "20230606    0.1210    0.1099    0.1212    0.1188    0.1170    0.1197   \n",
       "20231201    0.1173    0.1139    0.1206    0.1171    0.1137    0.1162   \n",
       "20240604   -0.0322   -0.0208    0.0074    0.0547    0.0661    0.0613   \n",
       "Avg         0.1126    0.1109    0.1138    0.1129    0.1106    0.1125   \n",
       "Sum       204.0893  200.9066  206.2645  204.5169  200.3760  203.8209   \n",
       "Std         0.0727    0.0711    0.0719    0.0710    0.0690    0.0706   \n",
       "T          65.9179   66.3388   67.3846   67.7030   68.1914   67.8367   \n",
       "IR          7.5863    7.6347    7.7551    7.7917    7.8480    7.8071   \n",
       "\n",
       "             gru.4                      \n",
       "              best   swalast   swabest  \n",
       "20170103    0.1489    0.1518    0.1532  \n",
       "20170704    0.1519    0.1481    0.1527  \n",
       "20171226    0.1473    0.1488    0.1477  \n",
       "20180627    0.1391    0.1345    0.1386  \n",
       "20181220    0.1089    0.1119    0.1126  \n",
       "20190624    0.1024    0.1002    0.1019  \n",
       "20191217    0.0995    0.1011    0.1000  \n",
       "20200617    0.1037    0.0956    0.1019  \n",
       "20201214    0.0950    0.0927    0.0955  \n",
       "20210615    0.0846    0.0808    0.0788  \n",
       "20211209    0.1128    0.1043    0.1084  \n",
       "20220613    0.1122    0.1027    0.1075  \n",
       "20221206    0.0866    0.0779    0.0806  \n",
       "20230606    0.1127    0.1076    0.1138  \n",
       "20231201    0.1036    0.1128    0.1115  \n",
       "20240604    0.0094    0.0212    0.0234  \n",
       "Avg         0.1133    0.1108    0.1130  \n",
       "Sum       205.2130  200.7310  204.8324  \n",
       "Std         0.0728    0.0702    0.0714  \n",
       "T          66.2153   67.1566   67.4314  \n",
       "IR          7.6205    7.7289    7.7605  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-07-04 05:55:08|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 90.8 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-04 05:55:16|MOD:test        |\u001b[0m: \u001b[1m\u001b[34mPerforming Factor and FMP test!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PerfManager calc Finished!\n",
      "PerfManager plot Finished!\n",
      "Group optimization of 3 alphas , 3 benchmarks , 2 lags , 363 dates , (6534 opts) start!\n",
      "Done Optimize    0th [best.csi300      ] at 20170104 , time cost (ms) : {'parse_input': 19.9, 'solve': 102.03, 'output': 18.13}\n",
      "Done Optimize   50th [swalast.csi1000  ] at 20170118 , time cost (ms) : {'parse_input': 19.94, 'solve': 109.46, 'output': 20.85}\n",
      "Done Optimize  100th [swabest.csi500.1 ] at 20170215 , time cost (ms) : {'parse_input': 19.28, 'solve': 107.61, 'output': 19.96}\n",
      "Done Optimize  150th [swabest.csi300   ] at 20170308 , time cost (ms) : {'parse_input': 18.68, 'solve': 130.65, 'output': 19.44}\n",
      "Done Optimize  200th [best.csi1000     ] at 20170329 , time cost (ms) : {'parse_input': 18.86, 'solve': 117.14, 'output': 19.91}\n",
      "Done Optimize  250th [swalast.csi500.1 ] at 20170414 , time cost (ms) : {'parse_input': 19.35, 'solve': 125.56, 'output': 19.97}\n",
      "Done Optimize  300th [swalast.csi300   ] at 20170508 , time cost (ms) : {'parse_input': 18.58, 'solve': 118.94, 'output': 19.41}\n",
      "Done Optimize  350th [swabest.csi1000  ] at 20170531 , time cost (ms) : {'parse_input': 18.71, 'solve': 120.63, 'output': 19.73}\n",
      "Done Optimize  400th [best.csi500.1    ] at 20170621 , time cost (ms) : {'parse_input': 18.65, 'solve': 112.73, 'output': 20.29}\n",
      "Done Optimize  450th [best.csi300      ] at 20170712 , time cost (ms) : {'parse_input': 19.8, 'solve': 118.47, 'output': 19.29}\n",
      "Done Optimize  500th [swalast.csi1000  ] at 20170726 , time cost (ms) : {'parse_input': 18.58, 'solve': 112.97, 'output': 19.71}\n",
      "Done Optimize  550th [swabest.csi500.1 ] at 20170816 , time cost (ms) : {'parse_input': 18.82, 'solve': 116.57, 'output': 19.35}\n",
      "Done Optimize  600th [swabest.csi300   ] at 20170906 , time cost (ms) : {'parse_input': 18.84, 'solve': 119.89, 'output': 19.22}\n",
      "Done Optimize  650th [best.csi1000     ] at 20170927 , time cost (ms) : {'parse_input': 19.43, 'solve': 114.74, 'output': 20.08}\n",
      "Done Optimize  700th [swalast.csi500.1 ] at 20171018 , time cost (ms) : {'parse_input': 19.27, 'solve': 112.11, 'output': 19.71}\n",
      "Done Optimize  750th [swalast.csi300   ] at 20171108 , time cost (ms) : {'parse_input': 19.05, 'solve': 128.83, 'output': 19.14}\n",
      "Done Optimize  800th [swabest.csi1000  ] at 20171129 , time cost (ms) : {'parse_input': 19.57, 'solve': 126.39, 'output': 20.2}\n",
      "Done Optimize  850th [best.csi500.1    ] at 20171220 , time cost (ms) : {'parse_input': 19.6, 'solve': 118.05, 'output': 19.83}\n",
      "Done Optimize  900th [best.csi300      ] at 20180111 , time cost (ms) : {'parse_input': 19.56, 'solve': 123.28, 'output': 19.74}\n",
      "Done Optimize  950th [swalast.csi1000  ] at 20180125 , time cost (ms) : {'parse_input': 19.36, 'solve': 115.51, 'output': 19.73}\n",
      "Done Optimize 1000th [swabest.csi500.1 ] at 20180222 , time cost (ms) : {'parse_input': 19.35, 'solve': 130.48, 'output': 20.08}\n",
      "Done Optimize 1050th [swabest.csi300   ] at 20180315 , time cost (ms) : {'parse_input': 19.38, 'solve': 125.7, 'output': 19.36}\n",
      "Done Optimize 1100th [best.csi1000     ] at 20180409 , time cost (ms) : {'parse_input': 19.68, 'solve': 127.44, 'output': 20.26}\n",
      "Done Optimize 1150th [swalast.csi500.1 ] at 20180423 , time cost (ms) : {'parse_input': 19.09, 'solve': 127.62, 'output': 19.41}\n",
      "Done Optimize 1200th [swalast.csi300   ] at 20180516 , time cost (ms) : {'parse_input': 22.0, 'solve': 124.75, 'output': 19.72}\n",
      "Done Optimize 1250th [swabest.csi1000  ] at 20180606 , time cost (ms) : {'parse_input': 19.38, 'solve': 128.7, 'output': 20.0}\n",
      "Done Optimize 1300th [best.csi500.1    ] at 20180628 , time cost (ms) : {'parse_input': 19.63, 'solve': 122.7, 'output': 19.75}\n",
      "Done Optimize 1350th [best.csi300      ] at 20180719 , time cost (ms) : {'parse_input': 20.42, 'solve': 132.82, 'output': 19.27}\n",
      "Done Optimize 1400th [swalast.csi1000  ] at 20180802 , time cost (ms) : {'parse_input': 19.66, 'solve': 130.83, 'output': 20.11}\n",
      "Done Optimize 1450th [swabest.csi500.1 ] at 20180823 , time cost (ms) : {'parse_input': 19.92, 'solve': 145.66, 'output': 20.17}\n",
      "Done Optimize 1500th [swabest.csi300   ] at 20180913 , time cost (ms) : {'parse_input': 19.69, 'solve': 149.61, 'output': 19.58}\n",
      "Done Optimize 1550th [best.csi1000     ] at 20181012 , time cost (ms) : {'parse_input': 20.13, 'solve': 136.26, 'output': 20.21}\n",
      "Done Optimize 1600th [swalast.csi500.1 ] at 20181026 , time cost (ms) : {'parse_input': 20.09, 'solve': 147.23, 'output': 19.48}\n",
      "Done Optimize 1650th [swalast.csi300   ] at 20181116 , time cost (ms) : {'parse_input': 20.26, 'solve': 137.29, 'output': 19.89}\n",
      "Done Optimize 1700th [swabest.csi1000  ] at 20181207 , time cost (ms) : {'parse_input': 20.45, 'solve': 137.98, 'output': 19.99}\n",
      "Done Optimize 1750th [best.csi500.1    ] at 20181228 , time cost (ms) : {'parse_input': 21.73, 'solve': 169.94, 'output': 19.71}\n",
      "Done Optimize 1800th [best.csi300      ] at 20190122 , time cost (ms) : {'parse_input': 20.78, 'solve': 148.54, 'output': 20.1}\n",
      "Done Optimize 1850th [swalast.csi1000  ] at 20190212 , time cost (ms) : {'parse_input': 20.52, 'solve': 134.7, 'output': 20.11}\n",
      "Done Optimize 1900th [swabest.csi500.1 ] at 20190305 , time cost (ms) : {'parse_input': 20.2, 'solve': 149.74, 'output': 19.98}\n",
      "Done Optimize 1950th [swabest.csi300   ] at 20190326 , time cost (ms) : {'parse_input': 20.5, 'solve': 133.94, 'output': 19.49}\n",
      "Done Optimize 2000th [best.csi1000     ] at 20190417 , time cost (ms) : {'parse_input': 20.38, 'solve': 151.4, 'output': 20.66}\n",
      "Done Optimize 2050th [swalast.csi500.1 ] at 20190506 , time cost (ms) : {'parse_input': 20.43, 'solve': 142.34, 'output': 19.99}\n",
      "Done Optimize 2100th [swalast.csi300   ] at 20190527 , time cost (ms) : {'parse_input': 20.21, 'solve': 146.61, 'output': 19.56}\n",
      "Done Optimize 2150th [swabest.csi1000  ] at 20190618 , time cost (ms) : {'parse_input': 21.29, 'solve': 145.59, 'output': 20.41}\n",
      "Done Optimize 2200th [best.csi500.1    ] at 20190709 , time cost (ms) : {'parse_input': 20.29, 'solve': 156.91, 'output': 19.91}\n",
      "Done Optimize 2250th [best.csi300      ] at 20190730 , time cost (ms) : {'parse_input': 21.3, 'solve': 143.79, 'output': 20.15}\n",
      "Done Optimize 2300th [swalast.csi1000  ] at 20190813 , time cost (ms) : {'parse_input': 20.79, 'solve': 156.23, 'output': 19.77}\n",
      "Done Optimize 2350th [swabest.csi500.1 ] at 20190903 , time cost (ms) : {'parse_input': 20.68, 'solve': 166.57, 'output': 19.9}\n",
      "Done Optimize 2400th [swabest.csi300   ] at 20190925 , time cost (ms) : {'parse_input': 20.48, 'solve': 156.42, 'output': 20.05}\n",
      "Done Optimize 2450th [best.csi1000     ] at 20191023 , time cost (ms) : {'parse_input': 20.48, 'solve': 155.1, 'output': 20.47}\n",
      "Done Optimize 2500th [swalast.csi500.1 ] at 20191106 , time cost (ms) : {'parse_input': 20.54, 'solve': 154.49, 'output': 19.95}\n",
      "Done Optimize 2550th [swalast.csi300   ] at 20191127 , time cost (ms) : {'parse_input': 20.59, 'solve': 148.13, 'output': 19.46}\n",
      "Done Optimize 2600th [swabest.csi1000  ] at 20191218 , time cost (ms) : {'parse_input': 20.57, 'solve': 148.34, 'output': 20.34}\n",
      "Done Optimize 2650th [best.csi500.1    ] at 20200109 , time cost (ms) : {'parse_input': 20.47, 'solve': 138.0, 'output': 19.95}\n",
      "Done Optimize 2700th [best.csi300      ] at 20200207 , time cost (ms) : {'parse_input': 21.11, 'solve': 141.48, 'output': 19.65}\n",
      "Done Optimize 2750th [swalast.csi1000  ] at 20200221 , time cost (ms) : {'parse_input': 20.26, 'solve': 131.44, 'output': 19.93}\n",
      "Done Optimize 2800th [swabest.csi500.1 ] at 20200313 , time cost (ms) : {'parse_input': 20.57, 'solve': 143.35, 'output': 19.81}\n",
      "Done Optimize 2850th [swabest.csi300   ] at 20200403 , time cost (ms) : {'parse_input': 20.71, 'solve': 178.09, 'output': 19.73}\n",
      "Done Optimize 2900th [best.csi1000     ] at 20200427 , time cost (ms) : {'parse_input': 20.91, 'solve': 142.31, 'output': 20.44}\n",
      "Done Optimize 2950th [swalast.csi500.1 ] at 20200514 , time cost (ms) : {'parse_input': 20.66, 'solve': 143.53, 'output': 19.76}\n",
      "Done Optimize 3000th [swalast.csi300   ] at 20200604 , time cost (ms) : {'parse_input': 20.52, 'solve': 169.01, 'output': 19.58}\n",
      "Done Optimize 3050th [swabest.csi1000  ] at 20200629 , time cost (ms) : {'parse_input': 20.76, 'solve': 142.26, 'output': 20.0}\n",
      "Done Optimize 3100th [best.csi500.1    ] at 20200720 , time cost (ms) : {'parse_input': 20.35, 'solve': 137.1, 'output': 20.15}\n",
      "Done Optimize 3150th [best.csi300      ] at 20200810 , time cost (ms) : {'parse_input': 21.32, 'solve': 141.66, 'output': 19.79}\n",
      "Done Optimize 3200th [swalast.csi1000  ] at 20200824 , time cost (ms) : {'parse_input': 23.16, 'solve': 139.92, 'output': 20.25}\n",
      "Done Optimize 3250th [swabest.csi500.1 ] at 20200914 , time cost (ms) : {'parse_input': 20.61, 'solve': 164.82, 'output': 19.86}\n",
      "Done Optimize 3300th [swabest.csi300   ] at 20201013 , time cost (ms) : {'parse_input': 21.12, 'solve': 141.44, 'output': 19.57}\n",
      "Done Optimize 3350th [best.csi1000     ] at 20201103 , time cost (ms) : {'parse_input': 20.78, 'solve': 139.3, 'output': 20.69}\n",
      "Done Optimize 3400th [swalast.csi500.1 ] at 20201117 , time cost (ms) : {'parse_input': 20.91, 'solve': 150.86, 'output': 19.91}\n",
      "Done Optimize 3450th [swalast.csi300   ] at 20201208 , time cost (ms) : {'parse_input': 21.17, 'solve': 151.45, 'output': 19.76}\n",
      "Done Optimize 3500th [swabest.csi1000  ] at 20201229 , time cost (ms) : {'parse_input': 20.66, 'solve': 146.27, 'output': 20.23}\n",
      "Done Optimize 3550th [best.csi500.1    ] at 20210120 , time cost (ms) : {'parse_input': 20.79, 'solve': 156.98, 'output': 20.3}\n",
      "Done Optimize 3600th [best.csi300      ] at 20210210 , time cost (ms) : {'parse_input': 21.27, 'solve': 143.32, 'output': 19.7}\n",
      "Done Optimize 3650th [swalast.csi1000  ] at 20210303 , time cost (ms) : {'parse_input': 20.66, 'solve': 152.2, 'output': 20.21}\n",
      "Done Optimize 3700th [swabest.csi500.1 ] at 20210324 , time cost (ms) : {'parse_input': 20.89, 'solve': 139.69, 'output': 20.11}\n",
      "Done Optimize 3750th [swabest.csi300   ] at 20210415 , time cost (ms) : {'parse_input': 20.85, 'solve': 155.46, 'output': 19.63}\n",
      "Done Optimize 3800th [best.csi1000     ] at 20210511 , time cost (ms) : {'parse_input': 20.8, 'solve': 139.08, 'output': 20.43}\n",
      "Done Optimize 3850th [swalast.csi500.1 ] at 20210525 , time cost (ms) : {'parse_input': 21.1, 'solve': 148.21, 'output': 19.94}\n",
      "Done Optimize 3900th [swalast.csi300   ] at 20210616 , time cost (ms) : {'parse_input': 20.88, 'solve': 166.85, 'output': 19.46}\n",
      "Done Optimize 3950th [swabest.csi1000  ] at 20210707 , time cost (ms) : {'parse_input': 21.29, 'solve': 157.4, 'output': 20.27}\n",
      "Done Optimize 4000th [best.csi500.1    ] at 20210728 , time cost (ms) : {'parse_input': 21.29, 'solve': 148.13, 'output': 20.13}\n",
      "Done Optimize 4050th [best.csi300      ] at 20210818 , time cost (ms) : {'parse_input': 21.62, 'solve': 151.47, 'output': 19.86}\n",
      "Done Optimize 4100th [swalast.csi1000  ] at 20210901 , time cost (ms) : {'parse_input': 21.24, 'solve': 151.84, 'output': 20.75}\n",
      "Done Optimize 4150th [swabest.csi500.1 ] at 20210924 , time cost (ms) : {'parse_input': 21.43, 'solve': 162.61, 'output': 21.7}\n",
      "Done Optimize 4200th [swabest.csi300   ] at 20211022 , time cost (ms) : {'parse_input': 21.54, 'solve': 148.9, 'output': 20.36}\n",
      "Done Optimize 4250th [best.csi1000     ] at 20211112 , time cost (ms) : {'parse_input': 21.66, 'solve': 179.18, 'output': 20.77}\n",
      "Done Optimize 4300th [swalast.csi500.1 ] at 20211126 , time cost (ms) : {'parse_input': 21.84, 'solve': 170.2, 'output': 20.26}\n",
      "Done Optimize 4350th [swalast.csi300   ] at 20211217 , time cost (ms) : {'parse_input': 21.76, 'solve': 164.47, 'output': 19.78}\n",
      "Done Optimize 4400th [swabest.csi1000  ] at 20220110 , time cost (ms) : {'parse_input': 21.98, 'solve': 179.94, 'output': 20.59}\n",
      "Done Optimize 4450th [best.csi500.1    ] at 20220207 , time cost (ms) : {'parse_input': 21.77, 'solve': 173.29, 'output': 20.18}\n",
      "Done Optimize 4500th [best.csi300      ] at 20220228 , time cost (ms) : {'parse_input': 22.66, 'solve': 184.9, 'output': 20.22}\n",
      "Done Optimize 4550th [swalast.csi1000  ] at 20220314 , time cost (ms) : {'parse_input': 22.25, 'solve': 182.63, 'output': 20.45}\n",
      "Done Optimize 4600th [swabest.csi500.1 ] at 20220406 , time cost (ms) : {'parse_input': 22.14, 'solve': 160.3, 'output': 20.36}\n",
      "Done Optimize 4650th [swabest.csi300   ] at 20220427 , time cost (ms) : {'parse_input': 22.22, 'solve': 176.32, 'output': 19.64}\n",
      "Done Optimize 4700th [best.csi1000     ] at 20220523 , time cost (ms) : {'parse_input': 21.86, 'solve': 159.71, 'output': 21.15}\n",
      "Done Optimize 4750th [swalast.csi500.1 ] at 20220607 , time cost (ms) : {'parse_input': 21.78, 'solve': 173.51, 'output': 20.24}\n",
      "Done Optimize 4800th [swalast.csi300   ] at 20220628 , time cost (ms) : {'parse_input': 22.29, 'solve': 173.3, 'output': 19.99}\n",
      "Done Optimize 4850th [swabest.csi1000  ] at 20220719 , time cost (ms) : {'parse_input': 22.27, 'solve': 176.68, 'output': 20.47}\n",
      "Done Optimize 4900th [best.csi500.1    ] at 20220809 , time cost (ms) : {'parse_input': 22.18, 'solve': 158.86, 'output': 21.89}\n",
      "Done Optimize 4950th [best.csi300      ] at 20220830 , time cost (ms) : {'parse_input': 22.74, 'solve': 192.4, 'output': 20.34}\n",
      "Done Optimize 5000th [swalast.csi1000  ] at 20220914 , time cost (ms) : {'parse_input': 22.22, 'solve': 179.83, 'output': 20.55}\n",
      "Done Optimize 5050th [swabest.csi500.1 ] at 20221012 , time cost (ms) : {'parse_input': 22.74, 'solve': 194.9, 'output': 21.86}\n",
      "Done Optimize 5100th [swabest.csi300   ] at 20221102 , time cost (ms) : {'parse_input': 22.41, 'solve': 173.51, 'output': 19.9}\n",
      "Done Optimize 5150th [best.csi1000     ] at 20221123 , time cost (ms) : {'parse_input': 22.56, 'solve': 185.51, 'output': 20.86}\n",
      "Done Optimize 5200th [swalast.csi500.1 ] at 20221207 , time cost (ms) : {'parse_input': 25.92, 'solve': 186.76, 'output': 20.46}\n",
      "Done Optimize 5250th [swalast.csi300   ] at 20221228 , time cost (ms) : {'parse_input': 22.79, 'solve': 184.68, 'output': 20.22}\n",
      "Done Optimize 5300th [swabest.csi1000  ] at 20230119 , time cost (ms) : {'parse_input': 22.88, 'solve': 202.3, 'output': 20.82}\n",
      "Done Optimize 5350th [best.csi500.1    ] at 20230216 , time cost (ms) : {'parse_input': 22.69, 'solve': 177.08, 'output': 20.7}\n",
      "Done Optimize 5400th [best.csi300      ] at 20230309 , time cost (ms) : {'parse_input': 23.17, 'solve': 183.19, 'output': 20.48}\n",
      "Done Optimize 5450th [swalast.csi1000  ] at 20230323 , time cost (ms) : {'parse_input': 22.93, 'solve': 182.05, 'output': 20.83}\n",
      "Done Optimize 5500th [swabest.csi500.1 ] at 20230414 , time cost (ms) : {'parse_input': 22.75, 'solve': 206.65, 'output': 20.46}\n",
      "Done Optimize 5550th [swabest.csi300   ] at 20230510 , time cost (ms) : {'parse_input': 23.4, 'solve': 171.6, 'output': 20.11}\n",
      "Done Optimize 5600th [best.csi1000     ] at 20230531 , time cost (ms) : {'parse_input': 23.19, 'solve': 171.75, 'output': 21.28}\n",
      "Done Optimize 5650th [swalast.csi500.1 ] at 20230614 , time cost (ms) : {'parse_input': 24.05, 'solve': 189.0, 'output': 21.04}\n",
      "Done Optimize 5700th [swalast.csi300   ] at 20230707 , time cost (ms) : {'parse_input': 24.63, 'solve': 185.87, 'output': 20.95}\n",
      "Done Optimize 5750th [swabest.csi1000  ] at 20230728 , time cost (ms) : {'parse_input': 22.93, 'solve': 212.59, 'output': 21.0}\n",
      "Done Optimize 5800th [best.csi500.1    ] at 20230818 , time cost (ms) : {'parse_input': 22.77, 'solve': 179.04, 'output': 20.52}\n",
      "Done Optimize 5850th [best.csi300      ] at 20230908 , time cost (ms) : {'parse_input': 23.52, 'solve': 195.46, 'output': 20.06}\n",
      "Done Optimize 5900th [swalast.csi1000  ] at 20230922 , time cost (ms) : {'parse_input': 23.23, 'solve': 177.97, 'output': 20.65}\n",
      "Done Optimize 5950th [swabest.csi500.1 ] at 20231023 , time cost (ms) : {'parse_input': 23.22, 'solve': 178.34, 'output': 20.51}\n",
      "Done Optimize 6000th [swabest.csi300   ] at 20231113 , time cost (ms) : {'parse_input': 22.64, 'solve': 200.47, 'output': 20.01}\n",
      "Done Optimize 6050th [best.csi1000     ] at 20231204 , time cost (ms) : {'parse_input': 22.69, 'solve': 198.61, 'output': 20.9}\n",
      "Done Optimize 6100th [swalast.csi500.1 ] at 20231218 , time cost (ms) : {'parse_input': 22.8, 'solve': 177.11, 'output': 20.27}\n",
      "Done Optimize 6150th [swalast.csi300   ] at 20240109 , time cost (ms) : {'parse_input': 22.87, 'solve': 212.01, 'output': 20.16}\n",
      "Done Optimize 6200th [swabest.csi1000  ] at 20240130 , time cost (ms) : {'parse_input': 23.24, 'solve': 195.29, 'output': 20.83}\n",
      "Done Optimize 6250th [best.csi500.1    ] at 20240228 , time cost (ms) : {'parse_input': 23.51, 'solve': 186.27, 'output': 20.81}\n",
      "Done Optimize 6300th [best.csi300      ] at 20240320 , time cost (ms) : {'parse_input': 24.93, 'solve': 201.18, 'output': 21.19}\n",
      "Done Optimize 6350th [swalast.csi1000  ] at 20240403 , time cost (ms) : {'parse_input': 23.54, 'solve': 193.74, 'output': 20.72}\n",
      "Done Optimize 6400th [swabest.csi500.1 ] at 20240426 , time cost (ms) : {'parse_input': 22.99, 'solve': 175.93, 'output': 20.33}\n",
      "Done Optimize 6450th [swabest.csi300   ] at 20240522 , time cost (ms) : {'parse_input': 23.33, 'solve': 191.25, 'output': 20.24}\n",
      "Done Optimize 6500th [best.csi1000     ] at 20240613 , time cost (ms) : {'parse_input': 23.06, 'solve': 210.38, 'output': 21.45}\n",
      "Group optimization Finished , Total time: 1433.73 secs, Setup time: 106.13 secs, Calc time: 1327.60 secs, Each optim time: 0.20\n",
      "FmpManager calc Finished!\n",
      "FmpManager plot Finished!\n",
      "FMP test Result:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>factor_name</th>\n",
       "      <th>benchmark</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>pf</th>\n",
       "      <th>bm</th>\n",
       "      <th>excess</th>\n",
       "      <th>annualized</th>\n",
       "      <th>mdd</th>\n",
       "      <th>te</th>\n",
       "      <th>ir</th>\n",
       "      <th>calmar</th>\n",
       "      <th>turnover</th>\n",
       "      <th>mdd_period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>best</td>\n",
       "      <td>csi300</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240628</td>\n",
       "      <td>175.90%</td>\n",
       "      <td>21.07%</td>\n",
       "      <td>154.83%</td>\n",
       "      <td>11.45%</td>\n",
       "      <td>4.65%</td>\n",
       "      <td>4.45%</td>\n",
       "      <td>2.571</td>\n",
       "      <td>2.465</td>\n",
       "      <td>146.006</td>\n",
       "      <td>20210623-20210901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>best</td>\n",
       "      <td>csi500</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240628</td>\n",
       "      <td>181.43%</td>\n",
       "      <td>-14.55%</td>\n",
       "      <td>195.98%</td>\n",
       "      <td>17.17%</td>\n",
       "      <td>8.26%</td>\n",
       "      <td>6.60%</td>\n",
       "      <td>2.603</td>\n",
       "      <td>2.079</td>\n",
       "      <td>146.148</td>\n",
       "      <td>20231225-20240206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best</td>\n",
       "      <td>csi1000</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240628</td>\n",
       "      <td>170.24%</td>\n",
       "      <td>-39.46%</td>\n",
       "      <td>209.70%</td>\n",
       "      <td>21.81%</td>\n",
       "      <td>14.22%</td>\n",
       "      <td>6.83%</td>\n",
       "      <td>3.195</td>\n",
       "      <td>1.534</td>\n",
       "      <td>146.256</td>\n",
       "      <td>20210429-20210908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>swabest</td>\n",
       "      <td>csi300</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240628</td>\n",
       "      <td>180.11%</td>\n",
       "      <td>21.07%</td>\n",
       "      <td>159.04%</td>\n",
       "      <td>11.71%</td>\n",
       "      <td>4.56%</td>\n",
       "      <td>4.41%</td>\n",
       "      <td>2.658</td>\n",
       "      <td>2.568</td>\n",
       "      <td>146.020</td>\n",
       "      <td>20210623-20210908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>swabest</td>\n",
       "      <td>csi500</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240628</td>\n",
       "      <td>201.82%</td>\n",
       "      <td>-14.55%</td>\n",
       "      <td>216.37%</td>\n",
       "      <td>18.26%</td>\n",
       "      <td>9.44%</td>\n",
       "      <td>6.93%</td>\n",
       "      <td>2.636</td>\n",
       "      <td>1.933</td>\n",
       "      <td>146.143</td>\n",
       "      <td>20240109-20240206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>swabest</td>\n",
       "      <td>csi1000</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240628</td>\n",
       "      <td>200.71%</td>\n",
       "      <td>-39.46%</td>\n",
       "      <td>240.17%</td>\n",
       "      <td>23.61%</td>\n",
       "      <td>13.40%</td>\n",
       "      <td>7.07%</td>\n",
       "      <td>3.339</td>\n",
       "      <td>1.763</td>\n",
       "      <td>146.239</td>\n",
       "      <td>20210429-20210908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>swalast</td>\n",
       "      <td>csi300</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240628</td>\n",
       "      <td>182.02%</td>\n",
       "      <td>21.07%</td>\n",
       "      <td>160.95%</td>\n",
       "      <td>11.82%</td>\n",
       "      <td>5.10%</td>\n",
       "      <td>4.38%</td>\n",
       "      <td>2.698</td>\n",
       "      <td>2.318</td>\n",
       "      <td>146.028</td>\n",
       "      <td>20231225-20240628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>swalast</td>\n",
       "      <td>csi500</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240628</td>\n",
       "      <td>197.56%</td>\n",
       "      <td>-14.55%</td>\n",
       "      <td>212.11%</td>\n",
       "      <td>18.09%</td>\n",
       "      <td>10.43%</td>\n",
       "      <td>6.85%</td>\n",
       "      <td>2.643</td>\n",
       "      <td>1.735</td>\n",
       "      <td>146.158</td>\n",
       "      <td>20210601-20210908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>swalast</td>\n",
       "      <td>csi1000</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240628</td>\n",
       "      <td>177.27%</td>\n",
       "      <td>-39.46%</td>\n",
       "      <td>216.73%</td>\n",
       "      <td>22.29%</td>\n",
       "      <td>16.09%</td>\n",
       "      <td>6.95%</td>\n",
       "      <td>3.207</td>\n",
       "      <td>1.385</td>\n",
       "      <td>146.234</td>\n",
       "      <td>20210429-20210901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  factor_name benchmark    start      end      pf       bm     excess  annualized   mdd     te     ir   calmar turnover     mdd_period    \n",
       "1       best     csi300  20170104  20240628  175.90%   21.07%  154.83%   11.45%     4.65%  4.45%  2.571  2.465  146.006  20210623-20210901\n",
       "2       best     csi500  20170104  20240628  181.43%  -14.55%  195.98%   17.17%     8.26%  6.60%  2.603  2.079  146.148  20231225-20240206\n",
       "0       best    csi1000  20170104  20240628  170.24%  -39.46%  209.70%   21.81%    14.22%  6.83%  3.195  1.534  146.256  20210429-20210908\n",
       "4    swabest     csi300  20170104  20240628  180.11%   21.07%  159.04%   11.71%     4.56%  4.41%  2.658  2.568  146.020  20210623-20210908\n",
       "5    swabest     csi500  20170104  20240628  201.82%  -14.55%  216.37%   18.26%     9.44%  6.93%  2.636  1.933  146.143  20240109-20240206\n",
       "3    swabest    csi1000  20170104  20240628  200.71%  -39.46%  240.17%   23.61%    13.40%  7.07%  3.339  1.763  146.239  20210429-20210908\n",
       "7    swalast     csi300  20170104  20240628  182.02%   21.07%  160.95%   11.82%     5.10%  4.38%  2.698  2.318  146.028  20231225-20240628\n",
       "8    swalast     csi500  20170104  20240628  197.56%  -14.55%  212.11%   18.09%    10.43%  6.85%  2.643  1.735  146.158  20210601-20210908\n",
       "6    swalast    csi1000  20170104  20240628  177.27%  -39.46%  216.73%   22.29%    16.09%  6.95%  3.207  1.385  146.234  20210429-20210901"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-07-04 06:37:08|MOD:time        |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 7 Hours 8 Minutes 8.3 Seconds\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytic datas are saved to /home/mengkjin/Workspace/learndl/model/gru_avg/detailed_analysis/data.xlsx\n",
      "Analytic plots are saved to /home/mengkjin/Workspace/learndl/model/gru_avg/detailed_analysis/plot.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<src.nn_model.trainer.trainer.NetTrainer at 0x76f7f85d6cb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.api import Trainer  \n",
    "app = Trainer.initialize(stage = 0 , resume = 1 , checkname= 1)\n",
    "app.go()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict is False , Data Processing start!\n",
      "4 datas :['y', 'day', '30m', 'risk']\n",
      "y blocks loading start!\n",
      " --> labels blocks reading [ret10_lag] DataBase...... finished! Cost 22.28 secs\n",
      " --> labels blocks reading [ret20_lag] DataBase...... finished! Cost 21.73 secs\n",
      " --> labels blocks merging (2)...... finished! Cost 4.75 secs\n",
      " --> models blocks reading [risk_exp] DataBase...... finished! Cost 114.89 secs\n",
      "y blocks loading finished! Cost 269.22 secs\n",
      "y blocks process...... finished! Cost 128.26 secs\n",
      "y blocks masking...... finished! Cost 1.38 secs\n",
      "y blocks saving ...... finished! Cost 2.88 secs\n",
      "y blocks norming...... finished! Cost 0.00 secs\n",
      "y finished! Cost 402.61 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "day blocks loading start!\n",
      " --> trade blocks reading [day] DataBase...... finished! Cost 33.11 secs\n",
      "day blocks loading finished! Cost 33.57 secs\n",
      "day blocks process...... finished! Cost 5.12 secs\n",
      "day blocks masking...... finished! Cost 1.16 secs\n",
      "day blocks saving ...... finished! Cost 3.25 secs\n",
      "day blocks norming...... finished! Cost 25.71 secs\n",
      "day finished! Cost 68.93 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "30m blocks loading start!\n",
      " --> trade blocks reading [30min] DataBase...... finished! Cost 183.87 secs\n",
      " --> trade blocks reading [day] DataBase...... finished! Cost 32.74 secs\n",
      "30m blocks loading finished! Cost 217.65 secs\n",
      "30m blocks process...... finished! Cost 334.88 secs\n",
      "30m blocks masking...... finished! Cost 4.27 secs\n",
      "30m blocks saving ...... finished! Cost 270.79 secs\n",
      "30m blocks norming...... finished! Cost 21.74 secs\n",
      "30m finished! Cost 850.95 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "risk blocks loading start!\n",
      " --> models blocks reading [risk_exp] DataBase...... finished! Cost 40.16 secs\n",
      "risk blocks loading finished! Cost 40.59 secs\n",
      "risk blocks process...... finished! Cost 0.00 secs\n",
      "risk blocks masking...... finished! Cost 1.10 secs\n",
      "risk blocks saving ...... finished! Cost 7.82 secs\n",
      "risk blocks norming...... finished! Cost 0.00 secs\n",
      "risk finished! Cost 49.62 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "Data Processing Finished! Cost 1372.12 Seconds\n",
      "predict is True , Data Processing start!\n",
      "4 datas :['y', 'day', '30m', 'risk']\n",
      "y blocks loading start!\n",
      " --> labels blocks reading [ret10_lag] DataBase...... finished! Cost 1.08 secs\n",
      " --> labels blocks reading [ret20_lag] DataBase...... finished! Cost 0.97 secs\n",
      " --> labels blocks merging (2)...... finished! Cost 0.15 secs\n",
      " --> models blocks reading [risk_exp] DataBase...... finished! Cost 2.75 secs\n",
      "y blocks loading finished! Cost 5.26 secs\n",
      "y blocks process...... finished! Cost 5.02 secs\n",
      "y blocks masking...... finished! Cost 0.13 secs\n",
      "y blocks saving ...... finished! Cost 0.16 secs\n",
      "y blocks norming...... finished! Cost 0.00 secs\n",
      "y finished! Cost 10.69 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "day blocks loading start!\n",
      " --> trade blocks reading [day] DataBase...... finished! Cost 1.55 secs\n",
      "day blocks loading finished! Cost 1.58 secs\n",
      "day blocks process...... finished! Cost 0.14 secs\n",
      "day blocks masking...... finished! Cost 0.07 secs\n",
      "day blocks saving ...... finished! Cost 0.15 secs\n",
      "day blocks norming...... finished! Cost 0.00 secs\n",
      "day finished! Cost 2.02 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "30m blocks loading start!\n",
      " --> trade blocks reading [30min] DataBase...... finished! Cost 6.24 secs\n",
      " --> trade blocks reading [day] DataBase...... finished! Cost 1.06 secs\n",
      "30m blocks loading finished! Cost 7.35 secs\n",
      "30m blocks process...... finished! Cost 1.65 secs\n",
      "30m blocks masking...... finished! Cost 0.17 secs\n",
      "30m blocks saving ...... finished! Cost 1.85 secs\n",
      "30m blocks norming...... finished! Cost 0.00 secs\n",
      "30m finished! Cost 11.11 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "risk blocks loading start!\n",
      " --> models blocks reading [risk_exp] DataBase...... finished! Cost 1.45 secs\n",
      "risk blocks loading finished! Cost 1.50 secs\n",
      "risk blocks process...... finished! Cost 0.00 secs\n",
      "risk blocks masking...... finished! Cost 0.08 secs\n",
      "risk blocks saving ...... finished! Cost 0.23 secs\n",
      "risk blocks norming...... finished! Cost 0.00 secs\n",
      "risk finished! Cost 1.90 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "Data Processing Finished! Cost 25.72 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "This is not server! Will not update models!\n"
     ]
    }
   ],
   "source": [
    "from src.api import DataAPI , Trainer\n",
    "DataAPI.reconstruct_train_data()\n",
    "Trainer.update_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device name: NVIDIA GeForce RTX 4090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:10:43|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:10:43|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Mon Jun 24 01:10:43 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Process Queue : Data + Fit + Test\n",
      "--Confirm Resume Training!\n",
      "--Model_name is set to gru_day!\n",
      "Callback : ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True) , reset optimizer on some epoch (can speedup scheduler)\n",
      "Callback : CallbackTimer(verbosity=2) , record time cost of callback hooks\n",
      "Callback : EarlyStoppage(patience=20) , stop fitting when validation score cease to improve\n",
      "Callback : ValidationConverge(patience=5,eps=1e-05) , stop fitting when valid_score converge\n",
      "Callback : EarlyExitRetrain(earliest=5,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100]) , retrain with new lr if fitting stopped too early\n",
      "Callback : NanLossRetrain(max_attempt=4) , retrain if fitting encounters nan loss\n",
      "Callback : BatchDisplay(verbosity=2) , display batch progress bar\n",
      "Callback : StatusDisplay(verbosity=2) , display epoch and event information\n",
      "{'random_seed': None,\n",
      " 'model_name': 'gruRES_day',\n",
      " 'model_module': 'gru',\n",
      " 'model_data_type': 'day',\n",
      " 'model_types': ['best', 'swalast', 'swabest'],\n",
      " 'labels': ['std_lag1_10'],\n",
      " 'beg_date': 20170103,\n",
      " 'end_date': 99991231,\n",
      " 'sample_method': 'train_shuffle',\n",
      " 'shuffle_option': 'epoch',\n",
      " 'lgbm_ensembler': False}\n",
      "{'hidden_dim': [32, 64],\n",
      " 'seqlens': [{'day': 30, '30m': 30, 'dms': 30}],\n",
      " 'tra_seqlens': [{'hist_loss': 40}],\n",
      " 'dropout': [0.1],\n",
      " 'enc_in': [True],\n",
      " 'enc_att': [False],\n",
      " 'rnn_type': ['lstm'],\n",
      " 'rnn_att': [False],\n",
      " 'rnn_layers': [2],\n",
      " 'dec_mlp_layers': [2],\n",
      " 'num_output': [1],\n",
      " 'kernel_size': [3],\n",
      " 'hidden_as_factor': [False],\n",
      " 'ordered_param_group': [False],\n",
      " 'tra_num_states': [5]}\n",
      "Load  2 DataBlocks...... finished! Cost 2.40 secs\n",
      "Align 2 DataBlocks...... finished! Cost 2.86 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:10:51|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 7.9 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:10:51|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Mon Jun 24 01:10:51 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n",
      "score function of [spearman] calculated and success!\n",
      "loss function of [pearson] calculated and success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mFirstBite Ep#  0 : loss  0.99770, train 0.00391, valid 0.03668, best 0.0367, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.87023, train 0.12798, valid 0.09558, best 0.0956, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.85418, train 0.14255, valid 0.08890, best 0.0956, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.84138, train 0.15293, valid 0.08662, best 0.0956, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.83995, train 0.15505, valid 0.09025, best 0.0956, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.83040, train 0.16270, valid 0.08154, best 0.0956, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.83040, train 0.16270, valid 0.08154, best 0.0956, lr3.1e-04, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  0 : loss  0.99907, train 0.00307, valid-0.00235, best-0.0024, lr1.3e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  5 : loss  0.89798, train 0.10238, valid 0.07134, best 0.0839, lr2.5e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 10 : loss  0.88484, train 0.11407, valid 0.08712, best 0.0892, lr1.9e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 15 : loss  0.87704, train 0.12033, valid 0.08682, best 0.0892, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 20 : loss  0.87598, train 0.12115, valid 0.08665, best 0.0892, lr9.4e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 25 : loss  0.87235, train 0.12489, valid 0.08559, best 0.0892, lr3.1e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 25 : loss  0.87235, train 0.12489, valid 0.08559, best 0.0892, lr3.1e-05, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  0 : loss  1.00802, train-0.01361, valid-0.04341, best-0.0434, lr1.3e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  5 : loss  0.90600, train 0.09749, valid 0.08519, best 0.0985, lr2.5e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 10 : loss  0.88704, train 0.11519, valid 0.10017, best 0.1002, lr1.9e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 15 : loss  0.88873, train 0.11359, valid 0.09794, best 0.1002, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 20 : loss  0.89135, train 0.11055, valid 0.09254, best 0.1002, lr9.4e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 25 : loss  0.88877, train 0.11064, valid 0.09957, best 0.1021, lr3.1e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 30 : loss  0.88839, train 0.10964, valid 0.08501, best 0.1021, lr1.6e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 35 : loss  0.88399, train 0.11418, valid 0.08975, best 0.1021, lr3.1e-03\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 39 , effective at epoch 40\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 40 : loss  0.88123, train 0.11494, valid 0.09168, best 0.1021, lr1.3e-02\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:16:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #1 @20231201|Retrain#2 Ep# 96 EarlyStop|Train 0.1037 Valid 0.0934 BestVal 0.0956|Cost  5.7Min,  3.5Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  1.00213, train-0.00346, valid-0.02870, best-0.0287, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.88101, train 0.11998, valid 0.10566, best 0.1118, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.86625, train 0.13116, valid 0.09471, best 0.1118, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.85764, train 0.13868, valid 0.09772, best 0.1118, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.85723, train 0.13898, valid 0.10404, best 0.1118, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 24 : loss  0.85347, train 0.14217, valid 0.09795, best 0.1118, lr1.6e-04, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  0 : loss  0.99619, train 0.00640, valid 0.02634, best 0.0263, lr1.3e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  5 : loss  0.91904, train 0.07677, valid 0.07904, best 0.0790, lr2.5e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 10 : loss  0.90607, train 0.09211, valid 0.09041, best 0.0904, lr1.9e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 15 : loss  0.89860, train 0.10379, valid 0.09553, best 0.0956, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 20 : loss  0.89763, train 0.10523, valid 0.09337, best 0.0956, lr9.4e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 24 : loss  0.89522, train 0.10913, valid 0.09689, best 0.0970, lr1.6e-05, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  0 : loss  1.00127, train-0.00532, valid-0.02776, best-0.0278, lr1.3e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  5 : loss  0.90841, train 0.09496, valid 0.09364, best 0.1105, lr2.5e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 10 : loss  0.88931, train 0.11243, valid 0.09934, best 0.1105, lr1.9e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 15 : loss  0.88411, train 0.11520, valid 0.09632, best 0.1105, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 20 : loss  0.88303, train 0.11682, valid 0.10441, best 0.1105, lr9.4e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 24 : loss  0.87640, train 0.12191, valid 0.10175, best 0.1105, lr1.6e-03, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  0 : loss  0.99942, train 0.00330, valid 0.03781, best 0.0378, lr1.3e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  5 : loss  0.96270, train 0.04376, valid 0.07316, best 0.0732, lr2.5e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 10 : loss  0.95829, train 0.04798, valid 0.07296, best 0.0736, lr1.9e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 15 : loss  0.95190, train 0.05224, valid 0.07273, best 0.0736, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 20 : loss  0.95024, train 0.05357, valid 0.07160, best 0.0736, lr9.5e-06\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 24 : loss  0.94829, train 0.05503, valid 0.07283, best 0.0736, lr1.7e-06, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  0 : loss  0.99623, train 0.00749, valid 0.03126, best 0.0313, lr1.3e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  5 : loss  0.97201, train 0.03669, valid 0.04230, best 0.0540, lr2.5e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 10 : loss  0.97588, train 0.03329, valid 0.03690, best 0.0540, lr1.9e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 15 : loss  0.97586, train 0.03358, valid 0.04005, best 0.0540, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 20 : loss  0.97032, train 0.04237, valid 0.04644, best 0.0540, lr9.4e-02\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:22:42|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #0 @20240604|Retrain#4 Ep#125 EarlyStop|Train 0.0369 Valid 0.0350 BestVal 0.1118|Cost  6.0Min,  2.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:24:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #1 @20240604|FirstBite Ep# 34 EarlyStop|Train 0.1667 Valid 0.0877 BestVal 0.0877|Cost  2.1Min,  3.6Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:24:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Fit], Cost 0.2 Hours, 4.7 Min/model, 3.3 Sec/Epoch\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:24:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Mon Jun 24 01:24:48 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:24:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mEach Model Date Testing Mean Score(spearman):\u001b[0m\n",
      "\u001b[32mModels            0       0       0       1       1       1\u001b[0m\n",
      "\u001b[32mOutput         best swalast swabest    best swalast swabest\u001b[0m\n",
      "\u001b[32m20170103     0.1536  0.1525  0.1520  0.1525  0.1521  0.1520\u001b[0m\n",
      "\u001b[32m20170704     0.1373  0.1327  0.1365  0.1413  0.1373  0.1405\u001b[0m\n",
      "\u001b[32m20171226     0.1305  0.1323  0.1329  0.1291  0.1298  0.1300\u001b[0m\n",
      "\u001b[32m20180627     0.1225  0.1186  0.1223  0.1226  0.1210  0.1236\u001b[0m\n",
      "\u001b[32m20181220     0.0998  0.0990  0.1004  0.0958  0.0981  0.0987\u001b[0m\n",
      "\u001b[32m20190624     0.0970  0.0955  0.0972  0.0920  0.0908  0.0924\u001b[0m\n",
      "\u001b[32m20191217     0.1011  0.0991  0.1005  0.1042  0.1048  0.1047\u001b[0m\n",
      "\u001b[32m20200617     0.0970  0.0963  0.0978  0.0947  0.0927  0.0946\u001b[0m\n",
      "\u001b[32m20201214     0.0837  0.0792  0.0842  0.0759  0.0766  0.0739\u001b[0m\n",
      "\u001b[32m20210615     0.0605  0.0572  0.0608  0.0689  0.0666  0.0662\u001b[0m\n",
      "\u001b[32m20211209     0.0935  0.0954  0.0962  0.1084  0.1106  0.1084\u001b[0m\n",
      "\u001b[32m20220613     0.0931  0.0909  0.0813  0.0857  0.0829  0.0876\u001b[0m\n",
      "\u001b[32m20221206     0.0650  0.0616  0.0635  0.0580  0.0576  0.0497\u001b[0m\n",
      "\u001b[32m20230606     0.0906  0.0857  0.0872  0.0868  0.0885  0.0856\u001b[0m\n",
      "\u001b[32m20231201     0.1217  0.1221  0.1235  0.1082  0.1025  0.1008\u001b[0m\n",
      "\u001b[32m20240604    -0.0130  0.0400 -0.0266  0.0401  0.0503  0.0572\u001b[0m\n",
      "\u001b[32mAllTimeAvg   0.1024  0.1008  0.1016  0.1012  0.1005  0.1003\u001b[0m\n",
      "\u001b[32mAllTimeSum   185.48  182.66  184.04  183.36  182.05  181.72\u001b[0m\n",
      "\u001b[32mStd          0.0668  0.0665  0.0669  0.0666  0.0656  0.0665\u001b[0m\n",
      "\u001b[32mTValue        65.23   64.56   64.64   64.69   65.24   64.18\u001b[0m\n",
      "\u001b[32mAnnIR        7.5077  7.4304  7.4389  7.4445  7.5079  7.3860\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:25:46|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 57.6 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:25:46|MOD:time        |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 15 Minutes 3.3 Seconds\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:25:46|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:25:46|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Mon Jun 24 01:25:46 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Process Queue : Data + Fit + Test\n",
      "--Confirm Resume Training!\n",
      "--Model_name is set to gruRTN_day!\n",
      "Callback : ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True) , reset optimizer on some epoch (can speedup scheduler)\n",
      "Callback : CallbackTimer(verbosity=2) , record time cost of callback hooks\n",
      "Callback : EarlyStoppage(patience=20) , stop fitting when validation score cease to improve\n",
      "Callback : ValidationConverge(patience=5,eps=1e-05) , stop fitting when valid_score converge\n",
      "Callback : EarlyExitRetrain(earliest=5,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100]) , retrain with new lr if fitting stopped too early\n",
      "Callback : NanLossRetrain(max_attempt=4) , retrain if fitting encounters nan loss\n",
      "Callback : BatchDisplay(verbosity=2) , display batch progress bar\n",
      "Callback : StatusDisplay(verbosity=2) , display epoch and event information\n",
      "{'random_seed': None,\n",
      " 'model_name': 'gruRTN_day',\n",
      " 'model_module': 'gru',\n",
      " 'model_data_type': 'day',\n",
      " 'model_types': ['best', 'swalast', 'swabest'],\n",
      " 'labels': ['rtn_lag1_10'],\n",
      " 'beg_date': 20170103,\n",
      " 'end_date': 99991231,\n",
      " 'sample_method': 'train_shuffle',\n",
      " 'shuffle_option': 'epoch',\n",
      " 'lgbm_ensembler': False}\n",
      "{'hidden_dim': [32, 64],\n",
      " 'seqlens': [{'day': 30, '30m': 30, 'dms': 30}],\n",
      " 'tra_seqlens': [{'hist_loss': 40}],\n",
      " 'dropout': [0.1],\n",
      " 'enc_in': [True],\n",
      " 'enc_att': [False],\n",
      " 'rnn_type': ['lstm'],\n",
      " 'rnn_att': [False],\n",
      " 'rnn_layers': [2],\n",
      " 'dec_mlp_layers': [2],\n",
      " 'num_output': [1],\n",
      " 'kernel_size': [3],\n",
      " 'hidden_as_factor': [False],\n",
      " 'ordered_param_group': [False],\n",
      " 'tra_num_states': [5]}\n",
      "try using /home/mengkjin/Workspace/learndl/data/DataSet/day.20240605.pt , success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:25:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 2.1 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:25:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Mon Jun 24 01:25:48 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mFirstBite Ep#  0 : loss  1.00382, train-0.00516, valid-0.03649, best-0.0365, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.86311, train 0.13764, valid 0.09228, best 0.0923, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.84547, train 0.15544, valid 0.09630, best 0.0963, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.83038, train 0.17066, valid 0.08826, best 0.0963, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.82732, train 0.17363, valid 0.08575, best 0.0963, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.81729, train 0.18370, valid 0.08407, best 0.0963, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 30 : loss  0.81421, train 0.18647, valid 0.08262, best 0.0963, lr1.6e-04\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:27:46|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRTN_day #1 @20231201|FirstBite Ep# 31 EarlyStop|Train 0.1865 Valid 0.0826 BestVal 0.0826|Cost  1.9Min,  3.6Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  1.00293, train-0.00341, valid-0.00344, best-0.0034, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.87709, train 0.12551, valid 0.08796, best 0.1129, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.86180, train 0.13731, valid 0.10436, best 0.1129, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.85287, train 0.14587, valid 0.10502, best 0.1129, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.85180, train 0.14778, valid 0.09660, best 0.1129, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 23 : loss  0.84699, train 0.15223, valid 0.10093, best 0.1129, lr1.0e-07, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  0 : loss  0.99854, train 0.00457, valid 0.01988, best 0.0199, lr1.3e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  5 : loss  0.92030, train 0.08103, valid 0.08255, best 0.0825, lr2.5e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 10 : loss  0.90702, train 0.09389, valid 0.08568, best 0.0857, lr1.9e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 15 : loss  0.89605, train 0.10568, valid 0.10030, best 0.1003, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 20 : loss  0.89414, train 0.10746, valid 0.09676, best 0.1003, lr9.4e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 23 : loss  0.89076, train 0.11050, valid 0.09963, best 0.1003, lr1.0e-07, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  0 : loss  0.99821, train 0.00317, valid 0.04276, best 0.0428, lr1.3e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  5 : loss  0.91434, train 0.07999, valid 0.07219, best 0.1080, lr2.5e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 10 : loss  0.89327, train 0.10612, valid 0.08880, best 0.1080, lr1.9e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 15 : loss  0.88616, train 0.11392, valid 0.08959, best 0.1080, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 20 : loss  0.88396, train 0.11643, valid 0.09105, best 0.1080, lr9.4e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 23 : loss  0.87573, train 0.12499, valid 0.08870, best 0.1080, lr1.0e-07, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  0 : loss  1.00152, train 0.00047, valid 0.02760, best 0.0276, lr1.3e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  5 : loss  0.96801, train 0.04157, valid 0.06108, best 0.0611, lr2.5e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 10 : loss  0.95807, train 0.04909, valid 0.05827, best 0.0611, lr1.9e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 15 : loss  0.95065, train 0.05443, valid 0.05588, best 0.0611, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 20 : loss  0.94877, train 0.05559, valid 0.05718, best 0.0611, lr9.5e-06\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 23 : loss  0.94672, train 0.05714, valid 0.05652, best 0.0611, lr1.0e-07, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  0 : loss  1.00047, train-0.00169, valid-0.00940, best-0.0094, lr1.3e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  5 : loss  0.97745, train 0.03642, valid 0.05397, best 0.0540, lr2.5e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 10 : loss  0.96087, train 0.04016, valid 0.04625, best 0.0540, lr1.9e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 15 : loss  0.97887, train 0.02974, valid 0.04515, best 0.0592, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 20 : loss  0.97797, train 0.03105, valid 0.02864, best 0.0592, lr9.4e-02\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:33:42|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRTN_day #0 @20240604|Retrain#4 Ep#120 EarlyStop|Train 0.0324 Valid 0.0466 BestVal 0.1129|Cost  5.9Min,  2.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:35:37|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRTN_day #1 @20240604|FirstBite Ep# 31 EarlyStop|Train 0.1916 Valid 0.0938 BestVal 0.0938|Cost  1.9Min,  3.6Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:35:37|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Fit], Cost 0.2 Hours, 3.3 Min/model, 3.2 Sec/Epoch\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:35:37|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Mon Jun 24 01:35:37 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:35:37|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mEach Model Date Testing Mean Score(spearman):\u001b[0m\n",
      "\u001b[32mModels            0       0       0       1       1       1\u001b[0m\n",
      "\u001b[32mOutput         best swalast swabest    best swalast swabest\u001b[0m\n",
      "\u001b[32m20170103     0.1415  0.1404  0.1427  0.1449  0.1435  0.1447\u001b[0m\n",
      "\u001b[32m20170704     0.1344  0.1262  0.1345  0.1305  0.1318  0.1309\u001b[0m\n",
      "\u001b[32m20171226     0.1443  0.1441  0.1454  0.1443  0.1440  0.1482\u001b[0m\n",
      "\u001b[32m20180627     0.1204  0.1149  0.1163  0.1097  0.1025  0.1077\u001b[0m\n",
      "\u001b[32m20181220     0.1134  0.1100  0.1116  0.1024  0.1004  0.1035\u001b[0m\n",
      "\u001b[32m20190624     0.0928  0.0923  0.0936  0.0977  0.0973  0.0969\u001b[0m\n",
      "\u001b[32m20191217     0.1172  0.1181  0.1180  0.1052  0.1068  0.1165\u001b[0m\n",
      "\u001b[32m20200617     0.0920  0.0939  0.0939  0.0902  0.0826  0.0929\u001b[0m\n",
      "\u001b[32m20201214     0.0986  0.0984  0.0981  0.0939  0.0944  0.0957\u001b[0m\n",
      "\u001b[32m20210615     0.0696  0.0720  0.0718  0.0754  0.0735  0.0734\u001b[0m\n",
      "\u001b[32m20211209     0.1126  0.1152  0.1148  0.1102  0.1128  0.1136\u001b[0m\n",
      "\u001b[32m20220613     0.1110  0.1046  0.1063  0.1074  0.1058  0.1127\u001b[0m\n",
      "\u001b[32m20221206     0.0574  0.0559  0.0579  0.0602  0.0553  0.0578\u001b[0m\n",
      "\u001b[32m20230606     0.0824  0.0776  0.0835  0.0856  0.0771  0.0824\u001b[0m\n",
      "\u001b[32m20231201     0.1327  0.1357  0.1289  0.1284  0.1280  0.1267\u001b[0m\n",
      "\u001b[32m20240604    -0.0291  0.0381  0.0338  0.0062  0.0266  0.0194\u001b[0m\n",
      "\u001b[32mAllTimeAvg   0.1071  0.1062  0.1073  0.1051  0.1032  0.1063\u001b[0m\n",
      "\u001b[32mAllTimeSum   194.08  192.38  194.48  190.38  187.02  192.67\u001b[0m\n",
      "\u001b[32mStd          0.0877  0.0881  0.0874  0.0877  0.0868  0.0861\u001b[0m\n",
      "\u001b[32mTValue        51.97   51.28   52.26   51.01   50.60   52.58\u001b[0m\n",
      "\u001b[32mAnnIR        5.9810  5.9022  6.0149  5.8711  5.8230  6.0509\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:36:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 54.9 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:36:32|MOD:time        |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 10 Minutes 45.7 Seconds\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:36:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:36:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Mon Jun 24 01:36:32 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Process Queue : Data + Fit + Test\n",
      "--Confirm Resume Training!\n",
      "--Model_name is set to gruRES_day!\n",
      "Callback : ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True) , reset optimizer on some epoch (can speedup scheduler)\n",
      "Callback : CallbackTimer(verbosity=2) , record time cost of callback hooks\n",
      "Callback : EarlyStoppage(patience=20) , stop fitting when validation score cease to improve\n",
      "Callback : ValidationConverge(patience=5,eps=1e-05) , stop fitting when valid_score converge\n",
      "Callback : EarlyExitRetrain(earliest=5,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100]) , retrain with new lr if fitting stopped too early\n",
      "Callback : NanLossRetrain(max_attempt=4) , retrain if fitting encounters nan loss\n",
      "Callback : BatchDisplay(verbosity=2) , display batch progress bar\n",
      "Callback : StatusDisplay(verbosity=2) , display epoch and event information\n",
      "{'random_seed': None,\n",
      " 'model_name': 'gruRES_day',\n",
      " 'model_module': 'gru',\n",
      " 'model_data_type': 'day',\n",
      " 'model_types': ['best', 'swalast', 'swabest'],\n",
      " 'labels': ['res_lag1_10'],\n",
      " 'beg_date': 20170103,\n",
      " 'end_date': 99991231,\n",
      " 'sample_method': 'train_shuffle',\n",
      " 'shuffle_option': 'epoch',\n",
      " 'lgbm_ensembler': False}\n",
      "{'hidden_dim': [32, 64],\n",
      " 'seqlens': [{'day': 30, '30m': 30, 'dms': 30}],\n",
      " 'tra_seqlens': [{'hist_loss': 40}],\n",
      " 'dropout': [0.1],\n",
      " 'enc_in': [True],\n",
      " 'enc_att': [False],\n",
      " 'rnn_type': ['lstm'],\n",
      " 'rnn_att': [False],\n",
      " 'rnn_layers': [2],\n",
      " 'dec_mlp_layers': [2],\n",
      " 'num_output': [1],\n",
      " 'kernel_size': [3],\n",
      " 'hidden_as_factor': [False],\n",
      " 'ordered_param_group': [False],\n",
      " 'tra_num_states': [5]}\n",
      "try using /home/mengkjin/Workspace/learndl/data/DataSet/day.20240605.pt , success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:36:34|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 2.0 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:36:34|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Mon Jun 24 01:36:34 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mFirstBite Ep#  0 : loss  1.01252, train-0.01521, valid-0.03742, best-0.0374, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.91026, train 0.08135, valid 0.03185, best 0.0599, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.89291, train 0.09436, valid 0.04522, best 0.0599, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.87755, train 0.10648, valid 0.03959, best 0.0599, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.87426, train 0.10831, valid 0.03543, best 0.0599, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 21 : loss  0.87082, train 0.11088, valid 0.03980, best 0.0599, lr6.3e-04, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  0 : loss  0.99637, train 0.00462, valid 0.03106, best 0.0311, lr1.3e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  5 : loss  0.94196, train 0.05698, valid 0.03578, best 0.0560, lr2.5e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 10 : loss  0.92553, train 0.06750, valid 0.03080, best 0.0560, lr1.9e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 15 : loss  0.91653, train 0.07499, valid 0.03537, best 0.0560, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 20 : loss  0.91509, train 0.07561, valid 0.04185, best 0.0560, lr9.4e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 21 : loss  0.91403, train 0.07721, valid 0.04641, best 0.0560, lr6.3e-05, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  0 : loss  1.00549, train-0.00683, valid-0.02416, best-0.0242, lr1.3e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  5 : loss  0.94359, train 0.04867, valid 0.02798, best 0.0514, lr2.5e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 10 : loss  0.93368, train 0.05600, valid 0.02787, best 0.0514, lr1.9e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 15 : loss  0.92716, train 0.06278, valid 0.03398, best 0.0514, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 20 : loss  0.92646, train 0.06384, valid 0.04303, best 0.0514, lr9.4e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 21 : loss  0.92585, train 0.06408, valid 0.03492, best 0.0514, lr6.3e-03, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  0 : loss  1.01260, train-0.01493, valid-0.03974, best-0.0397, lr1.3e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  5 : loss  0.96876, train 0.02948, valid 0.04494, best 0.0477, lr2.5e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 10 : loss  0.96445, train 0.03233, valid 0.04554, best 0.0477, lr1.9e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 15 : loss  0.96098, train 0.03497, valid 0.04404, best 0.0477, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 20 : loss  0.96022, train 0.03574, valid 0.04354, best 0.0477, lr9.5e-06\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 21 : loss  0.95992, train 0.03628, valid 0.04487, best 0.0477, lr6.3e-06, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  0 : loss  0.99746, train 0.00225, valid-0.00306, best-0.0031, lr1.3e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  5 : loss  0.97511, train 0.02839, valid 0.01301, best 0.0537, lr2.5e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 10 : loss  0.96875, train 0.03070, valid 0.01638, best 0.0537, lr1.9e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 15 : loss  0.96160, train 0.03236, valid 0.03529, best 0.0537, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 20 : loss  0.96655, train 0.02830, valid 0.04237, best 0.0537, lr9.4e-02\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:43:22|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #1 @20231201|Retrain#4 Ep#110 EarlyStop|Train 0.0293 Valid 0.0368 BestVal 0.0599|Cost  6.7Min,  3.6Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  0.99752, train 0.00292, valid 0.02356, best 0.0236, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.92738, train 0.06719, valid 0.04180, best 0.0533, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.90904, train 0.07964, valid 0.02435, best 0.0533, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.89805, train 0.08672, valid 0.03391, best 0.0533, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.89733, train 0.08698, valid 0.03455, best 0.0533, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 21 : loss  0.89541, train 0.08806, valid 0.04139, best 0.0533, lr6.3e-04, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  0 : loss  1.00733, train-0.00926, valid-0.03680, best-0.0368, lr1.3e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  5 : loss  0.95510, train 0.04285, valid 0.05432, best 0.0549, lr2.5e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 10 : loss  0.94688, train 0.04934, valid 0.04339, best 0.0549, lr1.9e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 15 : loss  0.93907, train 0.05436, valid 0.03791, best 0.0549, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 20 : loss  0.93641, train 0.05594, valid 0.03422, best 0.0549, lr9.4e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 22 : loss  0.93590, train 0.05702, valid 0.04172, best 0.0549, lr3.1e-05, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  0 : loss  0.99719, train 0.00375, valid 0.01564, best 0.0156, lr1.3e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  5 : loss  0.94177, train 0.04812, valid 0.03631, best 0.0465, lr2.5e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 10 : loss  0.93093, train 0.05774, valid 0.03864, best 0.0465, lr1.9e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 15 : loss  0.92785, train 0.05940, valid 0.02965, best 0.0465, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 20 : loss  0.92459, train 0.06304, valid 0.03018, best 0.0465, lr9.4e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 22 : loss  0.92154, train 0.06476, valid 0.03223, best 0.0465, lr3.1e-03, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  0 : loss  0.99855, train 0.00161, valid 0.01776, best 0.0178, lr1.3e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  5 : loss  0.97299, train 0.02829, valid 0.05143, best 0.0514, lr2.5e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 10 : loss  0.96918, train 0.02884, valid 0.05178, best 0.0518, lr1.9e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 15 : loss  0.96640, train 0.02942, valid 0.05233, best 0.0523, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 20 : loss  0.96524, train 0.02948, valid 0.05201, best 0.0523, lr9.5e-06\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 22 : loss  0.96442, train 0.03008, valid 0.05196, best 0.0523, lr3.2e-06, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  0 : loss  1.00164, train-0.00223, valid-0.02355, best-0.0236, lr1.3e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  5 : loss  0.97856, train 0.02428, valid 0.02481, best 0.0474, lr2.5e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 10 : loss  0.97707, train 0.02539, valid 0.04388, best 0.0474, lr1.9e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 15 : loss  0.97920, train 0.02491, valid 0.01606, best 0.0474, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 20 : loss  0.97573, train 0.02385, valid 0.01877, best 0.0474, lr9.4e-02\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:48:54|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #0 @20240604|Retrain#4 Ep#114 EarlyStop|Train 0.0265 Valid 0.0202 BestVal 0.0549|Cost  5.5Min,  2.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:56:07|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #1 @20240604|Retrain#4 Ep#115 EarlyStop|Train 0.0274 Valid 0.0224 BestVal 0.0568|Cost  7.2Min,  3.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:56:07|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Fit], Cost 0.3 Hours, 6.5 Min/model, 3.5 Sec/Epoch\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:56:07|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Mon Jun 24 01:56:07 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:56:07|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mEach Model Date Testing Mean Score(spearman):\u001b[0m\n",
      "\u001b[32mModels            0       0       0       1       1       1\u001b[0m\n",
      "\u001b[32mOutput         best swalast swabest    best swalast swabest\u001b[0m\n",
      "\u001b[32m20170103     0.1107  0.1119  0.1114  0.1060  0.1068  0.1081\u001b[0m\n",
      "\u001b[32m20170704     0.0937  0.0946  0.0972  0.0970  0.0989  0.0993\u001b[0m\n",
      "\u001b[32m20171226     0.0889  0.0892  0.0893  0.0901  0.0905  0.0905\u001b[0m\n",
      "\u001b[32m20180627     0.0713  0.0712  0.0713  0.0738  0.0715  0.0736\u001b[0m\n",
      "\u001b[32m20181220     0.0733  0.0742  0.0746  0.0740  0.0741  0.0728\u001b[0m\n",
      "\u001b[32m20190624     0.0702  0.0701  0.0703  0.0663  0.0672  0.0681\u001b[0m\n",
      "\u001b[32m20191217     0.0774  0.0745  0.0763  0.0789  0.0759  0.0750\u001b[0m\n",
      "\u001b[32m20200617     0.0555  0.0563  0.0570  0.0597  0.0579  0.0592\u001b[0m\n",
      "\u001b[32m20201214     0.0610  0.0605  0.0609  0.0619  0.0606  0.0642\u001b[0m\n",
      "\u001b[32m20210615     0.0406  0.0406  0.0438  0.0459  0.0475  0.0477\u001b[0m\n",
      "\u001b[32m20211209     0.0568  0.0572  0.0567  0.0561  0.0556  0.0573\u001b[0m\n",
      "\u001b[32m20220613     0.0457  0.0450  0.0465  0.0447  0.0461  0.0462\u001b[0m\n",
      "\u001b[32m20221206     0.0241  0.0235  0.0277  0.0287  0.0243  0.0296\u001b[0m\n",
      "\u001b[32m20230606     0.0589  0.0451  0.0425  0.0361  0.0324  0.0353\u001b[0m\n",
      "\u001b[32m20231201     0.0291  0.0323  0.0298  0.0140  0.0078  0.0160\u001b[0m\n",
      "\u001b[32m20240604    -0.0728 -0.0680 -0.0678 -0.0540  0.0016 -0.0326\u001b[0m\n",
      "\u001b[32mAllTimeAvg   0.0629  0.0622  0.0628  0.0614  0.0608  0.0622\u001b[0m\n",
      "\u001b[32mAllTimeSum   113.98  112.74  113.82  111.32  110.09  112.77\u001b[0m\n",
      "\u001b[32mStd          0.0537  0.0507  0.0500  0.0504  0.0493  0.0490\u001b[0m\n",
      "\u001b[32mTValue        49.87   52.28   53.42   51.92   52.51   54.01\u001b[0m\n",
      "\u001b[32mAnnIR        5.7394  6.0170  6.1485  5.9749  6.0436  6.2161\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:57:02|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 55.3 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:57:02|MOD:time        |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 20 Minutes 30.0 Seconds\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from src.api import Trainer\n",
    "Trainer.update_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
