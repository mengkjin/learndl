{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 2.1.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available() , torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device name: NVIDIA GeForce RTX 4090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[44m24-07-21 23:05:51|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-07-21 23:05:52|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Sun Jul 21 23:05:52 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet_gru nn\n",
      "--Process Queue : Data + Test\n",
      "--Model_name is set to resnet_gru_30m.1!\n",
      "Callback : ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True,kwargs={}) , reset optimizer on some epoch (can speedup scheduler)\n",
      "Callback : CallbackTimer(verbosity=2,kwargs={}) , record time cost of callback hooks\n",
      "Callback : EarlyStoppage(patience=20,kwargs={}) , stop fitting when validation score cease to improve\n",
      "Callback : ValidationConverge(patience=5,eps=1e-05,kwargs={}) , stop fitting when valid_score converge\n",
      "Callback : EarlyExitRetrain(earliest=10,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100, 1],kwargs={}) , retrain with new lr if fitting stopped too early\n",
      "Callback : NanLossRetrain(max_attempt=4,kwargs={}) , retrain if fitting encounters nan loss\n",
      "Callback : BatchDisplay(verbosity=2,kwargs={}) , display batch progress bar\n",
      "Callback : StatusDisplay(verbosity=2,kwargs={}) , display epoch and event information\n",
      "Callback : GroupReturnAnalysis(group_num=20,kwargs={}) , record and concat each model to Alpha model instance\n",
      "Callback : DetailedAlphaAnalysis(use_num=avg,kwargs={}) , record and concat each model to Alpha model instance\n",
      "{'model_name': 'resnet_gru_30m.1',\n",
      " 'model_module': 'resnet_gru',\n",
      " 'model.types': ['best', 'swalast', 'swabest'],\n",
      " 'model.lgbm_ensembler': False,\n",
      " 'data.type': '30m',\n",
      " 'data.labels': ['std_lag1_10', 'rtn_lag1_10'],\n",
      " 'random_seed': None,\n",
      " 'beg_date': 20170103,\n",
      " 'end_date': 99991231,\n",
      " 'sample_method': 'sequential',\n",
      " 'shuffle_option': 'epoch'}\n",
      "{'hidden_dim': [32, 64],\n",
      " 'seqlens': [{'day': 30, '30m': 30}],\n",
      " 'dropout': [0.1],\n",
      " 'enc_in': [True],\n",
      " 'enc_att': [True],\n",
      " 'rnn_type': ['gru'],\n",
      " 'rnn_att': [True],\n",
      " 'rnn_layers': [2],\n",
      " 'dec_mlp_layers': [2],\n",
      " 'num_output': [1],\n",
      " 'which_output': [0],\n",
      " 'resnet_blocks': [1],\n",
      " 'enc_in_dim': [16],\n",
      " 'hidden_as_factor': [True]}\n",
      "try using /home/mengkjin/Workspace/learndl/data/DataSet/30m.20240703.pt , success!\n",
      "Load  2 DataBlocks...... finished! Cost 1.11 secs\n",
      "Align 2 DataBlocks...... finished! Cost 0.78 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-07-21 23:06:07|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 15.3 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-07-21 23:06:07|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Sun Jul 21 23:06:07 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [30m] : {'divlast': False, 'histnorm': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengkjin/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score function of [spearman] calculated and success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[44m24-07-21 23:09:05|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mTesting Mean Score(spearman):\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">resnet_gru.0</th>\n",
       "      <th colspan=\"3\" halign=\"left\">resnet_gru.1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>best</th>\n",
       "      <th>swalast</th>\n",
       "      <th>swabest</th>\n",
       "      <th>best</th>\n",
       "      <th>swalast</th>\n",
       "      <th>swabest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20170103</th>\n",
       "      <td>0.165</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20170704</th>\n",
       "      <td>0.142</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20171226</th>\n",
       "      <td>0.132</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180627</th>\n",
       "      <td>0.150</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181220</th>\n",
       "      <td>0.117</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190624</th>\n",
       "      <td>0.094</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20191217</th>\n",
       "      <td>0.105</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20200617</th>\n",
       "      <td>0.103</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201214</th>\n",
       "      <td>0.098</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20210615</th>\n",
       "      <td>0.083</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211209</th>\n",
       "      <td>0.094</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20220613</th>\n",
       "      <td>0.117</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20221206</th>\n",
       "      <td>0.107</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20230606</th>\n",
       "      <td>0.134</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20231201</th>\n",
       "      <td>0.105</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20240604</th>\n",
       "      <td>0.087</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg</th>\n",
       "      <td>0.116</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sum</th>\n",
       "      <td>212.156</td>\n",
       "      <td>169.134</td>\n",
       "      <td>187.039</td>\n",
       "      <td>196.179</td>\n",
       "      <td>159.477</td>\n",
       "      <td>171.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Std</th>\n",
       "      <td>0.077</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>64.206</td>\n",
       "      <td>56.915</td>\n",
       "      <td>63.215</td>\n",
       "      <td>59.365</td>\n",
       "      <td>51.234</td>\n",
       "      <td>55.328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IR</th>\n",
       "      <td>7.351</td>\n",
       "      <td>6.516</td>\n",
       "      <td>7.237</td>\n",
       "      <td>6.797</td>\n",
       "      <td>5.866</td>\n",
       "      <td>6.334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         resnet_gru.0                   resnet_gru.1                  \n",
       "             best     swalast  swabest      best     swalast  swabest \n",
       "20170103      0.165      0.111    0.129      0.147      0.137    0.149\n",
       "20170704      0.142      0.127    0.123      0.125      0.108    0.108\n",
       "20171226      0.132      0.112    0.111      0.107      0.112    0.115\n",
       "20180627      0.150      0.107    0.140      0.150      0.121    0.126\n",
       "20181220      0.117      0.107    0.104      0.123      0.103    0.112\n",
       "20190624      0.094      0.055    0.084      0.095      0.053    0.075\n",
       "20191217      0.105      0.070    0.084      0.106      0.098    0.096\n",
       "20200617      0.103      0.084    0.102      0.102      0.068    0.068\n",
       "20201214      0.098      0.088    0.089      0.093      0.082    0.083\n",
       "20210615      0.083      0.055    0.071      0.088      0.061    0.070\n",
       "20211209      0.094      0.084    0.079      0.083      0.068    0.061\n",
       "20220613      0.117      0.086    0.107      0.088      0.031    0.063\n",
       "20221206      0.107      0.106    0.106      0.106      0.097    0.087\n",
       "20230606      0.134      0.104    0.122      0.106      0.103    0.112\n",
       "20231201      0.105      0.093    0.084      0.098      0.070    0.080\n",
       "20240604      0.087      0.088    0.089      0.071      0.073    0.075\n",
       "Avg           0.116      0.092    0.102      0.107      0.087    0.093\n",
       "Sum         212.156    169.134  187.039    196.179    159.477  171.037\n",
       "Std           0.077      0.069    0.069      0.077      0.073    0.072\n",
       "T            64.206     56.915   63.215     59.365     51.234   55.328\n",
       "IR            7.351      6.516    7.237      6.797      5.866    6.334"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-07-21 23:09:05|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 178.6 Secs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped Return Result:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>model_num</th>\n",
       "      <th colspan=\"3\" halign=\"left\">0</th>\n",
       "      <th colspan=\"3\" halign=\"left\">1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_type</th>\n",
       "      <th>best</th>\n",
       "      <th>swabest</th>\n",
       "      <th>swalast</th>\n",
       "      <th>best</th>\n",
       "      <th>swabest</th>\n",
       "      <th>swalast</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.651%</td>\n",
       "      <td>-1.570%</td>\n",
       "      <td>-1.513%</td>\n",
       "      <td>-1.508%</td>\n",
       "      <td>-1.481%</td>\n",
       "      <td>-1.490%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.400%</td>\n",
       "      <td>-0.381%</td>\n",
       "      <td>-0.437%</td>\n",
       "      <td>-0.318%</td>\n",
       "      <td>-0.399%</td>\n",
       "      <td>-0.352%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.097%</td>\n",
       "      <td>-0.000%</td>\n",
       "      <td>-0.109%</td>\n",
       "      <td>-0.014%</td>\n",
       "      <td>0.037%</td>\n",
       "      <td>-0.042%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.091%</td>\n",
       "      <td>0.121%</td>\n",
       "      <td>0.123%</td>\n",
       "      <td>0.159%</td>\n",
       "      <td>0.166%</td>\n",
       "      <td>0.135%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.201%</td>\n",
       "      <td>0.242%</td>\n",
       "      <td>0.175%</td>\n",
       "      <td>0.381%</td>\n",
       "      <td>0.294%</td>\n",
       "      <td>0.305%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.308%</td>\n",
       "      <td>0.361%</td>\n",
       "      <td>0.303%</td>\n",
       "      <td>0.344%</td>\n",
       "      <td>0.316%</td>\n",
       "      <td>0.352%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.381%</td>\n",
       "      <td>0.335%</td>\n",
       "      <td>0.359%</td>\n",
       "      <td>0.453%</td>\n",
       "      <td>0.400%</td>\n",
       "      <td>0.387%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.453%</td>\n",
       "      <td>0.447%</td>\n",
       "      <td>0.388%</td>\n",
       "      <td>0.513%</td>\n",
       "      <td>0.448%</td>\n",
       "      <td>0.482%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.495%</td>\n",
       "      <td>0.495%</td>\n",
       "      <td>0.459%</td>\n",
       "      <td>0.533%</td>\n",
       "      <td>0.511%</td>\n",
       "      <td>0.466%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.503%</td>\n",
       "      <td>0.517%</td>\n",
       "      <td>0.507%</td>\n",
       "      <td>0.583%</td>\n",
       "      <td>0.547%</td>\n",
       "      <td>0.491%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.633%</td>\n",
       "      <td>0.563%</td>\n",
       "      <td>0.611%</td>\n",
       "      <td>0.649%</td>\n",
       "      <td>0.581%</td>\n",
       "      <td>0.613%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.594%</td>\n",
       "      <td>0.650%</td>\n",
       "      <td>0.618%</td>\n",
       "      <td>0.673%</td>\n",
       "      <td>0.661%</td>\n",
       "      <td>0.664%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.682%</td>\n",
       "      <td>0.676%</td>\n",
       "      <td>0.693%</td>\n",
       "      <td>0.731%</td>\n",
       "      <td>0.662%</td>\n",
       "      <td>0.695%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.742%</td>\n",
       "      <td>0.715%</td>\n",
       "      <td>0.785%</td>\n",
       "      <td>0.751%</td>\n",
       "      <td>0.681%</td>\n",
       "      <td>0.782%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.800%</td>\n",
       "      <td>0.774%</td>\n",
       "      <td>0.856%</td>\n",
       "      <td>0.770%</td>\n",
       "      <td>0.807%</td>\n",
       "      <td>0.805%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.810%</td>\n",
       "      <td>0.817%</td>\n",
       "      <td>0.887%</td>\n",
       "      <td>0.718%</td>\n",
       "      <td>0.820%</td>\n",
       "      <td>0.881%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.949%</td>\n",
       "      <td>0.957%</td>\n",
       "      <td>1.002%</td>\n",
       "      <td>0.764%</td>\n",
       "      <td>0.935%</td>\n",
       "      <td>0.845%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.009%</td>\n",
       "      <td>0.991%</td>\n",
       "      <td>1.025%</td>\n",
       "      <td>0.827%</td>\n",
       "      <td>0.928%</td>\n",
       "      <td>0.898%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.157%</td>\n",
       "      <td>1.095%</td>\n",
       "      <td>1.131%</td>\n",
       "      <td>0.912%</td>\n",
       "      <td>1.023%</td>\n",
       "      <td>1.020%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.283%</td>\n",
       "      <td>1.178%</td>\n",
       "      <td>1.165%</td>\n",
       "      <td>1.060%</td>\n",
       "      <td>1.053%</td>\n",
       "      <td>1.066%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model_num     0                          1                      \n",
       "model_type   best   swabest  swalast    best   swabest  swalast \n",
       "group                                                           \n",
       "1           -1.651%  -1.570%  -1.513%  -1.508%  -1.481%  -1.490%\n",
       "2           -0.400%  -0.381%  -0.437%  -0.318%  -0.399%  -0.352%\n",
       "3           -0.097%  -0.000%  -0.109%  -0.014%   0.037%  -0.042%\n",
       "4            0.091%   0.121%   0.123%   0.159%   0.166%   0.135%\n",
       "5            0.201%   0.242%   0.175%   0.381%   0.294%   0.305%\n",
       "6            0.308%   0.361%   0.303%   0.344%   0.316%   0.352%\n",
       "7            0.381%   0.335%   0.359%   0.453%   0.400%   0.387%\n",
       "8            0.453%   0.447%   0.388%   0.513%   0.448%   0.482%\n",
       "9            0.495%   0.495%   0.459%   0.533%   0.511%   0.466%\n",
       "10           0.503%   0.517%   0.507%   0.583%   0.547%   0.491%\n",
       "11           0.633%   0.563%   0.611%   0.649%   0.581%   0.613%\n",
       "12           0.594%   0.650%   0.618%   0.673%   0.661%   0.664%\n",
       "13           0.682%   0.676%   0.693%   0.731%   0.662%   0.695%\n",
       "14           0.742%   0.715%   0.785%   0.751%   0.681%   0.782%\n",
       "15           0.800%   0.774%   0.856%   0.770%   0.807%   0.805%\n",
       "16           0.810%   0.817%   0.887%   0.718%   0.820%   0.881%\n",
       "17           0.949%   0.957%   1.002%   0.764%   0.935%   0.845%\n",
       "18           1.009%   0.991%   1.025%   0.827%   0.928%   0.898%\n",
       "19           1.157%   1.095%   1.131%   0.912%   1.023%   1.020%\n",
       "20           1.283%   1.178%   1.165%   1.060%   1.053%   1.066%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[44m24-07-21 23:09:35|MOD:test        |\u001b[0m: \u001b[1m\u001b[34mPerforming Factor and FMP test!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PerfManager calc Finished!\n",
      "PerfManager plot Finished!\n",
      "Group optimization of 3 alphas , 3 benchmarks , 2 lags , 367 dates , (6606 opts) start!\n",
      "Done Optimize    0th [best.csi300      ] at 20170104 , time cost (ms) : {'parse_input': 18.19, 'solve': 80.04, 'output': 16.79}\n",
      "Done Optimize   50th [swalast.csi1000  ] at 20170118 , time cost (ms) : {'parse_input': 18.93, 'solve': 105.35, 'output': 20.28}\n",
      "Done Optimize  100th [swabest.csi500.1 ] at 20170215 , time cost (ms) : {'parse_input': 19.19, 'solve': 128.92, 'output': 20.14}\n",
      "Done Optimize  150th [swabest.csi300   ] at 20170308 , time cost (ms) : {'parse_input': 19.06, 'solve': 128.03, 'output': 19.79}\n",
      "Done Optimize  200th [best.csi1000     ] at 20170329 , time cost (ms) : {'parse_input': 19.13, 'solve': 122.59, 'output': 20.68}\n",
      "Done Optimize  250th [swalast.csi500.1 ] at 20170414 , time cost (ms) : {'parse_input': 19.18, 'solve': 117.1, 'output': 20.3}\n",
      "Done Optimize  300th [swalast.csi300   ] at 20170508 , time cost (ms) : {'parse_input': 19.07, 'solve': 132.6, 'output': 19.55}\n",
      "Done Optimize  350th [swabest.csi1000  ] at 20170531 , time cost (ms) : {'parse_input': 19.0, 'solve': 121.21, 'output': 20.12}\n",
      "Done Optimize  400th [best.csi500.1    ] at 20170621 , time cost (ms) : {'parse_input': 19.26, 'solve': 108.98, 'output': 20.0}\n",
      "Done Optimize  450th [best.csi300      ] at 20170712 , time cost (ms) : {'parse_input': 19.84, 'solve': 125.87, 'output': 20.27}\n",
      "Done Optimize  500th [swalast.csi1000  ] at 20170726 , time cost (ms) : {'parse_input': 19.33, 'solve': 121.18, 'output': 20.32}\n",
      "Done Optimize  550th [swabest.csi500.1 ] at 20170816 , time cost (ms) : {'parse_input': 19.35, 'solve': 108.26, 'output': 19.97}\n",
      "Done Optimize  600th [swabest.csi300   ] at 20170906 , time cost (ms) : {'parse_input': 19.51, 'solve': 125.87, 'output': 20.02}\n",
      "Done Optimize  650th [best.csi1000     ] at 20170927 , time cost (ms) : {'parse_input': 19.29, 'solve': 136.5, 'output': 20.42}\n",
      "Done Optimize  700th [swalast.csi500.1 ] at 20171018 , time cost (ms) : {'parse_input': 19.65, 'solve': 129.73, 'output': 20.15}\n",
      "Done Optimize  750th [swalast.csi300   ] at 20171108 , time cost (ms) : {'parse_input': 19.56, 'solve': 138.18, 'output': 19.8}\n",
      "Done Optimize  800th [swabest.csi1000  ] at 20171129 , time cost (ms) : {'parse_input': 19.89, 'solve': 145.07, 'output': 20.63}\n",
      "Done Optimize  850th [best.csi500.1    ] at 20171220 , time cost (ms) : {'parse_input': 19.72, 'solve': 116.62, 'output': 20.17}\n",
      "Done Optimize  900th [best.csi300      ] at 20180111 , time cost (ms) : {'parse_input': 20.26, 'solve': 143.66, 'output': 20.18}\n",
      "Done Optimize  950th [swalast.csi1000  ] at 20180125 , time cost (ms) : {'parse_input': 19.92, 'solve': 136.4, 'output': 20.37}\n",
      "Done Optimize 1000th [swabest.csi500.1 ] at 20180222 , time cost (ms) : {'parse_input': 19.73, 'solve': 138.34, 'output': 20.46}\n",
      "Done Optimize 1050th [swabest.csi300   ] at 20180315 , time cost (ms) : {'parse_input': 19.92, 'solve': 122.59, 'output': 19.56}\n",
      "Done Optimize 1100th [best.csi1000     ] at 20180409 , time cost (ms) : {'parse_input': 19.59, 'solve': 131.09, 'output': 20.78}\n",
      "Done Optimize 1150th [swalast.csi500.1 ] at 20180423 , time cost (ms) : {'parse_input': 19.85, 'solve': 125.46, 'output': 20.11}\n",
      "Done Optimize 1200th [swalast.csi300   ] at 20180516 , time cost (ms) : {'parse_input': 19.93, 'solve': 119.11, 'output': 19.96}\n",
      "Done Optimize 1250th [swabest.csi1000  ] at 20180606 , time cost (ms) : {'parse_input': 19.95, 'solve': 128.49, 'output': 20.36}\n",
      "Done Optimize 1300th [best.csi500.1    ] at 20180628 , time cost (ms) : {'parse_input': 20.31, 'solve': 124.64, 'output': 20.04}\n",
      "Done Optimize 1350th [best.csi300      ] at 20180719 , time cost (ms) : {'parse_input': 20.27, 'solve': 137.09, 'output': 19.66}\n",
      "Done Optimize 1400th [swalast.csi1000  ] at 20180802 , time cost (ms) : {'parse_input': 20.09, 'solve': 134.7, 'output': 21.02}\n",
      "Done Optimize 1450th [swabest.csi500.1 ] at 20180823 , time cost (ms) : {'parse_input': 20.39, 'solve': 143.6, 'output': 19.99}\n",
      "Done Optimize 1500th [swabest.csi300   ] at 20180913 , time cost (ms) : {'parse_input': 20.06, 'solve': 152.71, 'output': 19.77}\n",
      "Done Optimize 1550th [best.csi1000     ] at 20181012 , time cost (ms) : {'parse_input': 20.86, 'solve': 132.72, 'output': 20.62}\n",
      "Done Optimize 1600th [swalast.csi500.1 ] at 20181026 , time cost (ms) : {'parse_input': 23.19, 'solve': 148.11, 'output': 20.47}\n",
      "Done Optimize 1650th [swalast.csi300   ] at 20181116 , time cost (ms) : {'parse_input': 20.46, 'solve': 133.44, 'output': 20.63}\n",
      "Done Optimize 1700th [swabest.csi1000  ] at 20181207 , time cost (ms) : {'parse_input': 20.7, 'solve': 149.46, 'output': 21.19}\n",
      "Done Optimize 1750th [best.csi500.1    ] at 20181228 , time cost (ms) : {'parse_input': 21.07, 'solve': 153.62, 'output': 20.58}\n",
      "Done Optimize 1800th [best.csi300      ] at 20190122 , time cost (ms) : {'parse_input': 22.04, 'solve': 147.04, 'output': 20.71}\n",
      "Done Optimize 1850th [swalast.csi1000  ] at 20190212 , time cost (ms) : {'parse_input': 20.66, 'solve': 142.29, 'output': 20.71}\n",
      "Done Optimize 1900th [swabest.csi500.1 ] at 20190305 , time cost (ms) : {'parse_input': 21.04, 'solve': 136.41, 'output': 20.48}\n",
      "Done Optimize 1950th [swabest.csi300   ] at 20190326 , time cost (ms) : {'parse_input': 21.0, 'solve': 154.08, 'output': 20.12}\n",
      "Done Optimize 2000th [best.csi1000     ] at 20190417 , time cost (ms) : {'parse_input': 49.25, 'solve': 175.49, 'output': 42.6}\n",
      "Done Optimize 2050th [swalast.csi500.1 ] at 20190506 , time cost (ms) : {'parse_input': 23.22, 'solve': 140.26, 'output': 20.32}\n",
      "Done Optimize 2100th [swalast.csi300   ] at 20190527 , time cost (ms) : {'parse_input': 20.55, 'solve': 147.95, 'output': 20.06}\n",
      "Done Optimize 2150th [swabest.csi1000  ] at 20190618 , time cost (ms) : {'parse_input': 21.21, 'solve': 135.74, 'output': 20.96}\n",
      "Done Optimize 2200th [best.csi500.1    ] at 20190709 , time cost (ms) : {'parse_input': 20.53, 'solve': 140.98, 'output': 20.68}\n",
      "Done Optimize 2250th [best.csi300      ] at 20190730 , time cost (ms) : {'parse_input': 21.78, 'solve': 138.52, 'output': 20.41}\n",
      "Done Optimize 2300th [swalast.csi1000  ] at 20190813 , time cost (ms) : {'parse_input': 21.09, 'solve': 140.49, 'output': 20.91}\n",
      "Done Optimize 2350th [swabest.csi500.1 ] at 20190903 , time cost (ms) : {'parse_input': 21.39, 'solve': 135.54, 'output': 20.34}\n",
      "Done Optimize 2400th [swabest.csi300   ] at 20190925 , time cost (ms) : {'parse_input': 20.97, 'solve': 141.81, 'output': 20.08}\n",
      "Done Optimize 2450th [best.csi1000     ] at 20191023 , time cost (ms) : {'parse_input': 21.06, 'solve': 139.95, 'output': 21.42}\n",
      "Failed optimization at 20191030, even with relax, use w0 instead.\n",
      "Done Optimize 2500th [swalast.csi500.1 ] at 20191106 , time cost (ms) : {'parse_input': 21.38, 'solve': 140.39, 'output': 20.49}\n",
      "Done Optimize 2550th [swalast.csi300   ] at 20191127 , time cost (ms) : {'parse_input': 21.27, 'solve': 142.27, 'output': 22.46}\n",
      "Done Optimize 2600th [swabest.csi1000  ] at 20191218 , time cost (ms) : {'parse_input': 21.48, 'solve': 147.25, 'output': 20.92}\n",
      "Done Optimize 2650th [best.csi500.1    ] at 20200109 , time cost (ms) : {'parse_input': 21.14, 'solve': 159.63, 'output': 20.76}\n",
      "Done Optimize 2700th [best.csi300      ] at 20200207 , time cost (ms) : {'parse_input': 22.06, 'solve': 133.83, 'output': 20.17}\n",
      "Done Optimize 2750th [swalast.csi1000  ] at 20200221 , time cost (ms) : {'parse_input': 21.29, 'solve': 134.97, 'output': 20.46}\n",
      "Done Optimize 2800th [swabest.csi500.1 ] at 20200313 , time cost (ms) : {'parse_input': 21.35, 'solve': 157.55, 'output': 20.9}\n",
      "Done Optimize 2850th [swabest.csi300   ] at 20200403 , time cost (ms) : {'parse_input': 21.35, 'solve': 153.72, 'output': 20.55}\n",
      "Done Optimize 2900th [best.csi1000     ] at 20200427 , time cost (ms) : {'parse_input': 21.51, 'solve': 161.71, 'output': 21.36}\n",
      "Done Optimize 2950th [swalast.csi500.1 ] at 20200514 , time cost (ms) : {'parse_input': 21.31, 'solve': 147.24, 'output': 20.7}\n",
      "Done Optimize 3000th [swalast.csi300   ] at 20200604 , time cost (ms) : {'parse_input': 21.31, 'solve': 154.25, 'output': 20.37}\n",
      "Done Optimize 3050th [swabest.csi1000  ] at 20200629 , time cost (ms) : {'parse_input': 21.34, 'solve': 155.93, 'output': 20.96}\n",
      "Done Optimize 3100th [best.csi500.1    ] at 20200720 , time cost (ms) : {'parse_input': 21.31, 'solve': 142.6, 'output': 21.21}\n",
      "Done Optimize 3150th [best.csi300      ] at 20200810 , time cost (ms) : {'parse_input': 21.44, 'solve': 147.2, 'output': 20.72}\n",
      "Done Optimize 3200th [swalast.csi1000  ] at 20200824 , time cost (ms) : {'parse_input': 21.25, 'solve': 161.85, 'output': 20.99}\n",
      "Done Optimize 3250th [swabest.csi500.1 ] at 20200914 , time cost (ms) : {'parse_input': 21.83, 'solve': 147.89, 'output': 20.85}\n",
      "Done Optimize 3300th [swabest.csi300   ] at 20201013 , time cost (ms) : {'parse_input': 21.64, 'solve': 150.15, 'output': 20.43}\n",
      "Done Optimize 3350th [best.csi1000     ] at 20201103 , time cost (ms) : {'parse_input': 21.8, 'solve': 162.44, 'output': 21.68}\n",
      "Done Optimize 3400th [swalast.csi500.1 ] at 20201117 , time cost (ms) : {'parse_input': 22.16, 'solve': 153.88, 'output': 20.75}\n",
      "Done Optimize 3450th [swalast.csi300   ] at 20201208 , time cost (ms) : {'parse_input': 21.6, 'solve': 172.46, 'output': 20.26}\n",
      "Done Optimize 3500th [swabest.csi1000  ] at 20201229 , time cost (ms) : {'parse_input': 21.36, 'solve': 155.84, 'output': 21.32}\n",
      "Done Optimize 3550th [best.csi500.1    ] at 20210120 , time cost (ms) : {'parse_input': 21.52, 'solve': 156.82, 'output': 21.09}\n",
      "Done Optimize 3600th [best.csi300      ] at 20210210 , time cost (ms) : {'parse_input': 21.48, 'solve': 150.92, 'output': 20.66}\n",
      "Done Optimize 3650th [swalast.csi1000  ] at 20210303 , time cost (ms) : {'parse_input': 22.12, 'solve': 175.19, 'output': 20.8}\n",
      "Done Optimize 3700th [swabest.csi500.1 ] at 20210324 , time cost (ms) : {'parse_input': 21.46, 'solve': 149.8, 'output': 20.7}\n",
      "Done Optimize 3750th [swabest.csi300   ] at 20210415 , time cost (ms) : {'parse_input': 21.97, 'solve': 149.24, 'output': 20.43}\n",
      "Done Optimize 3800th [best.csi1000     ] at 20210511 , time cost (ms) : {'parse_input': 21.35, 'solve': 164.73, 'output': 21.38}\n",
      "Done Optimize 3850th [swalast.csi500.1 ] at 20210525 , time cost (ms) : {'parse_input': 21.58, 'solve': 153.9, 'output': 20.92}\n",
      "Done Optimize 3900th [swalast.csi300   ] at 20210616 , time cost (ms) : {'parse_input': 22.28, 'solve': 171.55, 'output': 20.57}\n",
      "Done Optimize 3950th [swabest.csi1000  ] at 20210707 , time cost (ms) : {'parse_input': 21.68, 'solve': 157.11, 'output': 21.07}\n",
      "Done Optimize 4000th [best.csi500.1    ] at 20210728 , time cost (ms) : {'parse_input': 22.14, 'solve': 152.24, 'output': 20.81}\n",
      "Done Optimize 4050th [best.csi300      ] at 20210818 , time cost (ms) : {'parse_input': 22.79, 'solve': 147.48, 'output': 20.54}\n",
      "Done Optimize 4100th [swalast.csi1000  ] at 20210901 , time cost (ms) : {'parse_input': 22.33, 'solve': 168.18, 'output': 21.38}\n",
      "Done Optimize 4150th [swabest.csi500.1 ] at 20210924 , time cost (ms) : {'parse_input': 22.71, 'solve': 165.63, 'output': 22.21}\n",
      "Done Optimize 4200th [swabest.csi300   ] at 20211022 , time cost (ms) : {'parse_input': 21.7, 'solve': 177.52, 'output': 20.52}\n",
      "Done Optimize 4250th [best.csi1000     ] at 20211112 , time cost (ms) : {'parse_input': 22.32, 'solve': 169.49, 'output': 21.34}\n",
      "Done Optimize 4300th [swalast.csi500.1 ] at 20211126 , time cost (ms) : {'parse_input': 22.55, 'solve': 160.4, 'output': 21.02}\n",
      "Done Optimize 4350th [swalast.csi300   ] at 20211217 , time cost (ms) : {'parse_input': 22.53, 'solve': 170.04, 'output': 20.42}\n",
      "Done Optimize 4400th [swabest.csi1000  ] at 20220110 , time cost (ms) : {'parse_input': 22.82, 'solve': 151.3, 'output': 21.0}\n",
      "Done Optimize 4450th [best.csi500.1    ] at 20220207 , time cost (ms) : {'parse_input': 22.77, 'solve': 176.8, 'output': 20.73}\n",
      "Done Optimize 4500th [best.csi300      ] at 20220228 , time cost (ms) : {'parse_input': 22.95, 'solve': 167.22, 'output': 20.88}\n",
      "Done Optimize 4550th [swalast.csi1000  ] at 20220314 , time cost (ms) : {'parse_input': 23.19, 'solve': 178.02, 'output': 21.39}\n",
      "Done Optimize 4600th [swabest.csi500.1 ] at 20220406 , time cost (ms) : {'parse_input': 23.2, 'solve': 173.95, 'output': 21.11}\n",
      "Done Optimize 4650th [swabest.csi300   ] at 20220427 , time cost (ms) : {'parse_input': 23.43, 'solve': 165.81, 'output': 20.7}\n",
      "Done Optimize 4700th [best.csi1000     ] at 20220523 , time cost (ms) : {'parse_input': 23.48, 'solve': 170.53, 'output': 21.68}\n",
      "Done Optimize 4750th [swalast.csi500.1 ] at 20220607 , time cost (ms) : {'parse_input': 22.83, 'solve': 160.43, 'output': 20.96}\n",
      "Done Optimize 4800th [swalast.csi300   ] at 20220628 , time cost (ms) : {'parse_input': 22.98, 'solve': 186.06, 'output': 20.65}\n",
      "Done Optimize 4850th [swabest.csi1000  ] at 20220719 , time cost (ms) : {'parse_input': 22.94, 'solve': 184.96, 'output': 21.42}\n",
      "Done Optimize 4900th [best.csi500.1    ] at 20220809 , time cost (ms) : {'parse_input': 23.28, 'solve': 199.01, 'output': 22.52}\n",
      "Done Optimize 4950th [best.csi300      ] at 20220830 , time cost (ms) : {'parse_input': 23.61, 'solve': 203.86, 'output': 20.71}\n",
      "Done Optimize 5000th [swalast.csi1000  ] at 20220914 , time cost (ms) : {'parse_input': 23.22, 'solve': 171.22, 'output': 21.71}\n",
      "Done Optimize 5050th [swabest.csi500.1 ] at 20221012 , time cost (ms) : {'parse_input': 23.92, 'solve': 190.17, 'output': 22.79}\n",
      "Done Optimize 5100th [swabest.csi300   ] at 20221102 , time cost (ms) : {'parse_input': 23.19, 'solve': 197.13, 'output': 21.05}\n",
      "Done Optimize 5150th [best.csi1000     ] at 20221123 , time cost (ms) : {'parse_input': 23.55, 'solve': 177.36, 'output': 21.68}\n",
      "Done Optimize 5200th [swalast.csi500.1 ] at 20221207 , time cost (ms) : {'parse_input': 23.55, 'solve': 171.57, 'output': 21.26}\n",
      "Done Optimize 5250th [swalast.csi300   ] at 20221228 , time cost (ms) : {'parse_input': 23.68, 'solve': 167.58, 'output': 20.92}\n",
      "Done Optimize 5300th [swabest.csi1000  ] at 20230119 , time cost (ms) : {'parse_input': 24.36, 'solve': 174.11, 'output': 21.53}\n",
      "Done Optimize 5350th [best.csi500.1    ] at 20230216 , time cost (ms) : {'parse_input': 23.42, 'solve': 192.05, 'output': 21.28}\n",
      "Done Optimize 5400th [best.csi300      ] at 20230309 , time cost (ms) : {'parse_input': 24.08, 'solve': 183.37, 'output': 21.21}\n",
      "Failed optimization at 20230323, even with relax, use w0 instead.\n",
      "Done Optimize 5450th [swalast.csi1000  ] at 20230323 , time cost (ms) : {'parse_input': 24.23, 'solve': 174.02, 'output': 21.22}\n",
      "Done Optimize 5500th [swabest.csi500.1 ] at 20230414 , time cost (ms) : {'parse_input': 23.75, 'solve': 172.87, 'output': 21.19}\n",
      "Done Optimize 5550th [swabest.csi300   ] at 20230510 , time cost (ms) : {'parse_input': 27.81, 'solve': 182.44, 'output': 21.12}\n",
      "Done Optimize 5600th [best.csi1000     ] at 20230531 , time cost (ms) : {'parse_input': 24.04, 'solve': 184.45, 'output': 25.45}\n",
      "Done Optimize 5650th [swalast.csi500.1 ] at 20230614 , time cost (ms) : {'parse_input': 24.04, 'solve': 184.52, 'output': 21.3}\n",
      "Done Optimize 5700th [swalast.csi300   ] at 20230707 , time cost (ms) : {'parse_input': 24.39, 'solve': 175.88, 'output': 21.02}\n",
      "Done Optimize 5750th [swabest.csi1000  ] at 20230728 , time cost (ms) : {'parse_input': 23.56, 'solve': 196.44, 'output': 23.17}\n",
      "Done Optimize 5800th [best.csi500.1    ] at 20230818 , time cost (ms) : {'parse_input': 23.77, 'solve': 197.91, 'output': 21.4}\n",
      "Done Optimize 5850th [best.csi300      ] at 20230908 , time cost (ms) : {'parse_input': 26.65, 'solve': 180.47, 'output': 21.22}\n",
      "Done Optimize 5900th [swalast.csi1000  ] at 20230922 , time cost (ms) : {'parse_input': 24.55, 'solve': 189.91, 'output': 21.7}\n",
      "Done Optimize 5950th [swabest.csi500.1 ] at 20231023 , time cost (ms) : {'parse_input': 24.26, 'solve': 184.31, 'output': 21.6}\n",
      "Done Optimize 6000th [swabest.csi300   ] at 20231113 , time cost (ms) : {'parse_input': 27.54, 'solve': 181.04, 'output': 21.32}\n",
      "Done Optimize 6050th [best.csi1000     ] at 20231204 , time cost (ms) : {'parse_input': 24.08, 'solve': 202.62, 'output': 25.99}\n",
      "Done Optimize 6100th [swalast.csi500.1 ] at 20231218 , time cost (ms) : {'parse_input': 24.19, 'solve': 221.11, 'output': 21.9}\n",
      "Done Optimize 6150th [swalast.csi300   ] at 20240109 , time cost (ms) : {'parse_input': 24.17, 'solve': 188.53, 'output': 21.04}\n",
      "Done Optimize 6200th [swabest.csi1000  ] at 20240130 , time cost (ms) : {'parse_input': 24.51, 'solve': 175.89, 'output': 22.17}\n",
      "Done Optimize 6250th [best.csi500.1    ] at 20240228 , time cost (ms) : {'parse_input': 24.61, 'solve': 212.27, 'output': 22.42}\n",
      "Done Optimize 6300th [best.csi300      ] at 20240320 , time cost (ms) : {'parse_input': 25.11, 'solve': 208.26, 'output': 20.89}\n",
      "Done Optimize 6350th [swalast.csi1000  ] at 20240403 , time cost (ms) : {'parse_input': 24.39, 'solve': 180.77, 'output': 22.11}\n",
      "Done Optimize 6400th [swabest.csi500.1 ] at 20240426 , time cost (ms) : {'parse_input': 24.35, 'solve': 189.7, 'output': 21.55}\n",
      "Done Optimize 6450th [swabest.csi300   ] at 20240522 , time cost (ms) : {'parse_input': 24.73, 'solve': 180.1, 'output': 21.4}\n",
      "Done Optimize 6500th [best.csi1000     ] at 20240613 , time cost (ms) : {'parse_input': 23.85, 'solve': 183.15, 'output': 21.53}\n",
      "Done Optimize 6550th [swalast.csi500.1 ] at 20240627 , time cost (ms) : {'parse_input': 24.61, 'solve': 209.42, 'output': 21.98}\n",
      "Done Optimize 6600th [swalast.csi300   ] at 20240718 , time cost (ms) : {'parse_input': 24.37, 'solve': 187.03, 'output': 21.26}\n",
      "Group optimization Finished , Total time: 1469.21 secs, Setup time: 100.73 secs, Calc time: 1368.48 secs, Each optim time: 0.21\n",
      "FmpManager calc Finished!\n",
      "FmpManager plot Finished!\n",
      "FMP test Result:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>factor_name</th>\n",
       "      <th>benchmark</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>pf</th>\n",
       "      <th>bm</th>\n",
       "      <th>excess</th>\n",
       "      <th>annualized</th>\n",
       "      <th>mdd</th>\n",
       "      <th>te</th>\n",
       "      <th>ir</th>\n",
       "      <th>calmar</th>\n",
       "      <th>turnover</th>\n",
       "      <th>mdd_period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>best</td>\n",
       "      <td>csi300</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240718</td>\n",
       "      <td>133.96%</td>\n",
       "      <td>24.21%</td>\n",
       "      <td>109.75%</td>\n",
       "      <td>8.68%</td>\n",
       "      <td>4.83%</td>\n",
       "      <td>4.02%</td>\n",
       "      <td>2.158</td>\n",
       "      <td>1.799</td>\n",
       "      <td>146.760</td>\n",
       "      <td>20210608-20210804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>best</td>\n",
       "      <td>csi500</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240718</td>\n",
       "      <td>173.53%</td>\n",
       "      <td>-15.40%</td>\n",
       "      <td>188.93%</td>\n",
       "      <td>16.65%</td>\n",
       "      <td>9.30%</td>\n",
       "      <td>6.47%</td>\n",
       "      <td>2.574</td>\n",
       "      <td>1.791</td>\n",
       "      <td>147.269</td>\n",
       "      <td>20210601-20210915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best</td>\n",
       "      <td>csi1000</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240718</td>\n",
       "      <td>195.42%</td>\n",
       "      <td>-40.37%</td>\n",
       "      <td>235.79%</td>\n",
       "      <td>23.13%</td>\n",
       "      <td>11.01%</td>\n",
       "      <td>6.06%</td>\n",
       "      <td>3.814</td>\n",
       "      <td>2.100</td>\n",
       "      <td>147.359</td>\n",
       "      <td>20210429-20210915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>swabest</td>\n",
       "      <td>csi300</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240718</td>\n",
       "      <td>108.05%</td>\n",
       "      <td>24.21%</td>\n",
       "      <td>83.84%</td>\n",
       "      <td>7.06%</td>\n",
       "      <td>5.66%</td>\n",
       "      <td>3.60%</td>\n",
       "      <td>1.959</td>\n",
       "      <td>1.248</td>\n",
       "      <td>147.103</td>\n",
       "      <td>20201124-20210113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>swabest</td>\n",
       "      <td>csi500</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240718</td>\n",
       "      <td>122.56%</td>\n",
       "      <td>-15.40%</td>\n",
       "      <td>137.96%</td>\n",
       "      <td>13.73%</td>\n",
       "      <td>9.48%</td>\n",
       "      <td>6.41%</td>\n",
       "      <td>2.141</td>\n",
       "      <td>1.447</td>\n",
       "      <td>147.246</td>\n",
       "      <td>20231225-20240206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>swabest</td>\n",
       "      <td>csi1000</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240718</td>\n",
       "      <td>114.83%</td>\n",
       "      <td>-40.37%</td>\n",
       "      <td>155.20%</td>\n",
       "      <td>18.32%</td>\n",
       "      <td>8.98%</td>\n",
       "      <td>6.05%</td>\n",
       "      <td>3.027</td>\n",
       "      <td>2.040</td>\n",
       "      <td>147.334</td>\n",
       "      <td>20210608-20210915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>swalast</td>\n",
       "      <td>csi300</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240718</td>\n",
       "      <td>100.15%</td>\n",
       "      <td>24.21%</td>\n",
       "      <td>75.94%</td>\n",
       "      <td>6.55%</td>\n",
       "      <td>7.25%</td>\n",
       "      <td>3.49%</td>\n",
       "      <td>1.877</td>\n",
       "      <td>0.903</td>\n",
       "      <td>147.106</td>\n",
       "      <td>20221109-20240206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>swalast</td>\n",
       "      <td>csi500</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240718</td>\n",
       "      <td>163.46%</td>\n",
       "      <td>-15.40%</td>\n",
       "      <td>178.86%</td>\n",
       "      <td>16.36%</td>\n",
       "      <td>8.09%</td>\n",
       "      <td>6.23%</td>\n",
       "      <td>2.626</td>\n",
       "      <td>2.023</td>\n",
       "      <td>147.198</td>\n",
       "      <td>20231120-20240206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>swalast</td>\n",
       "      <td>csi1000</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240718</td>\n",
       "      <td>150.44%</td>\n",
       "      <td>-40.37%</td>\n",
       "      <td>190.80%</td>\n",
       "      <td>20.83%</td>\n",
       "      <td>6.91%</td>\n",
       "      <td>5.94%</td>\n",
       "      <td>3.506</td>\n",
       "      <td>3.016</td>\n",
       "      <td>146.881</td>\n",
       "      <td>20210630-20210915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  factor_name benchmark    start      end      pf       bm     excess  annualized   mdd     te     ir   calmar turnover     mdd_period    \n",
       "1       best     csi300  20170104  20240718  133.96%   24.21%  109.75%    8.68%     4.83%  4.02%  2.158  1.799  146.760  20210608-20210804\n",
       "2       best     csi500  20170104  20240718  173.53%  -15.40%  188.93%   16.65%     9.30%  6.47%  2.574  1.791  147.269  20210601-20210915\n",
       "0       best    csi1000  20170104  20240718  195.42%  -40.37%  235.79%   23.13%    11.01%  6.06%  3.814  2.100  147.359  20210429-20210915\n",
       "4    swabest     csi300  20170104  20240718  108.05%   24.21%   83.84%    7.06%     5.66%  3.60%  1.959  1.248  147.103  20201124-20210113\n",
       "5    swabest     csi500  20170104  20240718  122.56%  -15.40%  137.96%   13.73%     9.48%  6.41%  2.141  1.447  147.246  20231225-20240206\n",
       "3    swabest    csi1000  20170104  20240718  114.83%  -40.37%  155.20%   18.32%     8.98%  6.05%  3.027  2.040  147.334  20210608-20210915\n",
       "7    swalast     csi300  20170104  20240718  100.15%   24.21%   75.94%    6.55%     7.25%  3.49%  1.877  0.903  147.106  20221109-20240206\n",
       "8    swalast     csi500  20170104  20240718  163.46%  -15.40%  178.86%   16.36%     8.09%  6.23%  2.626  2.023  147.198  20231120-20240206\n",
       "6    swalast    csi1000  20170104  20240718  150.44%  -40.37%  190.80%   20.83%     6.91%  5.94%  3.506  3.016  146.881  20210630-20210915"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-07-21 23:52:23|MOD:time        |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 46 Minutes 31.9 Seconds\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytic datas are saved to /home/mengkjin/Workspace/learndl/model/resnet_gru_30m.1/detailed_analysis/data.xlsx\n",
      "Analytic plots are saved to /home/mengkjin/Workspace/learndl/model/resnet_gru_30m.1/detailed_analysis/plot.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<src.nn_model.trainer.trainers.net.NetTrainer at 0x7d5bf5eb9bd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.api import Trainer  \n",
    "app = Trainer.initialize(stage = 2 , resume = 0 , checkname= -1)\n",
    "app.go()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device name: NVIDIA GeForce RTX 4090\n",
      "try using /home/mengkjin/Workspace/learndl/data/DataSet/30m.20240703.pt , success!\n",
      "Load  2 DataBlocks...... finished! Cost 1.08 secs\n",
      "Align 2 DataBlocks...... finished! Cost 0.76 secs\n",
      "Pre-Norming method of [30m] : {'divlast': False, 'histnorm': True}\n"
     ]
    }
   ],
   "source": [
    "from src.api import HiddenExtractor\n",
    "extractor = HiddenExtractor('resnet_gru_30m' , model_nums=[1] , model_types=['best'])\n",
    "extractor.extract_hidden('update' , deploy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device name: NVIDIA GeForce RTX 4090\n",
      "predict is False , Data Processing start!\n",
      "6 datas :['y', 'day', '30m', 'style', 'indus', 'week']\n",
      "y blocks loading start!\n",
      " --> labels blocks reading [ret10_lag] DataBase...... finished! Cost 19.78 secs\n",
      " --> labels blocks reading [ret20_lag] DataBase...... finished! Cost 18.38 secs\n",
      " --> labels blocks merging (2)...... finished! Cost 3.39 secs\n",
      " --> models blocks reading [risk_exp] DataBase...... finished! Cost 67.56 secs\n",
      "y blocks loading finished! Cost 120.07 secs\n",
      "y blocks process...... finished! Cost 46.25 secs\n",
      "y blocks masking...... finished! Cost 0.76 secs\n",
      "y blocks saving ...... finished! Cost 3.51 secs\n",
      "y blocks norming...... finished! Cost 0.00 secs\n",
      "y finished! Cost 170.84 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "day blocks loading start!\n",
      " --> trade blocks reading [day] DataBase...... finished! Cost 29.59 secs\n",
      "day blocks loading finished! Cost 29.62 secs\n",
      "day blocks process...... finished! Cost 4.06 secs\n",
      "day blocks masking...... finished! Cost 0.86 secs\n",
      "day blocks saving ...... finished! Cost 3.50 secs\n",
      "day blocks norming...... finished! Cost 8.17 secs\n",
      "day finished! Cost 46.41 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "30m blocks loading start!\n",
      " --> trade blocks reading [30min] DataBase...... finished! Cost 66.96 secs\n",
      " --> trade blocks reading [day] DataBase...... finished! Cost 24.59 secs\n",
      "30m blocks loading finished! Cost 92.25 secs\n",
      "30m blocks process...... finished! Cost 37.03 secs\n",
      "30m blocks masking...... finished! Cost 2.11 secs\n",
      "30m blocks saving ...... finished! Cost 41.98 secs\n",
      "30m blocks norming...... finished! Cost 3.73 secs\n",
      "30m finished! Cost 177.29 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "style blocks loading start!\n",
      " --> models blocks reading [risk_exp] DataBase...... finished! Cost 32.81 secs\n",
      "style blocks loading finished! Cost 32.83 secs\n",
      "style blocks process...... finished! Cost 0.00 secs\n",
      "style blocks masking...... finished! Cost 0.91 secs\n",
      "style blocks saving ...... finished! Cost 5.58 secs\n",
      "style blocks norming...... finished! Cost 0.00 secs\n",
      "style finished! Cost 39.52 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "indus blocks loading start!\n",
      " --> models blocks reading [risk_exp] DataBase...... finished! Cost 59.46 secs\n",
      "indus blocks loading finished! Cost 59.46 secs\n",
      "indus blocks process...... finished! Cost 0.00 secs\n",
      "indus blocks masking...... finished! Cost 2.28 secs\n",
      "indus blocks saving ...... finished! Cost 29.85 secs\n",
      "indus blocks norming...... finished! Cost 0.00 secs\n",
      "indus finished! Cost 91.79 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "week blocks loading start!\n",
      " --> trade blocks reading [day] DataBase...... finished! Cost 25.89 secs\n",
      "week blocks loading finished! Cost 25.92 secs\n",
      "week blocks process...... finished! Cost 28.65 secs\n",
      "week blocks masking...... finished! Cost 2.27 secs\n",
      "week blocks saving ...... finished! Cost 25.41 secs\n",
      "week blocks norming...... finished! Cost 5.53 secs\n",
      "week finished! Cost 87.97 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "Data Processing Finished! Cost 613.81 Seconds\n",
      "predict is True , Data Processing start!\n",
      "6 datas :['y', 'day', '30m', 'style', 'indus', 'week']\n",
      "y blocks loading start!\n",
      " --> labels blocks reading [ret10_lag] DataBase...... finished! Cost 0.69 secs\n",
      " --> labels blocks reading [ret20_lag] DataBase...... finished! Cost 0.63 secs\n",
      " --> labels blocks merging (2)...... finished! Cost 0.10 secs\n",
      " --> models blocks reading [risk_exp] DataBase...... finished! Cost 2.67 secs\n",
      "y blocks loading finished! Cost 4.30 secs\n",
      "y blocks process...... finished! Cost 2.01 secs\n",
      "y blocks masking...... finished! Cost 0.07 secs\n",
      "y blocks saving ...... finished! Cost 0.13 secs\n",
      "y blocks norming...... finished! Cost 0.00 secs\n",
      "y finished! Cost 6.68 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "day blocks loading start!\n",
      " --> trade blocks reading [day] DataBase...... finished! Cost 1.08 secs\n",
      "day blocks loading finished! Cost 1.08 secs\n",
      "day blocks process...... finished! Cost 0.10 secs\n",
      "day blocks masking...... finished! Cost 0.07 secs\n",
      "day blocks saving ...... finished! Cost 0.14 secs\n",
      "day blocks norming...... finished! Cost 0.00 secs\n",
      "day finished! Cost 1.56 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "30m blocks loading start!\n",
      " --> trade blocks reading [30min] DataBase...... finished! Cost 4.71 secs\n",
      " --> trade blocks reading [day] DataBase...... finished! Cost 0.99 secs\n",
      "30m blocks loading finished! Cost 5.75 secs\n",
      "30m blocks process...... finished! Cost 1.50 secs\n",
      "30m blocks masking...... finished! Cost 0.12 secs\n",
      "30m blocks saving ...... finished! Cost 1.39 secs\n",
      "30m blocks norming...... finished! Cost 0.00 secs\n",
      "30m finished! Cost 8.93 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "style blocks loading start!\n",
      " --> models blocks reading [risk_exp] DataBase...... finished! Cost 1.49 secs\n",
      "style blocks loading finished! Cost 1.49 secs\n",
      "style blocks process...... finished! Cost 0.00 secs\n",
      "style blocks masking...... finished! Cost 0.07 secs\n",
      "style blocks saving ...... finished! Cost 0.20 secs\n",
      "style blocks norming...... finished! Cost 0.00 secs\n",
      "style finished! Cost 1.91 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "indus blocks loading start!\n",
      " --> models blocks reading [risk_exp] DataBase...... finished! Cost 2.61 secs\n",
      "indus blocks loading finished! Cost 2.61 secs\n",
      "indus blocks process...... finished! Cost 0.00 secs\n",
      "indus blocks masking...... finished! Cost 0.10 secs\n",
      "indus blocks saving ...... finished! Cost 1.08 secs\n",
      "indus blocks norming...... finished! Cost 0.00 secs\n",
      "indus finished! Cost 3.98 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "week blocks loading start!\n",
      " --> trade blocks reading [day] DataBase...... finished! Cost 2.14 secs\n",
      "week blocks loading finished! Cost 2.15 secs\n",
      "week blocks process...... finished! Cost 1.57 secs\n",
      "week blocks masking...... finished! Cost 0.14 secs\n",
      "week blocks saving ..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[44m24-07-20 01:02:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-07-20 01:02:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Sat Jul 20 01:02:48 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... finished! Cost 1.81 secs\n",
      "week blocks norming...... finished! Cost 0.00 secs\n",
      "week finished! Cost 5.83 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "Data Processing Finished! Cost 28.90 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "--Process Queue : Data + Fit + Test\n",
      "--Confirm Resume Training!\n",
      "--Model_name is set to gru_day!\n",
      "Callback : ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True,kwargs={}) , reset optimizer on some epoch (can speedup scheduler)\n",
      "Callback : CallbackTimer(verbosity=2,kwargs={}) , record time cost of callback hooks\n",
      "Callback : EarlyStoppage(patience=20,kwargs={}) , stop fitting when validation score cease to improve\n",
      "Callback : ValidationConverge(patience=5,eps=1e-05,kwargs={}) , stop fitting when valid_score converge\n",
      "Callback : EarlyExitRetrain(earliest=10,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100, 1],kwargs={}) , retrain with new lr if fitting stopped too early\n",
      "Callback : NanLossRetrain(max_attempt=4,kwargs={}) , retrain if fitting encounters nan loss\n",
      "Callback : BatchDisplay(verbosity=2,kwargs={}) , display batch progress bar\n",
      "Callback : StatusDisplay(verbosity=2,kwargs={}) , display epoch and event information\n",
      "Callback : DetailedAlphaAnalysis(use_num=avg,kwargs={}) , record and concat each model to Alpha model instance\n",
      "Callback : GroupReturnAnalysis(group_num=20,kwargs={}) , record and concat each model to Alpha model instance\n",
      "{'model_name': 'gru_day',\n",
      " 'model_module': 'gru',\n",
      " 'model.types': ['best', 'swalast', 'swabest'],\n",
      " 'model.lgbm_ensembler': False,\n",
      " 'data.type': 'day',\n",
      " 'data.labels': ['std_lag1_10'],\n",
      " 'random_seed': None,\n",
      " 'beg_date': 20170103,\n",
      " 'end_date': 99991231,\n",
      " 'sample_method': 'sequential',\n",
      " 'shuffle_option': 'epoch'}\n",
      "{'hidden_dim': [32, 64],\n",
      " 'seqlens': [{'day': 30, '30m': 30, 'dms': 30}],\n",
      " 'tra_seqlens': [{'hist_loss': 40}],\n",
      " 'dropout': [0.1],\n",
      " 'enc_in': [True],\n",
      " 'enc_att': [False],\n",
      " 'rnn_type': ['lstm'],\n",
      " 'rnn_att': [False],\n",
      " 'rnn_layers': [2],\n",
      " 'dec_mlp_layers': [2],\n",
      " 'num_output': [1],\n",
      " 'kernel_size': [3],\n",
      " 'hidden_as_factor': [False],\n",
      " 'ordered_param_group': [False],\n",
      " 'tra_num_states': [5]}\n",
      "Load  1 DataBlocks...... finished! Cost 0.34 secs\n",
      "Align 1 DataBlocks...... finished! Cost 0.98 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-07-20 01:02:50|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 2.5 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-07-20 01:02:50|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Sat Jul 20 01:02:50 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-20 01:02:50|MOD:mod         |\u001b[0m: \u001b[1m\u001b[34mFirst Iterance: (20170103 , 0)\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-07-20 01:02:50|MOD:time        |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 2.5 Seconds\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  1 DataBlocks...... finished! Cost 0.02 secs\n",
      "Align 1 DataBlocks...... finished! Cost 0.05 secs\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'data.hidden'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataAPI , Trainer\n\u001b[1;32m      2\u001b[0m DataAPI\u001b[38;5;241m.\u001b[39mreconstruct_train_data()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/learndl/src/nn_model/trainer/trainers/api.py:35\u001b[0m, in \u001b[0;36mTrainer.update_models\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m REG_MODELS:\n\u001b[1;32m     34\u001b[0m     config_path \u001b[38;5;241m=\u001b[39m TrainConfig\u001b[38;5;241m.\u001b[39mget_config_path(model\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresume\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/learndl/src/nn_model/trainer/trainers/basic.py:225\u001b[0m, in \u001b[0;36mTrainerModule.go\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgo\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m BigTimer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mcritical , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMain Process\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 225\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Workspace/learndl/src/nn_model/classes/mod.py:329\u001b[0m, in \u001b[0;36mBaseTrainer.main_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_configure_model()\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage_queue: \n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstage_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_summarize_model()\n",
      "File \u001b[0;32m~/Workspace/learndl/src/nn_model/classes/mod.py:346\u001b[0m, in \u001b[0;36mBaseTrainer.stage_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;241m.\u001b[39mfit_iter_num \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFirst Iterance: (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;241m.\u001b[39mmodel_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m , \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;241m.\u001b[39mmodel_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_fit_end()\n",
      "File \u001b[0;32m~/Workspace/learndl/src/nn_model/trainer/trainers/aggregator.py:171\u001b[0m, in \u001b[0;36mAggregatorTrainer.fit_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_fit_model_start\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_idx , \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mtrain_dataloader() , \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mval_dataloader())):\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;241m.\u001b[39mdataset_train()\n",
      "File \u001b[0;32m~/Workspace/learndl/src/nn_model/classes/mod.py:44\u001b[0m, in \u001b[0;36mBaseCB.hook_wrapper.<locals>.wrapper_with\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_with\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mat_enter(hook_name)\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mat_exit(hook_name)\n",
      "File \u001b[0;32m~/Workspace/learndl/src/nn_model/trainer/trainers/aggregator.py:199\u001b[0m, in \u001b[0;36mAggregatorTrainer.on_fit_model_start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_fit_model_start\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_param\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Workspace/learndl/src/nn_model/trainer/trainers/aggregator.py:56\u001b[0m, in \u001b[0;36mAggregatorDataModule.setup\u001b[0;34m(self, stage, param, model_date)\u001b[0m\n\u001b[1;32m     54\u001b[0m hidden_df : pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m|\u001b[39m Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     55\u001b[0m ds_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m stage \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hidden_key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.hidden\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     57\u001b[0m     model_name , model_num , model_type \u001b[38;5;241m=\u001b[39m hidden_key\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     58\u001b[0m     hidden_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(PATH\u001b[38;5;241m.\u001b[39mhidden , model_name , \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.feather\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Workspace/learndl/src/nn_model/util/config.py:245\u001b[0m, in \u001b[0;36mTrainConfig.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m , k): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrain\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/learndl/src/nn_model/util/config.py:78\u001b[0m, in \u001b[0;36mTrainParam.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m , key : \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParam\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'data.hidden'"
     ]
    }
   ],
   "source": [
    "from src.api import DataAPI , Trainer\n",
    "DataAPI.reconstruct_train_data()\n",
    "Trainer.update_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device name: NVIDIA GeForce RTX 4090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:10:43|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:10:43|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Mon Jun 24 01:10:43 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Process Queue : Data + Fit + Test\n",
      "--Confirm Resume Training!\n",
      "--Model_name is set to gru_day!\n",
      "Callback : ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True) , reset optimizer on some epoch (can speedup scheduler)\n",
      "Callback : CallbackTimer(verbosity=2) , record time cost of callback hooks\n",
      "Callback : EarlyStoppage(patience=20) , stop fitting when validation score cease to improve\n",
      "Callback : ValidationConverge(patience=5,eps=1e-05) , stop fitting when valid_score converge\n",
      "Callback : EarlyExitRetrain(earliest=5,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100]) , retrain with new lr if fitting stopped too early\n",
      "Callback : NanLossRetrain(max_attempt=4) , retrain if fitting encounters nan loss\n",
      "Callback : BatchDisplay(verbosity=2) , display batch progress bar\n",
      "Callback : StatusDisplay(verbosity=2) , display epoch and event information\n",
      "{'random_seed': None,\n",
      " 'model_name': 'gruRES_day',\n",
      " 'model_module': 'gru',\n",
      " 'model_data_type': 'day',\n",
      " 'model_types': ['best', 'swalast', 'swabest'],\n",
      " 'labels': ['std_lag1_10'],\n",
      " 'beg_date': 20170103,\n",
      " 'end_date': 99991231,\n",
      " 'sample_method': 'train_shuffle',\n",
      " 'shuffle_option': 'epoch',\n",
      " 'lgbm_ensembler': False}\n",
      "{'hidden_dim': [32, 64],\n",
      " 'seqlens': [{'day': 30, '30m': 30, 'dms': 30}],\n",
      " 'tra_seqlens': [{'hist_loss': 40}],\n",
      " 'dropout': [0.1],\n",
      " 'enc_in': [True],\n",
      " 'enc_att': [False],\n",
      " 'rnn_type': ['lstm'],\n",
      " 'rnn_att': [False],\n",
      " 'rnn_layers': [2],\n",
      " 'dec_mlp_layers': [2],\n",
      " 'num_output': [1],\n",
      " 'kernel_size': [3],\n",
      " 'hidden_as_factor': [False],\n",
      " 'ordered_param_group': [False],\n",
      " 'tra_num_states': [5]}\n",
      "Load  2 DataBlocks...... finished! Cost 2.40 secs\n",
      "Align 2 DataBlocks...... finished! Cost 2.86 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:10:51|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 7.9 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:10:51|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Mon Jun 24 01:10:51 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n",
      "score function of [spearman] calculated and success!\n",
      "loss function of [pearson] calculated and success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mFirstBite Ep#  0 : loss  0.99770, train 0.00391, valid 0.03668, best 0.0367, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.87023, train 0.12798, valid 0.09558, best 0.0956, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.85418, train 0.14255, valid 0.08890, best 0.0956, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.84138, train 0.15293, valid 0.08662, best 0.0956, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.83995, train 0.15505, valid 0.09025, best 0.0956, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.83040, train 0.16270, valid 0.08154, best 0.0956, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.83040, train 0.16270, valid 0.08154, best 0.0956, lr3.1e-04, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  0 : loss  0.99907, train 0.00307, valid-0.00235, best-0.0024, lr1.3e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  5 : loss  0.89798, train 0.10238, valid 0.07134, best 0.0839, lr2.5e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 10 : loss  0.88484, train 0.11407, valid 0.08712, best 0.0892, lr1.9e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 15 : loss  0.87704, train 0.12033, valid 0.08682, best 0.0892, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 20 : loss  0.87598, train 0.12115, valid 0.08665, best 0.0892, lr9.4e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 25 : loss  0.87235, train 0.12489, valid 0.08559, best 0.0892, lr3.1e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 25 : loss  0.87235, train 0.12489, valid 0.08559, best 0.0892, lr3.1e-05, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  0 : loss  1.00802, train-0.01361, valid-0.04341, best-0.0434, lr1.3e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  5 : loss  0.90600, train 0.09749, valid 0.08519, best 0.0985, lr2.5e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 10 : loss  0.88704, train 0.11519, valid 0.10017, best 0.1002, lr1.9e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 15 : loss  0.88873, train 0.11359, valid 0.09794, best 0.1002, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 20 : loss  0.89135, train 0.11055, valid 0.09254, best 0.1002, lr9.4e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 25 : loss  0.88877, train 0.11064, valid 0.09957, best 0.1021, lr3.1e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 30 : loss  0.88839, train 0.10964, valid 0.08501, best 0.1021, lr1.6e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 35 : loss  0.88399, train 0.11418, valid 0.08975, best 0.1021, lr3.1e-03\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 39 , effective at epoch 40\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 40 : loss  0.88123, train 0.11494, valid 0.09168, best 0.1021, lr1.3e-02\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:16:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #1 @20231201|Retrain#2 Ep# 96 EarlyStop|Train 0.1037 Valid 0.0934 BestVal 0.0956|Cost  5.7Min,  3.5Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  1.00213, train-0.00346, valid-0.02870, best-0.0287, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.88101, train 0.11998, valid 0.10566, best 0.1118, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.86625, train 0.13116, valid 0.09471, best 0.1118, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.85764, train 0.13868, valid 0.09772, best 0.1118, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.85723, train 0.13898, valid 0.10404, best 0.1118, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 24 : loss  0.85347, train 0.14217, valid 0.09795, best 0.1118, lr1.6e-04, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  0 : loss  0.99619, train 0.00640, valid 0.02634, best 0.0263, lr1.3e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  5 : loss  0.91904, train 0.07677, valid 0.07904, best 0.0790, lr2.5e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 10 : loss  0.90607, train 0.09211, valid 0.09041, best 0.0904, lr1.9e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 15 : loss  0.89860, train 0.10379, valid 0.09553, best 0.0956, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 20 : loss  0.89763, train 0.10523, valid 0.09337, best 0.0956, lr9.4e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 24 : loss  0.89522, train 0.10913, valid 0.09689, best 0.0970, lr1.6e-05, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  0 : loss  1.00127, train-0.00532, valid-0.02776, best-0.0278, lr1.3e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  5 : loss  0.90841, train 0.09496, valid 0.09364, best 0.1105, lr2.5e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 10 : loss  0.88931, train 0.11243, valid 0.09934, best 0.1105, lr1.9e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 15 : loss  0.88411, train 0.11520, valid 0.09632, best 0.1105, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 20 : loss  0.88303, train 0.11682, valid 0.10441, best 0.1105, lr9.4e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 24 : loss  0.87640, train 0.12191, valid 0.10175, best 0.1105, lr1.6e-03, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  0 : loss  0.99942, train 0.00330, valid 0.03781, best 0.0378, lr1.3e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  5 : loss  0.96270, train 0.04376, valid 0.07316, best 0.0732, lr2.5e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 10 : loss  0.95829, train 0.04798, valid 0.07296, best 0.0736, lr1.9e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 15 : loss  0.95190, train 0.05224, valid 0.07273, best 0.0736, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 20 : loss  0.95024, train 0.05357, valid 0.07160, best 0.0736, lr9.5e-06\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 24 : loss  0.94829, train 0.05503, valid 0.07283, best 0.0736, lr1.7e-06, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  0 : loss  0.99623, train 0.00749, valid 0.03126, best 0.0313, lr1.3e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  5 : loss  0.97201, train 0.03669, valid 0.04230, best 0.0540, lr2.5e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 10 : loss  0.97588, train 0.03329, valid 0.03690, best 0.0540, lr1.9e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 15 : loss  0.97586, train 0.03358, valid 0.04005, best 0.0540, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 20 : loss  0.97032, train 0.04237, valid 0.04644, best 0.0540, lr9.4e-02\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:22:42|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #0 @20240604|Retrain#4 Ep#125 EarlyStop|Train 0.0369 Valid 0.0350 BestVal 0.1118|Cost  6.0Min,  2.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:24:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #1 @20240604|FirstBite Ep# 34 EarlyStop|Train 0.1667 Valid 0.0877 BestVal 0.0877|Cost  2.1Min,  3.6Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:24:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Fit], Cost 0.2 Hours, 4.7 Min/model, 3.3 Sec/Epoch\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:24:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Mon Jun 24 01:24:48 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:24:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mEach Model Date Testing Mean Score(spearman):\u001b[0m\n",
      "\u001b[32mModels            0       0       0       1       1       1\u001b[0m\n",
      "\u001b[32mOutput         best swalast swabest    best swalast swabest\u001b[0m\n",
      "\u001b[32m20170103     0.1536  0.1525  0.1520  0.1525  0.1521  0.1520\u001b[0m\n",
      "\u001b[32m20170704     0.1373  0.1327  0.1365  0.1413  0.1373  0.1405\u001b[0m\n",
      "\u001b[32m20171226     0.1305  0.1323  0.1329  0.1291  0.1298  0.1300\u001b[0m\n",
      "\u001b[32m20180627     0.1225  0.1186  0.1223  0.1226  0.1210  0.1236\u001b[0m\n",
      "\u001b[32m20181220     0.0998  0.0990  0.1004  0.0958  0.0981  0.0987\u001b[0m\n",
      "\u001b[32m20190624     0.0970  0.0955  0.0972  0.0920  0.0908  0.0924\u001b[0m\n",
      "\u001b[32m20191217     0.1011  0.0991  0.1005  0.1042  0.1048  0.1047\u001b[0m\n",
      "\u001b[32m20200617     0.0970  0.0963  0.0978  0.0947  0.0927  0.0946\u001b[0m\n",
      "\u001b[32m20201214     0.0837  0.0792  0.0842  0.0759  0.0766  0.0739\u001b[0m\n",
      "\u001b[32m20210615     0.0605  0.0572  0.0608  0.0689  0.0666  0.0662\u001b[0m\n",
      "\u001b[32m20211209     0.0935  0.0954  0.0962  0.1084  0.1106  0.1084\u001b[0m\n",
      "\u001b[32m20220613     0.0931  0.0909  0.0813  0.0857  0.0829  0.0876\u001b[0m\n",
      "\u001b[32m20221206     0.0650  0.0616  0.0635  0.0580  0.0576  0.0497\u001b[0m\n",
      "\u001b[32m20230606     0.0906  0.0857  0.0872  0.0868  0.0885  0.0856\u001b[0m\n",
      "\u001b[32m20231201     0.1217  0.1221  0.1235  0.1082  0.1025  0.1008\u001b[0m\n",
      "\u001b[32m20240604    -0.0130  0.0400 -0.0266  0.0401  0.0503  0.0572\u001b[0m\n",
      "\u001b[32mAllTimeAvg   0.1024  0.1008  0.1016  0.1012  0.1005  0.1003\u001b[0m\n",
      "\u001b[32mAllTimeSum   185.48  182.66  184.04  183.36  182.05  181.72\u001b[0m\n",
      "\u001b[32mStd          0.0668  0.0665  0.0669  0.0666  0.0656  0.0665\u001b[0m\n",
      "\u001b[32mTValue        65.23   64.56   64.64   64.69   65.24   64.18\u001b[0m\n",
      "\u001b[32mAnnIR        7.5077  7.4304  7.4389  7.4445  7.5079  7.3860\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:25:46|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 57.6 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:25:46|MOD:time        |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 15 Minutes 3.3 Seconds\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:25:46|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:25:46|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Mon Jun 24 01:25:46 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Process Queue : Data + Fit + Test\n",
      "--Confirm Resume Training!\n",
      "--Model_name is set to gruRTN_day!\n",
      "Callback : ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True) , reset optimizer on some epoch (can speedup scheduler)\n",
      "Callback : CallbackTimer(verbosity=2) , record time cost of callback hooks\n",
      "Callback : EarlyStoppage(patience=20) , stop fitting when validation score cease to improve\n",
      "Callback : ValidationConverge(patience=5,eps=1e-05) , stop fitting when valid_score converge\n",
      "Callback : EarlyExitRetrain(earliest=5,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100]) , retrain with new lr if fitting stopped too early\n",
      "Callback : NanLossRetrain(max_attempt=4) , retrain if fitting encounters nan loss\n",
      "Callback : BatchDisplay(verbosity=2) , display batch progress bar\n",
      "Callback : StatusDisplay(verbosity=2) , display epoch and event information\n",
      "{'random_seed': None,\n",
      " 'model_name': 'gruRTN_day',\n",
      " 'model_module': 'gru',\n",
      " 'model_data_type': 'day',\n",
      " 'model_types': ['best', 'swalast', 'swabest'],\n",
      " 'labels': ['rtn_lag1_10'],\n",
      " 'beg_date': 20170103,\n",
      " 'end_date': 99991231,\n",
      " 'sample_method': 'train_shuffle',\n",
      " 'shuffle_option': 'epoch',\n",
      " 'lgbm_ensembler': False}\n",
      "{'hidden_dim': [32, 64],\n",
      " 'seqlens': [{'day': 30, '30m': 30, 'dms': 30}],\n",
      " 'tra_seqlens': [{'hist_loss': 40}],\n",
      " 'dropout': [0.1],\n",
      " 'enc_in': [True],\n",
      " 'enc_att': [False],\n",
      " 'rnn_type': ['lstm'],\n",
      " 'rnn_att': [False],\n",
      " 'rnn_layers': [2],\n",
      " 'dec_mlp_layers': [2],\n",
      " 'num_output': [1],\n",
      " 'kernel_size': [3],\n",
      " 'hidden_as_factor': [False],\n",
      " 'ordered_param_group': [False],\n",
      " 'tra_num_states': [5]}\n",
      "try using /home/mengkjin/Workspace/learndl/data/DataSet/day.20240605.pt , success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:25:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 2.1 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:25:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Mon Jun 24 01:25:48 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mFirstBite Ep#  0 : loss  1.00382, train-0.00516, valid-0.03649, best-0.0365, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.86311, train 0.13764, valid 0.09228, best 0.0923, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.84547, train 0.15544, valid 0.09630, best 0.0963, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.83038, train 0.17066, valid 0.08826, best 0.0963, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.82732, train 0.17363, valid 0.08575, best 0.0963, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.81729, train 0.18370, valid 0.08407, best 0.0963, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 30 : loss  0.81421, train 0.18647, valid 0.08262, best 0.0963, lr1.6e-04\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:27:46|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRTN_day #1 @20231201|FirstBite Ep# 31 EarlyStop|Train 0.1865 Valid 0.0826 BestVal 0.0826|Cost  1.9Min,  3.6Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  1.00293, train-0.00341, valid-0.00344, best-0.0034, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.87709, train 0.12551, valid 0.08796, best 0.1129, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.86180, train 0.13731, valid 0.10436, best 0.1129, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.85287, train 0.14587, valid 0.10502, best 0.1129, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.85180, train 0.14778, valid 0.09660, best 0.1129, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 23 : loss  0.84699, train 0.15223, valid 0.10093, best 0.1129, lr1.0e-07, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  0 : loss  0.99854, train 0.00457, valid 0.01988, best 0.0199, lr1.3e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  5 : loss  0.92030, train 0.08103, valid 0.08255, best 0.0825, lr2.5e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 10 : loss  0.90702, train 0.09389, valid 0.08568, best 0.0857, lr1.9e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 15 : loss  0.89605, train 0.10568, valid 0.10030, best 0.1003, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 20 : loss  0.89414, train 0.10746, valid 0.09676, best 0.1003, lr9.4e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 23 : loss  0.89076, train 0.11050, valid 0.09963, best 0.1003, lr1.0e-07, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  0 : loss  0.99821, train 0.00317, valid 0.04276, best 0.0428, lr1.3e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  5 : loss  0.91434, train 0.07999, valid 0.07219, best 0.1080, lr2.5e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 10 : loss  0.89327, train 0.10612, valid 0.08880, best 0.1080, lr1.9e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 15 : loss  0.88616, train 0.11392, valid 0.08959, best 0.1080, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 20 : loss  0.88396, train 0.11643, valid 0.09105, best 0.1080, lr9.4e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 23 : loss  0.87573, train 0.12499, valid 0.08870, best 0.1080, lr1.0e-07, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  0 : loss  1.00152, train 0.00047, valid 0.02760, best 0.0276, lr1.3e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  5 : loss  0.96801, train 0.04157, valid 0.06108, best 0.0611, lr2.5e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 10 : loss  0.95807, train 0.04909, valid 0.05827, best 0.0611, lr1.9e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 15 : loss  0.95065, train 0.05443, valid 0.05588, best 0.0611, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 20 : loss  0.94877, train 0.05559, valid 0.05718, best 0.0611, lr9.5e-06\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 23 : loss  0.94672, train 0.05714, valid 0.05652, best 0.0611, lr1.0e-07, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  0 : loss  1.00047, train-0.00169, valid-0.00940, best-0.0094, lr1.3e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  5 : loss  0.97745, train 0.03642, valid 0.05397, best 0.0540, lr2.5e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 10 : loss  0.96087, train 0.04016, valid 0.04625, best 0.0540, lr1.9e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 15 : loss  0.97887, train 0.02974, valid 0.04515, best 0.0592, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 20 : loss  0.97797, train 0.03105, valid 0.02864, best 0.0592, lr9.4e-02\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:33:42|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRTN_day #0 @20240604|Retrain#4 Ep#120 EarlyStop|Train 0.0324 Valid 0.0466 BestVal 0.1129|Cost  5.9Min,  2.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:35:37|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRTN_day #1 @20240604|FirstBite Ep# 31 EarlyStop|Train 0.1916 Valid 0.0938 BestVal 0.0938|Cost  1.9Min,  3.6Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:35:37|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Fit], Cost 0.2 Hours, 3.3 Min/model, 3.2 Sec/Epoch\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:35:37|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Mon Jun 24 01:35:37 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:35:37|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mEach Model Date Testing Mean Score(spearman):\u001b[0m\n",
      "\u001b[32mModels            0       0       0       1       1       1\u001b[0m\n",
      "\u001b[32mOutput         best swalast swabest    best swalast swabest\u001b[0m\n",
      "\u001b[32m20170103     0.1415  0.1404  0.1427  0.1449  0.1435  0.1447\u001b[0m\n",
      "\u001b[32m20170704     0.1344  0.1262  0.1345  0.1305  0.1318  0.1309\u001b[0m\n",
      "\u001b[32m20171226     0.1443  0.1441  0.1454  0.1443  0.1440  0.1482\u001b[0m\n",
      "\u001b[32m20180627     0.1204  0.1149  0.1163  0.1097  0.1025  0.1077\u001b[0m\n",
      "\u001b[32m20181220     0.1134  0.1100  0.1116  0.1024  0.1004  0.1035\u001b[0m\n",
      "\u001b[32m20190624     0.0928  0.0923  0.0936  0.0977  0.0973  0.0969\u001b[0m\n",
      "\u001b[32m20191217     0.1172  0.1181  0.1180  0.1052  0.1068  0.1165\u001b[0m\n",
      "\u001b[32m20200617     0.0920  0.0939  0.0939  0.0902  0.0826  0.0929\u001b[0m\n",
      "\u001b[32m20201214     0.0986  0.0984  0.0981  0.0939  0.0944  0.0957\u001b[0m\n",
      "\u001b[32m20210615     0.0696  0.0720  0.0718  0.0754  0.0735  0.0734\u001b[0m\n",
      "\u001b[32m20211209     0.1126  0.1152  0.1148  0.1102  0.1128  0.1136\u001b[0m\n",
      "\u001b[32m20220613     0.1110  0.1046  0.1063  0.1074  0.1058  0.1127\u001b[0m\n",
      "\u001b[32m20221206     0.0574  0.0559  0.0579  0.0602  0.0553  0.0578\u001b[0m\n",
      "\u001b[32m20230606     0.0824  0.0776  0.0835  0.0856  0.0771  0.0824\u001b[0m\n",
      "\u001b[32m20231201     0.1327  0.1357  0.1289  0.1284  0.1280  0.1267\u001b[0m\n",
      "\u001b[32m20240604    -0.0291  0.0381  0.0338  0.0062  0.0266  0.0194\u001b[0m\n",
      "\u001b[32mAllTimeAvg   0.1071  0.1062  0.1073  0.1051  0.1032  0.1063\u001b[0m\n",
      "\u001b[32mAllTimeSum   194.08  192.38  194.48  190.38  187.02  192.67\u001b[0m\n",
      "\u001b[32mStd          0.0877  0.0881  0.0874  0.0877  0.0868  0.0861\u001b[0m\n",
      "\u001b[32mTValue        51.97   51.28   52.26   51.01   50.60   52.58\u001b[0m\n",
      "\u001b[32mAnnIR        5.9810  5.9022  6.0149  5.8711  5.8230  6.0509\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:36:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 54.9 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:36:32|MOD:time        |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 10 Minutes 45.7 Seconds\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:36:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:36:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Mon Jun 24 01:36:32 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Process Queue : Data + Fit + Test\n",
      "--Confirm Resume Training!\n",
      "--Model_name is set to gruRES_day!\n",
      "Callback : ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True) , reset optimizer on some epoch (can speedup scheduler)\n",
      "Callback : CallbackTimer(verbosity=2) , record time cost of callback hooks\n",
      "Callback : EarlyStoppage(patience=20) , stop fitting when validation score cease to improve\n",
      "Callback : ValidationConverge(patience=5,eps=1e-05) , stop fitting when valid_score converge\n",
      "Callback : EarlyExitRetrain(earliest=5,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100]) , retrain with new lr if fitting stopped too early\n",
      "Callback : NanLossRetrain(max_attempt=4) , retrain if fitting encounters nan loss\n",
      "Callback : BatchDisplay(verbosity=2) , display batch progress bar\n",
      "Callback : StatusDisplay(verbosity=2) , display epoch and event information\n",
      "{'random_seed': None,\n",
      " 'model_name': 'gruRES_day',\n",
      " 'model_module': 'gru',\n",
      " 'model_data_type': 'day',\n",
      " 'model_types': ['best', 'swalast', 'swabest'],\n",
      " 'labels': ['res_lag1_10'],\n",
      " 'beg_date': 20170103,\n",
      " 'end_date': 99991231,\n",
      " 'sample_method': 'train_shuffle',\n",
      " 'shuffle_option': 'epoch',\n",
      " 'lgbm_ensembler': False}\n",
      "{'hidden_dim': [32, 64],\n",
      " 'seqlens': [{'day': 30, '30m': 30, 'dms': 30}],\n",
      " 'tra_seqlens': [{'hist_loss': 40}],\n",
      " 'dropout': [0.1],\n",
      " 'enc_in': [True],\n",
      " 'enc_att': [False],\n",
      " 'rnn_type': ['lstm'],\n",
      " 'rnn_att': [False],\n",
      " 'rnn_layers': [2],\n",
      " 'dec_mlp_layers': [2],\n",
      " 'num_output': [1],\n",
      " 'kernel_size': [3],\n",
      " 'hidden_as_factor': [False],\n",
      " 'ordered_param_group': [False],\n",
      " 'tra_num_states': [5]}\n",
      "try using /home/mengkjin/Workspace/learndl/data/DataSet/day.20240605.pt , success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:36:34|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 2.0 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:36:34|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Mon Jun 24 01:36:34 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mFirstBite Ep#  0 : loss  1.01252, train-0.01521, valid-0.03742, best-0.0374, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.91026, train 0.08135, valid 0.03185, best 0.0599, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.89291, train 0.09436, valid 0.04522, best 0.0599, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.87755, train 0.10648, valid 0.03959, best 0.0599, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.87426, train 0.10831, valid 0.03543, best 0.0599, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 21 : loss  0.87082, train 0.11088, valid 0.03980, best 0.0599, lr6.3e-04, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  0 : loss  0.99637, train 0.00462, valid 0.03106, best 0.0311, lr1.3e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  5 : loss  0.94196, train 0.05698, valid 0.03578, best 0.0560, lr2.5e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 10 : loss  0.92553, train 0.06750, valid 0.03080, best 0.0560, lr1.9e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 15 : loss  0.91653, train 0.07499, valid 0.03537, best 0.0560, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 20 : loss  0.91509, train 0.07561, valid 0.04185, best 0.0560, lr9.4e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 21 : loss  0.91403, train 0.07721, valid 0.04641, best 0.0560, lr6.3e-05, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  0 : loss  1.00549, train-0.00683, valid-0.02416, best-0.0242, lr1.3e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  5 : loss  0.94359, train 0.04867, valid 0.02798, best 0.0514, lr2.5e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 10 : loss  0.93368, train 0.05600, valid 0.02787, best 0.0514, lr1.9e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 15 : loss  0.92716, train 0.06278, valid 0.03398, best 0.0514, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 20 : loss  0.92646, train 0.06384, valid 0.04303, best 0.0514, lr9.4e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 21 : loss  0.92585, train 0.06408, valid 0.03492, best 0.0514, lr6.3e-03, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  0 : loss  1.01260, train-0.01493, valid-0.03974, best-0.0397, lr1.3e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  5 : loss  0.96876, train 0.02948, valid 0.04494, best 0.0477, lr2.5e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 10 : loss  0.96445, train 0.03233, valid 0.04554, best 0.0477, lr1.9e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 15 : loss  0.96098, train 0.03497, valid 0.04404, best 0.0477, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 20 : loss  0.96022, train 0.03574, valid 0.04354, best 0.0477, lr9.5e-06\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 21 : loss  0.95992, train 0.03628, valid 0.04487, best 0.0477, lr6.3e-06, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  0 : loss  0.99746, train 0.00225, valid-0.00306, best-0.0031, lr1.3e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  5 : loss  0.97511, train 0.02839, valid 0.01301, best 0.0537, lr2.5e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 10 : loss  0.96875, train 0.03070, valid 0.01638, best 0.0537, lr1.9e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 15 : loss  0.96160, train 0.03236, valid 0.03529, best 0.0537, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 20 : loss  0.96655, train 0.02830, valid 0.04237, best 0.0537, lr9.4e-02\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:43:22|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #1 @20231201|Retrain#4 Ep#110 EarlyStop|Train 0.0293 Valid 0.0368 BestVal 0.0599|Cost  6.7Min,  3.6Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  0.99752, train 0.00292, valid 0.02356, best 0.0236, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.92738, train 0.06719, valid 0.04180, best 0.0533, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.90904, train 0.07964, valid 0.02435, best 0.0533, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.89805, train 0.08672, valid 0.03391, best 0.0533, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.89733, train 0.08698, valid 0.03455, best 0.0533, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 21 : loss  0.89541, train 0.08806, valid 0.04139, best 0.0533, lr6.3e-04, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  0 : loss  1.00733, train-0.00926, valid-0.03680, best-0.0368, lr1.3e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  5 : loss  0.95510, train 0.04285, valid 0.05432, best 0.0549, lr2.5e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 10 : loss  0.94688, train 0.04934, valid 0.04339, best 0.0549, lr1.9e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 15 : loss  0.93907, train 0.05436, valid 0.03791, best 0.0549, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 20 : loss  0.93641, train 0.05594, valid 0.03422, best 0.0549, lr9.4e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 22 : loss  0.93590, train 0.05702, valid 0.04172, best 0.0549, lr3.1e-05, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  0 : loss  0.99719, train 0.00375, valid 0.01564, best 0.0156, lr1.3e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  5 : loss  0.94177, train 0.04812, valid 0.03631, best 0.0465, lr2.5e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 10 : loss  0.93093, train 0.05774, valid 0.03864, best 0.0465, lr1.9e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 15 : loss  0.92785, train 0.05940, valid 0.02965, best 0.0465, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 20 : loss  0.92459, train 0.06304, valid 0.03018, best 0.0465, lr9.4e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 22 : loss  0.92154, train 0.06476, valid 0.03223, best 0.0465, lr3.1e-03, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  0 : loss  0.99855, train 0.00161, valid 0.01776, best 0.0178, lr1.3e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  5 : loss  0.97299, train 0.02829, valid 0.05143, best 0.0514, lr2.5e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 10 : loss  0.96918, train 0.02884, valid 0.05178, best 0.0518, lr1.9e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 15 : loss  0.96640, train 0.02942, valid 0.05233, best 0.0523, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 20 : loss  0.96524, train 0.02948, valid 0.05201, best 0.0523, lr9.5e-06\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 22 : loss  0.96442, train 0.03008, valid 0.05196, best 0.0523, lr3.2e-06, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  0 : loss  1.00164, train-0.00223, valid-0.02355, best-0.0236, lr1.3e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  5 : loss  0.97856, train 0.02428, valid 0.02481, best 0.0474, lr2.5e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 10 : loss  0.97707, train 0.02539, valid 0.04388, best 0.0474, lr1.9e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 15 : loss  0.97920, train 0.02491, valid 0.01606, best 0.0474, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 20 : loss  0.97573, train 0.02385, valid 0.01877, best 0.0474, lr9.4e-02\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:48:54|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #0 @20240604|Retrain#4 Ep#114 EarlyStop|Train 0.0265 Valid 0.0202 BestVal 0.0549|Cost  5.5Min,  2.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:56:07|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #1 @20240604|Retrain#4 Ep#115 EarlyStop|Train 0.0274 Valid 0.0224 BestVal 0.0568|Cost  7.2Min,  3.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:56:07|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Fit], Cost 0.3 Hours, 6.5 Min/model, 3.5 Sec/Epoch\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:56:07|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Mon Jun 24 01:56:07 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:56:07|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mEach Model Date Testing Mean Score(spearman):\u001b[0m\n",
      "\u001b[32mModels            0       0       0       1       1       1\u001b[0m\n",
      "\u001b[32mOutput         best swalast swabest    best swalast swabest\u001b[0m\n",
      "\u001b[32m20170103     0.1107  0.1119  0.1114  0.1060  0.1068  0.1081\u001b[0m\n",
      "\u001b[32m20170704     0.0937  0.0946  0.0972  0.0970  0.0989  0.0993\u001b[0m\n",
      "\u001b[32m20171226     0.0889  0.0892  0.0893  0.0901  0.0905  0.0905\u001b[0m\n",
      "\u001b[32m20180627     0.0713  0.0712  0.0713  0.0738  0.0715  0.0736\u001b[0m\n",
      "\u001b[32m20181220     0.0733  0.0742  0.0746  0.0740  0.0741  0.0728\u001b[0m\n",
      "\u001b[32m20190624     0.0702  0.0701  0.0703  0.0663  0.0672  0.0681\u001b[0m\n",
      "\u001b[32m20191217     0.0774  0.0745  0.0763  0.0789  0.0759  0.0750\u001b[0m\n",
      "\u001b[32m20200617     0.0555  0.0563  0.0570  0.0597  0.0579  0.0592\u001b[0m\n",
      "\u001b[32m20201214     0.0610  0.0605  0.0609  0.0619  0.0606  0.0642\u001b[0m\n",
      "\u001b[32m20210615     0.0406  0.0406  0.0438  0.0459  0.0475  0.0477\u001b[0m\n",
      "\u001b[32m20211209     0.0568  0.0572  0.0567  0.0561  0.0556  0.0573\u001b[0m\n",
      "\u001b[32m20220613     0.0457  0.0450  0.0465  0.0447  0.0461  0.0462\u001b[0m\n",
      "\u001b[32m20221206     0.0241  0.0235  0.0277  0.0287  0.0243  0.0296\u001b[0m\n",
      "\u001b[32m20230606     0.0589  0.0451  0.0425  0.0361  0.0324  0.0353\u001b[0m\n",
      "\u001b[32m20231201     0.0291  0.0323  0.0298  0.0140  0.0078  0.0160\u001b[0m\n",
      "\u001b[32m20240604    -0.0728 -0.0680 -0.0678 -0.0540  0.0016 -0.0326\u001b[0m\n",
      "\u001b[32mAllTimeAvg   0.0629  0.0622  0.0628  0.0614  0.0608  0.0622\u001b[0m\n",
      "\u001b[32mAllTimeSum   113.98  112.74  113.82  111.32  110.09  112.77\u001b[0m\n",
      "\u001b[32mStd          0.0537  0.0507  0.0500  0.0504  0.0493  0.0490\u001b[0m\n",
      "\u001b[32mTValue        49.87   52.28   53.42   51.92   52.51   54.01\u001b[0m\n",
      "\u001b[32mAnnIR        5.7394  6.0170  6.1485  5.9749  6.0436  6.2161\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:57:02|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 55.3 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:57:02|MOD:time        |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 20 Minutes 30.0 Seconds\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from src.api import Trainer\n",
    "Trainer.update_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
