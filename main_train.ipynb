{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available() , torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device name: NVIDIA GeForce RTX 4090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:10:30|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-08-29 09:10:31|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Thu Aug 29 09:10:31 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Process Queue : Data + Fit + Test\n",
      "--Model_name is set to gru_2year_day!\n",
      "Callback : ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True,kwargs={}) , reset optimizer on some epoch (can speedup scheduler)\n",
      "Callback : CallbackTimer(verbosity=2,kwargs={}) , record time cost of callback hooks\n",
      "Callback : EarlyStoppage(patience=20,kwargs={}) , stop fitting when validation score cease to improve\n",
      "Callback : ValidationConverge(patience=5,eps=1e-05,kwargs={}) , stop fitting when valid_score converge\n",
      "Callback : EarlyExitRetrain(earliest=10,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100, 1],kwargs={}) , retrain with new lr if fitting stopped too early\n",
      "Callback : NanLossRetrain(max_attempt=4,kwargs={}) , retrain if fitting encounters nan loss\n",
      "Callback : BatchDisplay(verbosity=2,kwargs={}) , display batch progress bar\n",
      "Callback : StatusDisplay(verbosity=2,kwargs={}) , display epoch and event information\n",
      "Callback : GroupReturnAnalysis(group_num=20,kwargs={}) , record and concat each model to Alpha model instance\n",
      "Callback : DetailedAlphaAnalysis(use_num=avg,kwargs={}) , record and concat each model to Alpha model instance\n",
      "{'model_name': 'gru_2year_day',\n",
      " 'model_module': 'gru',\n",
      " 'model.types': ['best', 'swalast', 'swabest'],\n",
      " 'model.booster_type': 'lgbm',\n",
      " 'model.booster_head': False,\n",
      " 'data.types': 'day',\n",
      " 'data.labels': ['std_lag1_10', 'rtn_lag1_10'],\n",
      " 'random_seed': None,\n",
      " 'beg_date': 20170103,\n",
      " 'end_date': 99991231,\n",
      " 'sample_method': 'sequential',\n",
      " 'shuffle_option': 'epoch'}\n",
      "{'hidden_dim': [32, 32, 32, 32, 32],\n",
      " 'seqlens': [{'day': 30, '30m': 30}],\n",
      " 'dropout': [0.1],\n",
      " 'enc_in': [True],\n",
      " 'enc_att': [False],\n",
      " 'rnn_type': ['gru'],\n",
      " 'rnn_att': [False],\n",
      " 'rnn_layers': [2],\n",
      " 'dec_mlp_layers': [2],\n",
      " 'num_output': [1],\n",
      " 'which_output': [0],\n",
      " 'kernel_size': [3],\n",
      " 'hidden_as_factor': [True],\n",
      " 'ordered_param_group': [False],\n",
      " 'verbosity': 2}\n",
      "try using /home/mengkjin/Workspace/learndl/data/DataSet/day.20240703.pt , success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-08-29 09:10:33|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 2.3 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-08-29 09:10:33|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Thu Aug 29 09:10:33 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:10:33|MOD:mod         |\u001b[0m: \u001b[1m\u001b[34mFirst Iterance: (20170103 , 0)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n",
      "score function of [spearman] calculated and success!\n",
      "loss function of [pearson] calculated and success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mFirstBite Ep#  0 : loss  0.99038, train 0.02216, valid 0.06462, best 0.0646, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.90292, train 0.12178, valid 0.10875, best 0.1088, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.87031, train 0.14308, valid 0.11810, best 0.1220, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.85130, train 0.16598, valid 0.16846, best 0.1685, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.85032, train 0.16306, valid 0.16968, best 0.1736, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.83462, train 0.17855, valid 0.17743, best 0.1774, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 30 : loss  0.83087, train 0.18163, valid 0.17926, best 0.1793, lr1.6e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 35 : loss  0.82852, train 0.18298, valid 0.17861, best 0.1793, lr3.1e-04\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 39 , effective at epoch 40, and will speedup2x\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 40 : loss  0.82407, train 0.18749, valid 0.18199, best 0.1820, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 45 : loss  0.83617, train 0.17441, valid 0.17580, best 0.1833, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 50 : loss  0.82096, train 0.19403, valid 0.19818, best 0.1982, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 55 : loss  0.81144, train 0.20345, valid 0.18116, best 0.1982, lr1.0e-07\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 59 , effective at epoch 60, and will speedup2x\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 60 : loss  0.80694, train 0.20931, valid 0.19057, best 0.1982, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 65 : loss  0.82302, train 0.19074, valid 0.19052, best 0.1982, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 70 : loss  0.80452, train 0.21196, valid 0.19924, best 0.1992, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 75 : loss  0.79516, train 0.22380, valid 0.19060, best 0.1992, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 80 : loss  0.79476, train 0.22408, valid 0.18217, best 0.1992, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 85 : loss  0.78806, train 0.23006, valid 0.19001, best 0.1992, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 90 : loss  0.78747, train 0.23171, valid 0.19110, best 0.1992, lr1.6e-04\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:12:01|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #0 @20170103|FirstBite Ep# 91 EarlyStop|Train 0.2317 Valid 0.1911 BestVal 0.1911|Cost  1.5Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  0.99791, train 0.00899, valid 0.00342, best 0.0034, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.91523, train 0.11793, valid 0.04013, best 0.0491, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.89417, train 0.12529, valid 0.08916, best 0.0892, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.86547, train 0.14816, valid 0.13106, best 0.1311, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.85877, train 0.15588, valid 0.15494, best 0.1598, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.84580, train 0.16853, valid 0.16857, best 0.1686, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 30 : loss  0.84231, train 0.17063, valid 0.15323, best 0.1686, lr1.6e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 35 : loss  0.83905, train 0.17335, valid 0.16626, best 0.1746, lr3.1e-04\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 39 , effective at epoch 40, and will speedup2x\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 40 : loss  0.83410, train 0.17852, valid 0.17297, best 0.1749, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 45 : loss  0.84391, train 0.16539, valid 0.14905, best 0.1777, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 50 : loss  0.82406, train 0.18665, valid 0.18110, best 0.1811, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 55 : loss  0.81421, train 0.19684, valid 0.17451, best 0.1841, lr1.0e-07\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 59 , effective at epoch 60, and will speedup2x\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 60 : loss  0.81097, train 0.20034, valid 0.18551, best 0.1855, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 65 : loss  0.82879, train 0.18323, valid 0.18885, best 0.1939, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 70 : loss  0.80574, train 0.20794, valid 0.18847, best 0.1939, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 75 : loss  0.79703, train 0.21827, valid 0.18173, best 0.1939, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 80 : loss  0.79875, train 0.21640, valid 0.18697, best 0.1939, lr9.4e-04\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:13:22|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #1 @20170103|FirstBite Ep# 82 EarlyStop|Train 0.2205 Valid 0.1824 BestVal 0.1824|Cost  1.3Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  0.99938, train-0.00096, valid-0.00278, best-0.0028, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.91656, train 0.11411, valid 0.06416, best 0.0642, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.87657, train 0.13585, valid 0.08182, best 0.0854, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.86027, train 0.15120, valid 0.11802, best 0.1180, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.85864, train 0.15357, valid 0.14164, best 0.1416, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.84310, train 0.16677, valid 0.15525, best 0.1552, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 30 : loss  0.83958, train 0.17065, valid 0.16004, best 0.1600, lr1.6e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 35 : loss  0.83737, train 0.17257, valid 0.16339, best 0.1634, lr3.1e-04\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 39 , effective at epoch 40, and will speedup2x\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 40 : loss  0.83041, train 0.17861, valid 0.16395, best 0.1702, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 45 : loss  0.84440, train 0.17086, valid 0.14453, best 0.1702, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 50 : loss  0.82285, train 0.18758, valid 0.15951, best 0.1817, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 55 : loss  0.81221, train 0.19989, valid 0.18302, best 0.1830, lr1.0e-07\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 59 , effective at epoch 60, and will speedup2x\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 60 : loss  0.80896, train 0.20213, valid 0.16837, best 0.1830, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 65 : loss  0.82333, train 0.19082, valid 0.16320, best 0.1932, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 70 : loss  0.80436, train 0.21215, valid 0.18235, best 0.1932, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 75 : loss  0.79915, train 0.21821, valid 0.18123, best 0.1932, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 80 : loss  0.79771, train 0.21723, valid 0.17677, best 0.1932, lr9.4e-04\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:14:40|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #2 @20170103|FirstBite Ep# 84 EarlyStop|Train 0.2257 Valid 0.1845 BestVal 0.1845|Cost  1.3Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  1.00613, train-0.00930, valid-0.07520, best-0.0752, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.92003, train 0.11133, valid 0.04196, best 0.0455, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.89942, train 0.12913, valid 0.07118, best 0.0712, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.86551, train 0.14813, valid 0.10732, best 0.1078, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.86057, train 0.14981, valid 0.10507, best 0.1132, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.84681, train 0.16757, valid 0.12784, best 0.1278, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 30 : loss  0.84296, train 0.17043, valid 0.13316, best 0.1343, lr1.6e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 35 : loss  0.83950, train 0.17528, valid 0.13907, best 0.1408, lr3.1e-04\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 39 , effective at epoch 40, and will speedup2x\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 40 : loss  0.83679, train 0.17774, valid 0.14269, best 0.1427, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 45 : loss  0.84912, train 0.16468, valid 0.15042, best 0.1504, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 50 : loss  0.82260, train 0.18928, valid 0.16447, best 0.1716, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 55 : loss  0.81269, train 0.20383, valid 0.17139, best 0.1751, lr1.0e-07\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 59 , effective at epoch 60, and will speedup2x\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 60 : loss  0.80722, train 0.21218, valid 0.17302, best 0.1751, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 65 : loss  0.82043, train 0.19112, valid 0.15973, best 0.1893, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 70 : loss  0.80276, train 0.21663, valid 0.16749, best 0.1893, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 75 : loss  0.79663, train 0.22484, valid 0.17984, best 0.1893, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 80 : loss  0.79607, train 0.22434, valid 0.17167, best 0.1893, lr9.4e-04\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:15:58|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #3 @20170103|FirstBite Ep# 84 EarlyStop|Train 0.2313 Valid 0.1755 BestVal 0.1755|Cost  1.3Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  1.02041, train-0.02845, valid-0.01544, best-0.0154, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.91618, train 0.11130, valid 0.07160, best 0.0716, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.89270, train 0.13405, valid 0.09413, best 0.0941, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.85442, train 0.15920, valid 0.14490, best 0.1449, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.85280, train 0.15983, valid 0.12921, best 0.1496, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.84039, train 0.17387, valid 0.15833, best 0.1583, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 30 : loss  0.83662, train 0.17662, valid 0.15563, best 0.1614, lr1.6e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 35 : loss  0.83731, train 0.17491, valid 0.16386, best 0.1639, lr3.1e-04\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 39 , effective at epoch 40, and will speedup2x\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 40 : loss  0.83119, train 0.18240, valid 0.16086, best 0.1654, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 45 : loss  0.84690, train 0.16356, valid 0.11116, best 0.1654, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 50 : loss  0.82049, train 0.19046, valid 0.17231, best 0.1723, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 55 : loss  0.81162, train 0.20058, valid 0.17663, best 0.1766, lr1.0e-07\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 59 , effective at epoch 60, and will speedup2x\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 60 : loss  0.80615, train 0.20672, valid 0.17425, best 0.1770, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 65 : loss  0.81775, train 0.19262, valid 0.17493, best 0.1795, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 70 : loss  0.80301, train 0.21148, valid 0.17260, best 0.1795, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 75 : loss  0.79405, train 0.22284, valid 0.17918, best 0.1824, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 80 : loss  0.79523, train 0.22217, valid 0.17944, best 0.1824, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 85 : loss  0.78713, train 0.23058, valid 0.17924, best 0.1824, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 90 : loss  0.78654, train 0.23239, valid 0.18156, best 0.1824, lr1.6e-04\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:17:26|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #4 @20170103|FirstBite Ep# 95 EarlyStop|Train 0.2345 Valid 0.1818 BestVal 0.1818|Cost  1.5Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:18:39|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #0 @20170704|FirstBite Ep# 78 EarlyStop|Train 0.2476 Valid 0.1667 BestVal 0.1667|Cost  1.2Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:19:49|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #1 @20170704|FirstBite Ep# 75 EarlyStop|Train 0.2382 Valid 0.1653 BestVal 0.1653|Cost  1.2Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:21:21|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #2 @20170704|FirstBite Ep# 99 EarlyStop|Train 0.2484 Valid 0.1644 BestVal 0.1644|Cost  1.5Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:22:29|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #3 @20170704|FirstBite Ep# 72 EarlyStop|Train 0.2210 Valid 0.1647 BestVal 0.1647|Cost  1.1Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:23:37|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #4 @20170704|FirstBite Ep# 72 EarlyStop|Train 0.2284 Valid 0.1573 BestVal 0.1573|Cost  1.1Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:25:18|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #0 @20171226|FirstBite Ep#108 EarlyStop|Train 0.2480 Valid 0.1474 BestVal 0.1474|Cost  1.7Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:26:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #1 @20171226|FirstBite Ep# 95 EarlyStop|Train 0.2494 Valid 0.1468 BestVal 0.1468|Cost  1.5Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:28:44|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #2 @20171226|FirstBite Ep#125 EarlyStop|Train 0.2507 Valid 0.1468 BestVal 0.1468|Cost  1.9Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:30:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #3 @20171226|FirstBite Ep#110 EarlyStop|Train 0.2475 Valid 0.1503 BestVal 0.1503|Cost  1.8Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:32:23|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #4 @20171226|FirstBite Ep#112 EarlyStop|Train 0.2475 Valid 0.1470 BestVal 0.1470|Cost  1.8Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:34:26|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #0 @20180627|FirstBite Ep#126 EarlyStop|Train 0.2227 Valid 0.1294 BestVal 0.1294|Cost  2.0Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:36:03|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #1 @20180627|FirstBite Ep#102 EarlyStop|Train 0.2207 Valid 0.1243 BestVal 0.1243|Cost  1.6Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:37:49|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #2 @20180627|FirstBite Ep#109 EarlyStop|Train 0.2206 Valid 0.1260 BestVal 0.1260|Cost  1.8Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:39:42|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #3 @20180627|FirstBite Ep#118 EarlyStop|Train 0.2212 Valid 0.1295 BestVal 0.1295|Cost  1.9Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:41:41|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #4 @20180627|FirstBite Ep#125 EarlyStop|Train 0.2221 Valid 0.1285 BestVal 0.1285|Cost  2.0Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:43:09|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #0 @20181220|FirstBite Ep# 93 EarlyStop|Train 0.1922 Valid 0.1580 BestVal 0.1580|Cost  1.5Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:44:52|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #1 @20181220|FirstBite Ep#109 EarlyStop|Train 0.1911 Valid 0.1584 BestVal 0.1584|Cost  1.7Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:46:27|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #2 @20181220|FirstBite Ep#102 EarlyStop|Train 0.1882 Valid 0.1658 BestVal 0.1658|Cost  1.6Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:47:42|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #3 @20181220|FirstBite Ep# 80 EarlyStop|Train 0.1753 Valid 0.1621 BestVal 0.1621|Cost  1.2Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:49:19|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #4 @20181220|FirstBite Ep#104 EarlyStop|Train 0.1905 Valid 0.1620 BestVal 0.1620|Cost  1.6Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:51:22|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #0 @20190624|Retrain#4 Ep#131 EarlyStop|Train 0.0300 Valid 0.0657 BestVal 0.1199|Cost  2.0Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:53:30|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #1 @20190624|Retrain#4 Ep#135 EarlyStop|Train 0.0505 Valid 0.0694 BestVal 0.1117|Cost  2.1Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:54:09|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #2 @20190624|FirstBite Ep# 42 EarlyStop|Train 0.1358 Valid 0.0810 BestVal 0.0810|Cost  0.7Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:54:47|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #3 @20190624|FirstBite Ep# 40 EarlyStop|Train 0.1492 Valid 0.0919 BestVal 0.0919|Cost  0.6Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:56:45|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #4 @20190624|Retrain#4 Ep#125 EarlyStop|Train 0.0289 Valid 0.0548 BestVal 0.1162|Cost  2.0Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 09:58:43|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #0 @20191217|FirstBite Ep#126 Valid Cvg|Train 0.1692 Valid 0.1232 BestVal 0.1232|Cost  2.0Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:00:41|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #1 @20191217|FirstBite Ep#126 EarlyStop|Train 0.1681 Valid 0.1244 BestVal 0.1244|Cost  2.0Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:02:16|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #2 @20191217|FirstBite Ep#101 EarlyStop|Train 0.1762 Valid 0.1160 BestVal 0.1160|Cost  1.6Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:04:10|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #3 @20191217|FirstBite Ep#118 EarlyStop|Train 0.1670 Valid 0.1193 BestVal 0.1193|Cost  1.9Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:05:39|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #4 @20191217|FirstBite Ep# 89 EarlyStop|Train 0.1657 Valid 0.1196 BestVal 0.1196|Cost  1.5Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:06:52|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #0 @20200617|FirstBite Ep# 65 EarlyStop|Train 0.1239 Valid 0.0849 BestVal 0.0849|Cost  1.2Min,  1.1Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:07:37|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #1 @20200617|FirstBite Ep# 40 EarlyStop|Train 0.1291 Valid 0.0757 BestVal 0.0757|Cost  0.8Min,  1.1Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:09:23|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #2 @20200617|FirstBite Ep# 94 EarlyStop|Train 0.1629 Valid 0.0914 BestVal 0.0914|Cost  1.8Min,  1.1Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:11:09|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #3 @20200617|FirstBite Ep#109 EarlyStop|Train 0.1636 Valid 0.0906 BestVal 0.0906|Cost  1.8Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:12:53|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #4 @20200617|FirstBite Ep#110 EarlyStop|Train 0.1622 Valid 0.0884 BestVal 0.0884|Cost  1.7Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:13:26|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #0 @20201214|FirstBite Ep# 32 EarlyStop|Train 0.1109 Valid 0.0622 BestVal 0.0622|Cost  0.5Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:14:00|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #1 @20201214|FirstBite Ep# 33 EarlyStop|Train 0.1074 Valid 0.0620 BestVal 0.0620|Cost  0.6Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:14:34|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #2 @20201214|FirstBite Ep# 35 EarlyStop|Train 0.1168 Valid 0.0677 BestVal 0.0677|Cost  0.6Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:15:13|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #3 @20201214|FirstBite Ep# 41 EarlyStop|Train 0.1178 Valid 0.0731 BestVal 0.0731|Cost  0.6Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:15:52|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #4 @20201214|FirstBite Ep# 41 EarlyStop|Train 0.1168 Valid 0.0720 BestVal 0.0720|Cost  0.6Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:17:22|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #0 @20210615|FirstBite Ep# 94 EarlyStop|Train 0.1216 Valid 0.0802 BestVal 0.0802|Cost  1.5Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:18:41|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #1 @20210615|FirstBite Ep# 83 EarlyStop|Train 0.1238 Valid 0.0769 BestVal 0.0769|Cost  1.3Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:19:21|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #2 @20210615|FirstBite Ep# 42 EarlyStop|Train 0.0837 Valid 0.0613 BestVal 0.0613|Cost  0.7Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:19:59|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #3 @20210615|FirstBite Ep# 39 EarlyStop|Train 0.0933 Valid 0.0710 BestVal 0.0710|Cost  0.6Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:21:05|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #4 @20210615|FirstBite Ep# 69 EarlyStop|Train 0.1171 Valid 0.0816 BestVal 0.0816|Cost  1.1Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:22:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #0 @20211209|FirstBite Ep# 95 EarlyStop|Train 0.1086 Valid 0.0556 BestVal 0.0556|Cost  1.5Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:23:17|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #1 @20211209|FirstBite Ep# 42 EarlyStop|Train 0.0625 Valid 0.0528 BestVal 0.0528|Cost  0.7Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:24:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #2 @20211209|FirstBite Ep# 94 EarlyStop|Train 0.1133 Valid 0.0610 BestVal 0.0610|Cost  1.5Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:25:22|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #3 @20211209|FirstBite Ep# 34 EarlyStop|Train 0.0811 Valid 0.0542 BestVal 0.0542|Cost  0.6Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:25:56|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #4 @20211209|FirstBite Ep# 35 EarlyStop|Train 0.0856 Valid 0.0532 BestVal 0.0532|Cost  0.6Min,  0.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:26:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #0 @20220613|FirstBite Ep# 40 EarlyStop|Train 0.0854 Valid 0.0822 BestVal 0.0822|Cost  0.7Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:27:14|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #1 @20220613|FirstBite Ep# 38 EarlyStop|Train 0.0849 Valid 0.0752 BestVal 0.0752|Cost  0.6Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:27:56|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #2 @20220613|FirstBite Ep# 42 EarlyStop|Train 0.0763 Valid 0.0664 BestVal 0.0664|Cost  0.7Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:28:57|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #3 @20220613|FirstBite Ep# 62 EarlyStop|Train 0.0848 Valid 0.0763 BestVal 0.0763|Cost  1.0Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:30:00|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #4 @20220613|FirstBite Ep# 63 EarlyStop|Train 0.0842 Valid 0.1008 BestVal 0.1008|Cost  1.0Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:30:38|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #0 @20221206|FirstBite Ep# 35 EarlyStop|Train 0.0877 Valid 0.0750 BestVal 0.0750|Cost  0.6Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:31:11|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #1 @20221206|FirstBite Ep# 33 EarlyStop|Train 0.0847 Valid 0.0638 BestVal 0.0638|Cost  0.6Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:31:46|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #2 @20221206|FirstBite Ep# 33 EarlyStop|Train 0.0889 Valid 0.0720 BestVal 0.0720|Cost  0.6Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:32:22|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #3 @20221206|FirstBite Ep# 35 EarlyStop|Train 0.0866 Valid 0.0716 BestVal 0.0716|Cost  0.6Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:32:56|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #4 @20221206|FirstBite Ep# 34 EarlyStop|Train 0.0869 Valid 0.0642 BestVal 0.0642|Cost  0.6Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:33:33|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #0 @20230606|FirstBite Ep# 36 EarlyStop|Train 0.0944 Valid 0.0873 BestVal 0.0873|Cost  0.6Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:34:14|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #1 @20230606|FirstBite Ep# 41 EarlyStop|Train 0.0894 Valid 0.0865 BestVal 0.0865|Cost  0.7Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:34:49|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #2 @20230606|FirstBite Ep# 34 EarlyStop|Train 0.0983 Valid 0.0945 BestVal 0.0945|Cost  0.6Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:35:29|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #3 @20230606|FirstBite Ep# 40 EarlyStop|Train 0.0936 Valid 0.0964 BestVal 0.0964|Cost  0.7Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:36:05|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #4 @20230606|FirstBite Ep# 35 EarlyStop|Train 0.0959 Valid 0.0898 BestVal 0.0898|Cost  0.6Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:37:03|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #0 @20231201|FirstBite Ep# 58 EarlyStop|Train 0.1227 Valid 0.1202 BestVal 0.1202|Cost  1.0Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:37:54|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #1 @20231201|FirstBite Ep# 50 EarlyStop|Train 0.1197 Valid 0.1240 BestVal 0.1240|Cost  0.8Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:39:30|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #2 @20231201|FirstBite Ep# 93 EarlyStop|Train 0.1328 Valid 0.1208 BestVal 0.1208|Cost  1.6Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:40:29|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #3 @20231201|FirstBite Ep# 58 EarlyStop|Train 0.1220 Valid 0.1190 BestVal 0.1190|Cost  1.0Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:41:53|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #4 @20231201|FirstBite Ep# 82 EarlyStop|Train 0.1274 Valid 0.1238 BestVal 0.1238|Cost  1.4Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:42:34|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #0 @20240604|FirstBite Ep# 38 EarlyStop|Train 0.1195 Valid 0.0767 BestVal 0.0767|Cost  0.7Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:43:16|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #1 @20240604|FirstBite Ep# 41 EarlyStop|Train 0.1243 Valid 0.0758 BestVal 0.0758|Cost  0.7Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:44:01|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #2 @20240604|FirstBite Ep# 43 EarlyStop|Train 0.1077 Valid 0.0602 BestVal 0.0602|Cost  0.7Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:45:04|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #3 @20240604|FirstBite Ep# 62 EarlyStop|Train 0.1274 Valid 0.0907 BestVal 0.0907|Cost  1.0Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:45:39|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_2year_day #4 @20240604|FirstBite Ep# 34 EarlyStop|Train 0.1252 Valid 0.0803 BestVal 0.0803|Cost  0.6Min,  1.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-08-29 10:45:39|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Fit], Cost 1.6 Hours, 1.2 Min/model, 1.0 Sec/Epoch\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-08-29 10:45:39|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Thu Aug 29 10:45:39 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-29 10:48:08|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mTesting Mean Score(spearman):\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">gru.0</th>\n",
       "      <th colspan=\"3\" halign=\"left\">gru.1</th>\n",
       "      <th colspan=\"3\" halign=\"left\">gru.2</th>\n",
       "      <th colspan=\"3\" halign=\"left\">gru.3</th>\n",
       "      <th colspan=\"3\" halign=\"left\">gru.4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>best</th>\n",
       "      <th>swalast</th>\n",
       "      <th>swabest</th>\n",
       "      <th>best</th>\n",
       "      <th>swalast</th>\n",
       "      <th>swabest</th>\n",
       "      <th>best</th>\n",
       "      <th>swalast</th>\n",
       "      <th>swabest</th>\n",
       "      <th>best</th>\n",
       "      <th>swalast</th>\n",
       "      <th>swabest</th>\n",
       "      <th>best</th>\n",
       "      <th>swalast</th>\n",
       "      <th>swabest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20170103</th>\n",
       "      <td>0.159</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20170704</th>\n",
       "      <td>0.118</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20171226</th>\n",
       "      <td>0.102</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180627</th>\n",
       "      <td>0.157</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181220</th>\n",
       "      <td>0.121</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190624</th>\n",
       "      <td>0.054</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20191217</th>\n",
       "      <td>0.088</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20200617</th>\n",
       "      <td>0.077</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201214</th>\n",
       "      <td>0.051</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20210615</th>\n",
       "      <td>0.053</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211209</th>\n",
       "      <td>0.092</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20220613</th>\n",
       "      <td>0.069</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20221206</th>\n",
       "      <td>0.077</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20230606</th>\n",
       "      <td>0.107</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20231201</th>\n",
       "      <td>0.102</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20240604</th>\n",
       "      <td>0.046</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg</th>\n",
       "      <td>0.094</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sum</th>\n",
       "      <td>172.683</td>\n",
       "      <td>169.475</td>\n",
       "      <td>171.493</td>\n",
       "      <td>176.657</td>\n",
       "      <td>174.800</td>\n",
       "      <td>178.270</td>\n",
       "      <td>180.774</td>\n",
       "      <td>177.743</td>\n",
       "      <td>179.649</td>\n",
       "      <td>187.284</td>\n",
       "      <td>183.904</td>\n",
       "      <td>185.208</td>\n",
       "      <td>179.908</td>\n",
       "      <td>178.368</td>\n",
       "      <td>183.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Std</th>\n",
       "      <td>0.077</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>52.197</td>\n",
       "      <td>51.159</td>\n",
       "      <td>52.684</td>\n",
       "      <td>53.134</td>\n",
       "      <td>54.138</td>\n",
       "      <td>54.380</td>\n",
       "      <td>56.636</td>\n",
       "      <td>54.957</td>\n",
       "      <td>56.109</td>\n",
       "      <td>56.987</td>\n",
       "      <td>56.475</td>\n",
       "      <td>57.115</td>\n",
       "      <td>54.634</td>\n",
       "      <td>55.654</td>\n",
       "      <td>56.925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IR</th>\n",
       "      <td>5.976</td>\n",
       "      <td>5.857</td>\n",
       "      <td>6.032</td>\n",
       "      <td>6.083</td>\n",
       "      <td>6.198</td>\n",
       "      <td>6.226</td>\n",
       "      <td>6.484</td>\n",
       "      <td>6.292</td>\n",
       "      <td>6.424</td>\n",
       "      <td>6.524</td>\n",
       "      <td>6.466</td>\n",
       "      <td>6.539</td>\n",
       "      <td>6.255</td>\n",
       "      <td>6.372</td>\n",
       "      <td>6.517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          gru.0                      gru.1                      gru.2                      gru.3                      gru.4                    \n",
       "           best   swalast  swabest    best   swalast  swabest    best   swalast  swabest    best   swalast  swabest    best   swalast  swabest \n",
       "20170103    0.159    0.158    0.158    0.161    0.142    0.156    0.154    0.149    0.156    0.158    0.153    0.155    0.157    0.152    0.155\n",
       "20170704    0.118    0.119    0.117    0.120    0.122    0.121    0.119    0.121    0.119    0.113    0.118    0.116    0.117    0.121    0.118\n",
       "20171226    0.102    0.102    0.103    0.106    0.108    0.108    0.108    0.108    0.108    0.108    0.108    0.109    0.105    0.104    0.104\n",
       "20180627    0.157    0.158    0.157    0.153    0.153    0.152    0.152    0.158    0.156    0.154    0.156    0.155    0.153    0.153    0.153\n",
       "20181220    0.121    0.103    0.110    0.111    0.104    0.107    0.114    0.106    0.109    0.100    0.101    0.102    0.104    0.103    0.104\n",
       "20190624    0.054    0.051    0.044    0.071    0.076    0.076    0.092    0.087    0.088    0.094    0.089    0.088    0.086    0.086    0.092\n",
       "20191217    0.088    0.088    0.088    0.089    0.090    0.089    0.083    0.084    0.085    0.090    0.090    0.089    0.093    0.090    0.092\n",
       "20200617    0.077    0.077    0.081    0.077    0.073    0.077    0.081    0.080    0.078    0.085    0.084    0.083    0.082    0.080    0.084\n",
       "20201214    0.051    0.056    0.062    0.057    0.057    0.063    0.070    0.068    0.068    0.068    0.068    0.070    0.067    0.069    0.069\n",
       "20210615    0.053    0.058    0.054    0.059    0.061    0.062    0.056    0.057    0.058    0.072    0.073    0.070    0.070    0.065    0.067\n",
       "20211209    0.092    0.083    0.092    0.088    0.083    0.083    0.097    0.093    0.096    0.087    0.079    0.086    0.078    0.079    0.083\n",
       "20220613    0.069    0.069    0.066    0.062    0.068    0.068    0.071    0.059    0.067    0.096    0.075    0.076    0.085    0.086    0.087\n",
       "20221206    0.077    0.066    0.075    0.099    0.088    0.091    0.098    0.090    0.095    0.089    0.086    0.090    0.060    0.068    0.074\n",
       "20230606    0.107    0.107    0.106    0.103    0.107    0.112    0.113    0.108    0.112    0.123    0.124    0.124    0.115    0.112    0.119\n",
       "20231201    0.102    0.105    0.102    0.103    0.111    0.104    0.084    0.101    0.090    0.108    0.111    0.112    0.113    0.105    0.110\n",
       "20240604    0.046    0.041    0.050    0.051    0.050    0.059    0.048    0.040    0.046    0.065    0.063    0.070    0.050    0.054    0.062\n",
       "Avg         0.094    0.093    0.094    0.097    0.096    0.097    0.099    0.097    0.098    0.102    0.100    0.101    0.098    0.097    0.100\n",
       "Sum       172.683  169.475  171.493  176.657  174.800  178.270  180.774  177.743  179.649  187.284  183.904  185.208  179.908  178.368  183.330\n",
       "Std         0.077    0.077    0.076    0.078    0.075    0.077    0.075    0.076    0.075    0.077    0.076    0.076    0.077    0.075    0.075\n",
       "T          52.197   51.159   52.684   53.134   54.138   54.380   56.636   54.957   56.109   56.987   56.475   57.115   54.634   55.654   56.925\n",
       "IR          5.976    5.857    6.032    6.083    6.198    6.226    6.484    6.292    6.424    6.524    6.466    6.539    6.255    6.372    6.517"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-08-29 10:48:10|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 151.3 Secs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results are saved to /home/mengkjin/Workspace/learndl/models/gru_2year_day/detailed_analysis/test.xlsx\n",
      "trade , day , 20210210 entry has columns [['limit']] all-NA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num</th>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>best</th>\n",
       "      <td>-1.665%</td>\n",
       "      <td>-0.544%</td>\n",
       "      <td>-0.352%</td>\n",
       "      <td>-0.229%</td>\n",
       "      <td>-0.100%</td>\n",
       "      <td>-0.105%</td>\n",
       "      <td>-0.028%</td>\n",
       "      <td>0.032%</td>\n",
       "      <td>0.029%</td>\n",
       "      <td>0.102%</td>\n",
       "      <td>0.127%</td>\n",
       "      <td>0.163%</td>\n",
       "      <td>0.189%</td>\n",
       "      <td>0.265%</td>\n",
       "      <td>0.274%</td>\n",
       "      <td>0.304%</td>\n",
       "      <td>0.318%</td>\n",
       "      <td>0.379%</td>\n",
       "      <td>0.405%</td>\n",
       "      <td>0.493%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swabest</th>\n",
       "      <td>-1.684%</td>\n",
       "      <td>-0.655%</td>\n",
       "      <td>-0.398%</td>\n",
       "      <td>-0.226%</td>\n",
       "      <td>-0.211%</td>\n",
       "      <td>-0.083%</td>\n",
       "      <td>0.006%</td>\n",
       "      <td>-0.012%</td>\n",
       "      <td>0.025%</td>\n",
       "      <td>0.121%</td>\n",
       "      <td>0.147%</td>\n",
       "      <td>0.186%</td>\n",
       "      <td>0.220%</td>\n",
       "      <td>0.252%</td>\n",
       "      <td>0.294%</td>\n",
       "      <td>0.342%</td>\n",
       "      <td>0.368%</td>\n",
       "      <td>0.434%</td>\n",
       "      <td>0.424%</td>\n",
       "      <td>0.510%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swalast</th>\n",
       "      <td>-1.668%</td>\n",
       "      <td>-0.582%</td>\n",
       "      <td>-0.374%</td>\n",
       "      <td>-0.255%</td>\n",
       "      <td>-0.178%</td>\n",
       "      <td>-0.047%</td>\n",
       "      <td>0.008%</td>\n",
       "      <td>0.026%</td>\n",
       "      <td>0.081%</td>\n",
       "      <td>0.150%</td>\n",
       "      <td>0.134%</td>\n",
       "      <td>0.177%</td>\n",
       "      <td>0.204%</td>\n",
       "      <td>0.273%</td>\n",
       "      <td>0.269%</td>\n",
       "      <td>0.342%</td>\n",
       "      <td>0.325%</td>\n",
       "      <td>0.335%</td>\n",
       "      <td>0.384%</td>\n",
       "      <td>0.455%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>best</th>\n",
       "      <td>-1.645%</td>\n",
       "      <td>-0.643%</td>\n",
       "      <td>-0.301%</td>\n",
       "      <td>-0.152%</td>\n",
       "      <td>-0.153%</td>\n",
       "      <td>-0.099%</td>\n",
       "      <td>-0.052%</td>\n",
       "      <td>0.028%</td>\n",
       "      <td>0.005%</td>\n",
       "      <td>0.071%</td>\n",
       "      <td>0.149%</td>\n",
       "      <td>0.169%</td>\n",
       "      <td>0.219%</td>\n",
       "      <td>0.199%</td>\n",
       "      <td>0.272%</td>\n",
       "      <td>0.324%</td>\n",
       "      <td>0.380%</td>\n",
       "      <td>0.401%</td>\n",
       "      <td>0.431%</td>\n",
       "      <td>0.455%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swabest</th>\n",
       "      <td>-1.680%</td>\n",
       "      <td>-0.639%</td>\n",
       "      <td>-0.301%</td>\n",
       "      <td>-0.193%</td>\n",
       "      <td>-0.106%</td>\n",
       "      <td>-0.125%</td>\n",
       "      <td>-0.027%</td>\n",
       "      <td>0.023%</td>\n",
       "      <td>0.053%</td>\n",
       "      <td>0.035%</td>\n",
       "      <td>0.168%</td>\n",
       "      <td>0.179%</td>\n",
       "      <td>0.161%</td>\n",
       "      <td>0.223%</td>\n",
       "      <td>0.261%</td>\n",
       "      <td>0.313%</td>\n",
       "      <td>0.373%</td>\n",
       "      <td>0.390%</td>\n",
       "      <td>0.472%</td>\n",
       "      <td>0.479%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swalast</th>\n",
       "      <td>-1.647%</td>\n",
       "      <td>-0.635%</td>\n",
       "      <td>-0.347%</td>\n",
       "      <td>-0.242%</td>\n",
       "      <td>-0.142%</td>\n",
       "      <td>-0.093%</td>\n",
       "      <td>-0.021%</td>\n",
       "      <td>0.028%</td>\n",
       "      <td>0.018%</td>\n",
       "      <td>0.089%</td>\n",
       "      <td>0.142%</td>\n",
       "      <td>0.198%</td>\n",
       "      <td>0.174%</td>\n",
       "      <td>0.234%</td>\n",
       "      <td>0.304%</td>\n",
       "      <td>0.328%</td>\n",
       "      <td>0.379%</td>\n",
       "      <td>0.397%</td>\n",
       "      <td>0.447%</td>\n",
       "      <td>0.452%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>best</th>\n",
       "      <td>-1.799%</td>\n",
       "      <td>-0.652%</td>\n",
       "      <td>-0.340%</td>\n",
       "      <td>-0.200%</td>\n",
       "      <td>-0.148%</td>\n",
       "      <td>-0.032%</td>\n",
       "      <td>0.044%</td>\n",
       "      <td>0.066%</td>\n",
       "      <td>0.078%</td>\n",
       "      <td>0.056%</td>\n",
       "      <td>0.124%</td>\n",
       "      <td>0.169%</td>\n",
       "      <td>0.181%</td>\n",
       "      <td>0.230%</td>\n",
       "      <td>0.303%</td>\n",
       "      <td>0.356%</td>\n",
       "      <td>0.330%</td>\n",
       "      <td>0.434%</td>\n",
       "      <td>0.438%</td>\n",
       "      <td>0.417%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swabest</th>\n",
       "      <td>-1.776%</td>\n",
       "      <td>-0.674%</td>\n",
       "      <td>-0.352%</td>\n",
       "      <td>-0.264%</td>\n",
       "      <td>-0.076%</td>\n",
       "      <td>-0.037%</td>\n",
       "      <td>-0.029%</td>\n",
       "      <td>0.049%</td>\n",
       "      <td>0.062%</td>\n",
       "      <td>0.064%</td>\n",
       "      <td>0.156%</td>\n",
       "      <td>0.193%</td>\n",
       "      <td>0.190%</td>\n",
       "      <td>0.264%</td>\n",
       "      <td>0.294%</td>\n",
       "      <td>0.350%</td>\n",
       "      <td>0.324%</td>\n",
       "      <td>0.433%</td>\n",
       "      <td>0.443%</td>\n",
       "      <td>0.440%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swalast</th>\n",
       "      <td>-1.793%</td>\n",
       "      <td>-0.643%</td>\n",
       "      <td>-0.373%</td>\n",
       "      <td>-0.226%</td>\n",
       "      <td>-0.183%</td>\n",
       "      <td>-0.053%</td>\n",
       "      <td>-0.009%</td>\n",
       "      <td>0.048%</td>\n",
       "      <td>0.044%</td>\n",
       "      <td>0.073%</td>\n",
       "      <td>0.116%</td>\n",
       "      <td>0.133%</td>\n",
       "      <td>0.228%</td>\n",
       "      <td>0.301%</td>\n",
       "      <td>0.337%</td>\n",
       "      <td>0.353%</td>\n",
       "      <td>0.363%</td>\n",
       "      <td>0.462%</td>\n",
       "      <td>0.427%</td>\n",
       "      <td>0.445%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">3</th>\n",
       "      <th>best</th>\n",
       "      <td>-1.702%</td>\n",
       "      <td>-0.669%</td>\n",
       "      <td>-0.386%</td>\n",
       "      <td>-0.296%</td>\n",
       "      <td>-0.213%</td>\n",
       "      <td>-0.095%</td>\n",
       "      <td>-0.055%</td>\n",
       "      <td>-0.007%</td>\n",
       "      <td>0.079%</td>\n",
       "      <td>0.122%</td>\n",
       "      <td>0.122%</td>\n",
       "      <td>0.200%</td>\n",
       "      <td>0.197%</td>\n",
       "      <td>0.317%</td>\n",
       "      <td>0.282%</td>\n",
       "      <td>0.314%</td>\n",
       "      <td>0.366%</td>\n",
       "      <td>0.417%</td>\n",
       "      <td>0.500%</td>\n",
       "      <td>0.567%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swabest</th>\n",
       "      <td>-1.769%</td>\n",
       "      <td>-0.668%</td>\n",
       "      <td>-0.412%</td>\n",
       "      <td>-0.279%</td>\n",
       "      <td>-0.224%</td>\n",
       "      <td>-0.119%</td>\n",
       "      <td>-0.075%</td>\n",
       "      <td>0.014%</td>\n",
       "      <td>0.035%</td>\n",
       "      <td>0.092%</td>\n",
       "      <td>0.195%</td>\n",
       "      <td>0.163%</td>\n",
       "      <td>0.184%</td>\n",
       "      <td>0.284%</td>\n",
       "      <td>0.337%</td>\n",
       "      <td>0.339%</td>\n",
       "      <td>0.407%</td>\n",
       "      <td>0.471%</td>\n",
       "      <td>0.506%</td>\n",
       "      <td>0.580%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swalast</th>\n",
       "      <td>-1.711%</td>\n",
       "      <td>-0.687%</td>\n",
       "      <td>-0.426%</td>\n",
       "      <td>-0.309%</td>\n",
       "      <td>-0.196%</td>\n",
       "      <td>-0.094%</td>\n",
       "      <td>-0.058%</td>\n",
       "      <td>0.021%</td>\n",
       "      <td>0.085%</td>\n",
       "      <td>0.091%</td>\n",
       "      <td>0.134%</td>\n",
       "      <td>0.190%</td>\n",
       "      <td>0.184%</td>\n",
       "      <td>0.265%</td>\n",
       "      <td>0.296%</td>\n",
       "      <td>0.391%</td>\n",
       "      <td>0.381%</td>\n",
       "      <td>0.425%</td>\n",
       "      <td>0.503%</td>\n",
       "      <td>0.575%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4</th>\n",
       "      <th>best</th>\n",
       "      <td>-1.735%</td>\n",
       "      <td>-0.705%</td>\n",
       "      <td>-0.399%</td>\n",
       "      <td>-0.256%</td>\n",
       "      <td>-0.169%</td>\n",
       "      <td>-0.094%</td>\n",
       "      <td>-0.055%</td>\n",
       "      <td>0.027%</td>\n",
       "      <td>0.102%</td>\n",
       "      <td>0.129%</td>\n",
       "      <td>0.105%</td>\n",
       "      <td>0.217%</td>\n",
       "      <td>0.228%</td>\n",
       "      <td>0.225%</td>\n",
       "      <td>0.290%</td>\n",
       "      <td>0.330%</td>\n",
       "      <td>0.385%</td>\n",
       "      <td>0.483%</td>\n",
       "      <td>0.474%</td>\n",
       "      <td>0.476%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swabest</th>\n",
       "      <td>-1.773%</td>\n",
       "      <td>-0.735%</td>\n",
       "      <td>-0.385%</td>\n",
       "      <td>-0.269%</td>\n",
       "      <td>-0.160%</td>\n",
       "      <td>-0.098%</td>\n",
       "      <td>0.003%</td>\n",
       "      <td>-0.009%</td>\n",
       "      <td>0.105%</td>\n",
       "      <td>0.095%</td>\n",
       "      <td>0.187%</td>\n",
       "      <td>0.191%</td>\n",
       "      <td>0.255%</td>\n",
       "      <td>0.225%</td>\n",
       "      <td>0.306%</td>\n",
       "      <td>0.349%</td>\n",
       "      <td>0.393%</td>\n",
       "      <td>0.423%</td>\n",
       "      <td>0.490%</td>\n",
       "      <td>0.465%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swalast</th>\n",
       "      <td>-1.779%</td>\n",
       "      <td>-0.693%</td>\n",
       "      <td>-0.397%</td>\n",
       "      <td>-0.263%</td>\n",
       "      <td>-0.119%</td>\n",
       "      <td>-0.073%</td>\n",
       "      <td>-0.021%</td>\n",
       "      <td>0.027%</td>\n",
       "      <td>0.111%</td>\n",
       "      <td>0.108%</td>\n",
       "      <td>0.137%</td>\n",
       "      <td>0.181%</td>\n",
       "      <td>0.195%</td>\n",
       "      <td>0.284%</td>\n",
       "      <td>0.285%</td>\n",
       "      <td>0.364%</td>\n",
       "      <td>0.381%</td>\n",
       "      <td>0.445%</td>\n",
       "      <td>0.441%</td>\n",
       "      <td>0.441%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "group          1        2        3        4        5        6        7        8        9       10      11      12      13      14      15      16      17      18      19      20  \n",
       "num type                                                                                                                                                                           \n",
       "0   best     -1.665%  -0.544%  -0.352%  -0.229%  -0.100%  -0.105%  -0.028%   0.032%  0.029%  0.102%  0.127%  0.163%  0.189%  0.265%  0.274%  0.304%  0.318%  0.379%  0.405%  0.493%\n",
       "    swabest  -1.684%  -0.655%  -0.398%  -0.226%  -0.211%  -0.083%   0.006%  -0.012%  0.025%  0.121%  0.147%  0.186%  0.220%  0.252%  0.294%  0.342%  0.368%  0.434%  0.424%  0.510%\n",
       "    swalast  -1.668%  -0.582%  -0.374%  -0.255%  -0.178%  -0.047%   0.008%   0.026%  0.081%  0.150%  0.134%  0.177%  0.204%  0.273%  0.269%  0.342%  0.325%  0.335%  0.384%  0.455%\n",
       "1   best     -1.645%  -0.643%  -0.301%  -0.152%  -0.153%  -0.099%  -0.052%   0.028%  0.005%  0.071%  0.149%  0.169%  0.219%  0.199%  0.272%  0.324%  0.380%  0.401%  0.431%  0.455%\n",
       "    swabest  -1.680%  -0.639%  -0.301%  -0.193%  -0.106%  -0.125%  -0.027%   0.023%  0.053%  0.035%  0.168%  0.179%  0.161%  0.223%  0.261%  0.313%  0.373%  0.390%  0.472%  0.479%\n",
       "    swalast  -1.647%  -0.635%  -0.347%  -0.242%  -0.142%  -0.093%  -0.021%   0.028%  0.018%  0.089%  0.142%  0.198%  0.174%  0.234%  0.304%  0.328%  0.379%  0.397%  0.447%  0.452%\n",
       "2   best     -1.799%  -0.652%  -0.340%  -0.200%  -0.148%  -0.032%   0.044%   0.066%  0.078%  0.056%  0.124%  0.169%  0.181%  0.230%  0.303%  0.356%  0.330%  0.434%  0.438%  0.417%\n",
       "    swabest  -1.776%  -0.674%  -0.352%  -0.264%  -0.076%  -0.037%  -0.029%   0.049%  0.062%  0.064%  0.156%  0.193%  0.190%  0.264%  0.294%  0.350%  0.324%  0.433%  0.443%  0.440%\n",
       "    swalast  -1.793%  -0.643%  -0.373%  -0.226%  -0.183%  -0.053%  -0.009%   0.048%  0.044%  0.073%  0.116%  0.133%  0.228%  0.301%  0.337%  0.353%  0.363%  0.462%  0.427%  0.445%\n",
       "3   best     -1.702%  -0.669%  -0.386%  -0.296%  -0.213%  -0.095%  -0.055%  -0.007%  0.079%  0.122%  0.122%  0.200%  0.197%  0.317%  0.282%  0.314%  0.366%  0.417%  0.500%  0.567%\n",
       "    swabest  -1.769%  -0.668%  -0.412%  -0.279%  -0.224%  -0.119%  -0.075%   0.014%  0.035%  0.092%  0.195%  0.163%  0.184%  0.284%  0.337%  0.339%  0.407%  0.471%  0.506%  0.580%\n",
       "    swalast  -1.711%  -0.687%  -0.426%  -0.309%  -0.196%  -0.094%  -0.058%   0.021%  0.085%  0.091%  0.134%  0.190%  0.184%  0.265%  0.296%  0.391%  0.381%  0.425%  0.503%  0.575%\n",
       "4   best     -1.735%  -0.705%  -0.399%  -0.256%  -0.169%  -0.094%  -0.055%   0.027%  0.102%  0.129%  0.105%  0.217%  0.228%  0.225%  0.290%  0.330%  0.385%  0.483%  0.474%  0.476%\n",
       "    swabest  -1.773%  -0.735%  -0.385%  -0.269%  -0.160%  -0.098%   0.003%  -0.009%  0.105%  0.095%  0.187%  0.191%  0.255%  0.225%  0.306%  0.349%  0.393%  0.423%  0.490%  0.465%\n",
       "    swalast  -1.779%  -0.693%  -0.397%  -0.263%  -0.119%  -0.073%  -0.021%   0.027%  0.111%  0.108%  0.137%  0.181%  0.195%  0.284%  0.285%  0.364%  0.381%  0.445%  0.441%  0.441%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped Return Results are saved to /home/mengkjin/Workspace/learndl/models/gru_2year_day/detailed_analysis/group.xlsx\n",
      "trade , day , 20210210 entry has columns [['limit']] all-NA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade , day , 20210210 entry has columns [['limit']] all-NA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade , day , 20210210 entry has columns [['limit']] all-NA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade , day , 20210210 entry has columns [['limit']] all-NA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade , day , 20210210 entry has columns [['limit']] all-NA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade , day , 20210210 entry has columns [['limit']] all-NA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade , day , 20210210 entry has columns [['limit']] all-NA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade , day , 20210210 entry has columns [['limit']] all-NA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade , day , 20210210 entry has columns [['limit']] all-NA\n",
      "Not accurate but assessed as success at 20220726 for [swabest]!\n",
      "Accuarcy(Is Accurate=False,\n",
      "          lin_ub_bias=(√)-1.8744555128069607e-06,\n",
      "          lin_lb_bias=(X)-2.8298810165194865e-06,\n",
      "          bnd_ub_bias=(√)-4.6584850733888405e-09,\n",
      "          bnd_lb_bias=(√)0.0,\n",
      "          excess_turn=(√)-1.6764656923240473e-06\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>factor_name</th>\n",
       "      <th>benchmark</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>pf</th>\n",
       "      <th>bm</th>\n",
       "      <th>excess</th>\n",
       "      <th>annualized</th>\n",
       "      <th>mdd</th>\n",
       "      <th>te</th>\n",
       "      <th>ir</th>\n",
       "      <th>calmar</th>\n",
       "      <th>turnover</th>\n",
       "      <th>mdd_period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>best</td>\n",
       "      <td>csi300</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240726</td>\n",
       "      <td>35.23%</td>\n",
       "      <td>20.59%</td>\n",
       "      <td>14.65%</td>\n",
       "      <td>1.31%</td>\n",
       "      <td>15.95%</td>\n",
       "      <td>4.35%</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.082</td>\n",
       "      <td>147.550</td>\n",
       "      <td>20190603-20210804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>best</td>\n",
       "      <td>csi500</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240726</td>\n",
       "      <td>59.13%</td>\n",
       "      <td>-17.81%</td>\n",
       "      <td>76.94%</td>\n",
       "      <td>8.87%</td>\n",
       "      <td>12.96%</td>\n",
       "      <td>7.13%</td>\n",
       "      <td>1.244</td>\n",
       "      <td>0.685</td>\n",
       "      <td>147.731</td>\n",
       "      <td>20210408-20210915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best</td>\n",
       "      <td>csi1000</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240726</td>\n",
       "      <td>54.31%</td>\n",
       "      <td>-41.69%</td>\n",
       "      <td>96.00%</td>\n",
       "      <td>13.20%</td>\n",
       "      <td>18.62%</td>\n",
       "      <td>6.80%</td>\n",
       "      <td>1.942</td>\n",
       "      <td>0.709</td>\n",
       "      <td>147.909</td>\n",
       "      <td>20210415-20210915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>swabest</td>\n",
       "      <td>csi300</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240726</td>\n",
       "      <td>28.90%</td>\n",
       "      <td>20.59%</td>\n",
       "      <td>8.32%</td>\n",
       "      <td>0.66%</td>\n",
       "      <td>19.14%</td>\n",
       "      <td>4.42%</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.034</td>\n",
       "      <td>147.566</td>\n",
       "      <td>20181228-20210804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>swabest</td>\n",
       "      <td>csi500</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240726</td>\n",
       "      <td>61.29%</td>\n",
       "      <td>-17.81%</td>\n",
       "      <td>79.10%</td>\n",
       "      <td>9.05%</td>\n",
       "      <td>13.08%</td>\n",
       "      <td>6.99%</td>\n",
       "      <td>1.294</td>\n",
       "      <td>0.692</td>\n",
       "      <td>147.724</td>\n",
       "      <td>20210408-20210915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>swabest</td>\n",
       "      <td>csi1000</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240726</td>\n",
       "      <td>63.37%</td>\n",
       "      <td>-41.69%</td>\n",
       "      <td>105.06%</td>\n",
       "      <td>14.07%</td>\n",
       "      <td>18.15%</td>\n",
       "      <td>6.78%</td>\n",
       "      <td>2.075</td>\n",
       "      <td>0.775</td>\n",
       "      <td>147.941</td>\n",
       "      <td>20210415-20210915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>swalast</td>\n",
       "      <td>csi300</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240726</td>\n",
       "      <td>24.63%</td>\n",
       "      <td>20.59%</td>\n",
       "      <td>4.04%</td>\n",
       "      <td>0.20%</td>\n",
       "      <td>20.95%</td>\n",
       "      <td>4.39%</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.010</td>\n",
       "      <td>147.569</td>\n",
       "      <td>20181228-20210804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>swalast</td>\n",
       "      <td>csi500</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240726</td>\n",
       "      <td>52.17%</td>\n",
       "      <td>-17.81%</td>\n",
       "      <td>69.98%</td>\n",
       "      <td>8.23%</td>\n",
       "      <td>13.34%</td>\n",
       "      <td>6.92%</td>\n",
       "      <td>1.189</td>\n",
       "      <td>0.617</td>\n",
       "      <td>147.750</td>\n",
       "      <td>20201110-20210915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>swalast</td>\n",
       "      <td>csi1000</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240726</td>\n",
       "      <td>37.13%</td>\n",
       "      <td>-41.69%</td>\n",
       "      <td>78.82%</td>\n",
       "      <td>11.45%</td>\n",
       "      <td>17.59%</td>\n",
       "      <td>6.67%</td>\n",
       "      <td>1.717</td>\n",
       "      <td>0.651</td>\n",
       "      <td>147.948</td>\n",
       "      <td>20210415-20210915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  factor_name benchmark    start      end      pf      bm     excess  annualized   mdd     te     ir   calmar turnover     mdd_period    \n",
       "1       best     csi300  20170104  20240726  35.23%   20.59%   14.65%    1.31%    15.95%  4.35%  0.301  0.082  147.550  20190603-20210804\n",
       "2       best     csi500  20170104  20240726  59.13%  -17.81%   76.94%    8.87%    12.96%  7.13%  1.244  0.685  147.731  20210408-20210915\n",
       "0       best    csi1000  20170104  20240726  54.31%  -41.69%   96.00%   13.20%    18.62%  6.80%  1.942  0.709  147.909  20210415-20210915\n",
       "4    swabest     csi300  20170104  20240726  28.90%   20.59%    8.32%    0.66%    19.14%  4.42%  0.149  0.034  147.566  20181228-20210804\n",
       "5    swabest     csi500  20170104  20240726  61.29%  -17.81%   79.10%    9.05%    13.08%  6.99%  1.294  0.692  147.724  20210408-20210915\n",
       "3    swabest    csi1000  20170104  20240726  63.37%  -41.69%  105.06%   14.07%    18.15%  6.78%  2.075  0.775  147.941  20210415-20210915\n",
       "7    swalast     csi300  20170104  20240726  24.63%   20.59%    4.04%    0.20%    20.95%  4.39%  0.046  0.010  147.569  20181228-20210804\n",
       "8    swalast     csi500  20170104  20240726  52.17%  -17.81%   69.98%    8.23%    13.34%  6.92%  1.189  0.617  147.750  20201110-20210915\n",
       "6    swalast    csi1000  20170104  20240726  37.13%  -41.69%   78.82%   11.45%    17.59%  6.67%  1.717  0.651  147.948  20210415-20210915"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-08-29 11:29:43|MOD:time        |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 2 Hours 19 Minutes 12.6 Seconds\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytic datas are saved to /home/mengkjin/Workspace/learndl/models/gru_2year_day/detailed_analysis/perf.xlsx\n",
      "Analytic plots are saved to /home/mengkjin/Workspace/learndl/models/gru_2year_day/detailed_analysis/plot.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<src.model.trainer.trainers.net.NetTrainer at 0x78302ccb9c60>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.api import Trainer  \n",
    "app = Trainer.initialize(stage = 0 , resume = 0 , checkname= 1)\n",
    "app.go()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.api import HiddenExtractor\n",
    "extractor = HiddenExtractor('resnet_gru_30m' , model_nums=[1] , model_types=['best'])\n",
    "extractor.extract_hidden('update' , deploy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.api import DataAPI , Trainer\n",
    "DataAPI.reconstruct_train_data()\n",
    "Trainer.update_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.api import Trainer\n",
    "Trainer.update_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
