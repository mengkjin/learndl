{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 2.1.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available() , torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device name: NVIDIA GeForce RTX 4090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[44m24-07-13 20:01:23|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-07-13 20:01:23|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Sat Jul 13 20:01:23 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Process Queue : Data + Fit + Test\n",
      "--Start Training New!\n",
      "--Model_name is set to resnet_gru_avg.3!\n",
      "Callback : ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True) , reset optimizer on some epoch (can speedup scheduler)\n",
      "Callback : CallbackTimer(verbosity=2) , record time cost of callback hooks\n",
      "Callback : EarlyStoppage(patience=20) , stop fitting when validation score cease to improve\n",
      "Callback : ValidationConverge(patience=5,eps=1e-05) , stop fitting when valid_score converge\n",
      "Callback : EarlyExitRetrain(earliest=10,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100, 1]) , retrain with new lr if fitting stopped too early\n",
      "Callback : NanLossRetrain(max_attempt=4) , retrain if fitting encounters nan loss\n",
      "Callback : BatchDisplay(verbosity=2) , display batch progress bar\n",
      "Callback : StatusDisplay(verbosity=2) , display epoch and event information\n",
      "Callback : DetailedAlphaAnalysis(which_alpha=avg) , record and concat each model to Alpha model instance\n",
      "{'random_seed': None,\n",
      " 'model_name': 'resnet_gru_avg.3',\n",
      " 'model_module': 'resnet_gru',\n",
      " 'model_data_type': '30m',\n",
      " 'model_types': ['best', 'swalast', 'swabest'],\n",
      " 'labels': ['std_lag1_10', 'rtn_lag1_10'],\n",
      " 'beg_date': 20170103,\n",
      " 'end_date': 99991231,\n",
      " 'sample_method': 'sequential',\n",
      " 'shuffle_option': 'epoch',\n",
      " 'lgbm_ensembler': False}\n",
      "{'hidden_dim': [32],\n",
      " 'seqlens': [{'day': 30, '30m': 30}],\n",
      " 'dropout': [0.1],\n",
      " 'enc_in': [True],\n",
      " 'enc_att': [True],\n",
      " 'rnn_type': ['gru'],\n",
      " 'rnn_att': [True],\n",
      " 'rnn_layers': [2],\n",
      " 'dec_mlp_layers': [2],\n",
      " 'num_output': [1],\n",
      " 'which_output': [0],\n",
      " 'kernel_size': [3],\n",
      " 'resnet_blocks': [1, 2],\n",
      " 'enc_in_dim': [16],\n",
      " 'hidden_as_factor': [True],\n",
      " 'ordered_param_group': [False]}\n",
      "try using /home/mengkjin/Workspace/learndl/data/DataSet/30m.20240620.pt , success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-07-13 20:01:28|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 4.2 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-07-13 20:01:28|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Sat Jul 13 20:01:28 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-13 20:01:28|MOD:mod         |\u001b[0m: \u001b[1m\u001b[34mFirst Iterance: (20170103 , 0)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [30m] : {'divlast': False, 'histnorm': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengkjin/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score function of [spearman] calculated and success!\n",
      "loss function of [pearson] calculated and success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mFirstBite Ep#  0 : loss  1.00233, train-0.00096, valid-0.09897, best-0.0990, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.88359, train 0.13135, valid 0.13617, best 0.1362, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.85394, train 0.16120, valid 0.08951, best 0.1362, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.83339, train 0.18126, valid 0.09973, best 0.1362, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.83302, train 0.18037, valid 0.09163, best 0.1362, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.81604, train 0.19832, valid 0.10390, best 0.1485, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 30 : loss  0.81323, train 0.20136, valid 0.10439, best 0.1485, lr1.6e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 35 : loss  0.81122, train 0.20228, valid 0.10576, best 0.1485, lr3.1e-04\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 39 , effective at epoch 40\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 40 : loss  0.80585, train 0.20771, valid 0.10374, best 0.1485, lr1.3e-03\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-13 20:06:38|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #0 @20170103|FirstBite Ep# 43 EarlyStop|Train 0.1879 Valid 0.1372 BestVal 0.1372|Cost  5.0Min,  6.8Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  1.00299, train-0.00561, valid-0.08016, best-0.0802, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.88905, train 0.12604, valid 0.13667, best 0.1541, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.85730, train 0.15845, valid 0.13345, best 0.1541, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.83085, train 0.18517, valid 0.13012, best 0.1544, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.82791, train 0.18800, valid 0.14601, best 0.1544, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.81336, train 0.20072, valid 0.13391, best 0.1544, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 30 : loss  0.80958, train 0.20443, valid 0.12519, best 0.1544, lr1.6e-04\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-13 20:12:31|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #1 @20170103|FirstBite Ep# 34 EarlyStop|Train 0.2076 Valid 0.1232 BestVal 0.1232|Cost  5.9Min, 10.1Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-13 20:16:54|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #0 @20170704|FirstBite Ep# 33 EarlyStop|Train 0.2046 Valid 0.1414 BestVal 0.1414|Cost  4.2Min,  7.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-13 20:23:39|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #1 @20170704|FirstBite Ep# 36 EarlyStop|Train 0.2040 Valid 0.1411 BestVal 0.1411|Cost  6.8Min, 11.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-13 20:28:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #0 @20171226|FirstBite Ep# 35 EarlyStop|Train 0.2088 Valid 0.1059 BestVal 0.1059|Cost  4.8Min,  8.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-13 20:55:54|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #1 @20171226|Retrain#4 Ep#136 Valid Cvg|Train 0.0001 Valid 0.0080 BestVal 0.1555|Cost 27.3Min, 12.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-13 21:01:34|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #0 @20180627|FirstBite Ep# 38 EarlyStop|Train 0.2054 Valid 0.1286 BestVal 0.1286|Cost  5.5Min,  8.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-13 21:10:29|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #1 @20180627|FirstBite Ep# 41 EarlyStop|Train 0.2117 Valid 0.1476 BestVal 0.1476|Cost  8.9Min, 12.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-13 21:17:13|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #0 @20181220|FirstBite Ep# 41 EarlyStop|Train 0.2142 Valid 0.1053 BestVal 0.1053|Cost  6.5Min,  9.3Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-13 21:25:50|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #1 @20181220|FirstBite Ep# 36 EarlyStop|Train 0.2112 Valid 0.1066 BestVal 0.1066|Cost  8.6Min, 14.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-13 21:32:04|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #0 @20190624|FirstBite Ep# 36 EarlyStop|Train 0.2139 Valid 0.1038 BestVal 0.1038|Cost  6.1Min,  9.8Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-13 21:41:37|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #1 @20190624|FirstBite Ep# 38 EarlyStop|Train 0.2120 Valid 0.1303 BestVal 0.1303|Cost  9.5Min, 14.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-13 21:49:29|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #0 @20191217|FirstBite Ep# 43 EarlyStop|Train 0.1922 Valid 0.0850 BestVal 0.0850|Cost  7.7Min, 10.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-13 22:00:49|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #1 @20191217|FirstBite Ep# 41 EarlyStop|Train 0.2156 Valid 0.1139 BestVal 0.1139|Cost 11.3Min, 16.2Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-13 22:07:17|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #0 @20200617|FirstBite Ep# 33 EarlyStop|Train 0.2136 Valid 0.1022 BestVal 0.1022|Cost  6.3Min, 11.1Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-13 22:18:51|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #1 @20200617|FirstBite Ep# 39 EarlyStop|Train 0.2163 Valid 0.1168 BestVal 0.1168|Cost 11.6Min, 17.3Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-13 22:25:28|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #0 @20201214|FirstBite Ep# 32 EarlyStop|Train 0.2112 Valid 0.0992 BestVal 0.0992|Cost  6.4Min, 11.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-13 22:39:00|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #1 @20201214|FirstBite Ep# 44 EarlyStop|Train 0.1836 Valid 0.0858 BestVal 0.0858|Cost 13.5Min, 18.1Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-13 22:47:04|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #0 @20210615|FirstBite Ep# 38 EarlyStop|Train 0.2057 Valid 0.0733 BestVal 0.0733|Cost  7.9Min, 12.1Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-13 22:59:13|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #1 @20210615|FirstBite Ep# 38 EarlyStop|Train 0.2085 Valid 0.1020 BestVal 0.1020|Cost 12.1Min, 18.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-13 23:18:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #0 @20211209|Retrain#1 Ep# 90 EarlyStop|Train 0.1729 Valid 0.0854 BestVal 0.0882|Cost 19.2Min, 12.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-14 00:05:30|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #1 @20211209|Retrain#4 Ep#145 EarlyStop|Train 0.0136 Valid 0.0090 BestVal 0.0943|Cost 46.9Min, 19.3Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-14 00:31:17|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #0 @20220613|Retrain#4 Ep#118 EarlyStop|Train 0.0288 Valid 0.0280 BestVal 0.0887|Cost 25.6Min, 12.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-14 00:42:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #1 @20220613|FirstBite Ep# 33 EarlyStop|Train 0.1882 Valid 0.0556 BestVal 0.0556|Cost 11.2Min, 19.8Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-14 01:09:31|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #0 @20221206|Retrain#4 Ep#120 EarlyStop|Train-0.0054 Valid-0.0055 BestVal 0.1020|Cost 26.8Min, 13.3Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-14 01:29:43|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #1 @20221206|Retrain#1 Ep# 57 EarlyStop|Train 0.1519 Valid 0.0938 BestVal 0.1038|Cost 20.2Min, 20.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-14 01:55:57|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #0 @20230606|Retrain#4 Ep#112 EarlyStop|Train 0.0152 Valid 0.0080 BestVal 0.1095|Cost 26.0Min, 13.8Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-14 02:35:18|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #1 @20230606|Retrain#4 Ep#110 EarlyStop|Train-0.0002 Valid-0.0038 BestVal 0.1151|Cost 39.4Min, 21.3Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-14 03:05:25|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #0 @20231201|Retrain#4 Ep#125 EarlyStop|Train 0.0031 Valid-0.0061 BestVal 0.1181|Cost 29.9Min, 14.2Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-14 03:48:26|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #1 @20231201|Retrain#4 Ep#115 EarlyStop|Train 0.0009 Valid-0.0088 BestVal 0.1113|Cost 43.0Min, 22.2Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-14 03:57:02|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #0 @20240604|FirstBite Ep# 32 EarlyStop|Train 0.1674 Valid 0.0747 BestVal 0.0747|Cost  8.4Min, 15.3Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-14 04:37:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mresnet_gru_avg.3 #1 @20240604|Retrain#4 Ep#101 Valid Cvg|Train 0.0010 Valid-0.0059 BestVal 0.1160|Cost 40.5Min, 23.8Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-07-14 04:37:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Fit], Cost 8.6 Hours, 16.1 Min/model, 15.4 Sec/Epoch\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-07-14 04:37:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Sun Jul 14 04:37:32 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-14 04:40:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mTesting Mean Score(spearman):\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">resnet_gru.0</th>\n",
       "      <th colspan=\"3\" halign=\"left\">resnet_gru.1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>best</th>\n",
       "      <th>swalast</th>\n",
       "      <th>swabest</th>\n",
       "      <th>best</th>\n",
       "      <th>swalast</th>\n",
       "      <th>swabest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20170103</th>\n",
       "      <td>0.1488</td>\n",
       "      <td>0.0968</td>\n",
       "      <td>0.1006</td>\n",
       "      <td>0.1246</td>\n",
       "      <td>0.1065</td>\n",
       "      <td>0.1141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20170704</th>\n",
       "      <td>0.1331</td>\n",
       "      <td>0.1352</td>\n",
       "      <td>0.1391</td>\n",
       "      <td>0.1482</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>0.1426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20171226</th>\n",
       "      <td>0.1274</td>\n",
       "      <td>0.0925</td>\n",
       "      <td>0.0992</td>\n",
       "      <td>0.1031</td>\n",
       "      <td>0.1006</td>\n",
       "      <td>0.1055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180627</th>\n",
       "      <td>0.1469</td>\n",
       "      <td>0.1284</td>\n",
       "      <td>0.1189</td>\n",
       "      <td>0.1518</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>0.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181220</th>\n",
       "      <td>0.1042</td>\n",
       "      <td>0.1004</td>\n",
       "      <td>0.1002</td>\n",
       "      <td>0.1227</td>\n",
       "      <td>0.1302</td>\n",
       "      <td>0.1290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190624</th>\n",
       "      <td>0.0866</td>\n",
       "      <td>0.0622</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0926</td>\n",
       "      <td>0.0929</td>\n",
       "      <td>0.0943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20191217</th>\n",
       "      <td>0.1078</td>\n",
       "      <td>0.0805</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.1011</td>\n",
       "      <td>0.0911</td>\n",
       "      <td>0.0924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20200617</th>\n",
       "      <td>0.1007</td>\n",
       "      <td>0.0811</td>\n",
       "      <td>0.0928</td>\n",
       "      <td>0.1001</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.0971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201214</th>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.0871</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.0901</td>\n",
       "      <td>0.0779</td>\n",
       "      <td>0.0864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20210615</th>\n",
       "      <td>0.0773</td>\n",
       "      <td>0.0595</td>\n",
       "      <td>0.0569</td>\n",
       "      <td>0.0861</td>\n",
       "      <td>0.0863</td>\n",
       "      <td>0.0839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211209</th>\n",
       "      <td>0.1009</td>\n",
       "      <td>0.0993</td>\n",
       "      <td>0.1027</td>\n",
       "      <td>0.1029</td>\n",
       "      <td>0.0929</td>\n",
       "      <td>0.1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20220613</th>\n",
       "      <td>0.1297</td>\n",
       "      <td>0.1142</td>\n",
       "      <td>0.1164</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.0889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20221206</th>\n",
       "      <td>0.1061</td>\n",
       "      <td>0.0413</td>\n",
       "      <td>0.0908</td>\n",
       "      <td>0.1096</td>\n",
       "      <td>0.1074</td>\n",
       "      <td>0.1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20230606</th>\n",
       "      <td>0.1156</td>\n",
       "      <td>0.0849</td>\n",
       "      <td>0.0909</td>\n",
       "      <td>0.1188</td>\n",
       "      <td>0.0979</td>\n",
       "      <td>0.1022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20231201</th>\n",
       "      <td>0.0899</td>\n",
       "      <td>0.0634</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>0.0759</td>\n",
       "      <td>0.0776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20240604</th>\n",
       "      <td>0.1065</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.0659</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.0767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg</th>\n",
       "      <td>0.1111</td>\n",
       "      <td>0.0886</td>\n",
       "      <td>0.0946</td>\n",
       "      <td>0.1090</td>\n",
       "      <td>0.0983</td>\n",
       "      <td>0.1034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sum</th>\n",
       "      <td>202.3339</td>\n",
       "      <td>161.4170</td>\n",
       "      <td>172.2947</td>\n",
       "      <td>198.6053</td>\n",
       "      <td>179.0918</td>\n",
       "      <td>188.4214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Std</th>\n",
       "      <td>0.0785</td>\n",
       "      <td>0.0721</td>\n",
       "      <td>0.0732</td>\n",
       "      <td>0.0770</td>\n",
       "      <td>0.0751</td>\n",
       "      <td>0.0747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>60.4222</td>\n",
       "      <td>52.4269</td>\n",
       "      <td>55.1509</td>\n",
       "      <td>60.4440</td>\n",
       "      <td>55.8839</td>\n",
       "      <td>59.0823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IR</th>\n",
       "      <td>6.9347</td>\n",
       "      <td>6.0171</td>\n",
       "      <td>6.3297</td>\n",
       "      <td>6.9372</td>\n",
       "      <td>6.4138</td>\n",
       "      <td>6.7809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         resnet_gru.0                     resnet_gru.1                    \n",
       "                 best   swalast   swabest         best   swalast   swabest\n",
       "20170103       0.1488    0.0968    0.1006       0.1246    0.1065    0.1141\n",
       "20170704       0.1331    0.1352    0.1391       0.1482    0.1406    0.1426\n",
       "20171226       0.1274    0.0925    0.0992       0.1031    0.1006    0.1055\n",
       "20180627       0.1469    0.1284    0.1189       0.1518    0.1230    0.1302\n",
       "20181220       0.1042    0.1004    0.1002       0.1227    0.1302    0.1290\n",
       "20190624       0.0866    0.0622    0.0583       0.0926    0.0929    0.0943\n",
       "20191217       0.1078    0.0805    0.0833       0.1011    0.0911    0.0924\n",
       "20200617       0.1007    0.0811    0.0928       0.1001    0.0981    0.0971\n",
       "20201214       0.0915    0.0871    0.0974       0.0901    0.0779    0.0864\n",
       "20210615       0.0773    0.0595    0.0569       0.0861    0.0863    0.0839\n",
       "20211209       0.1009    0.0993    0.1027       0.1029    0.0929    0.1053\n",
       "20220613       0.1297    0.1142    0.1164       0.1032    0.0557    0.0889\n",
       "20221206       0.1061    0.0413    0.0908       0.1096    0.1074    0.1065\n",
       "20230606       0.1156    0.0849    0.0909       0.1188    0.0979    0.1022\n",
       "20231201       0.0899    0.0634    0.0694       0.0880    0.0759    0.0776\n",
       "20240604       0.1065    0.1000    0.1019       0.0659    0.0829    0.0767\n",
       "Avg            0.1111    0.0886    0.0946       0.1090    0.0983    0.1034\n",
       "Sum          202.3339  161.4170  172.2947     198.6053  179.0918  188.4214\n",
       "Std            0.0785    0.0721    0.0732       0.0770    0.0751    0.0747\n",
       "T             60.4222   52.4269   55.1509      60.4440   55.8839   59.0823\n",
       "IR             6.9347    6.0171    6.3297       6.9372    6.4138    6.7809"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-07-14 04:40:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 184.4 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-14 04:40:41|MOD:test        |\u001b[0m: \u001b[1m\u001b[34mPerforming Factor and FMP test!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PerfManager calc Finished!\n",
      "PerfManager plot Finished!\n",
      "Group optimization of 3 alphas , 3 benchmarks , 2 lags , 365 dates , (6570 opts) start!\n",
      "Done Optimize    0th [best.csi300      ] at 20170104 , time cost (ms) : {'parse_input': 17.97, 'solve': 94.49, 'output': 20.21}\n",
      "Done Optimize   50th [swalast.csi1000  ] at 20170118 , time cost (ms) : {'parse_input': 19.73, 'solve': 126.75, 'output': 23.53}\n",
      "Done Optimize  100th [swabest.csi500.1 ] at 20170215 , time cost (ms) : {'parse_input': 19.88, 'solve': 129.4, 'output': 20.63}\n",
      "Done Optimize  150th [swabest.csi300   ] at 20170308 , time cost (ms) : {'parse_input': 20.08, 'solve': 132.93, 'output': 25.51}\n",
      "Done Optimize  200th [best.csi1000     ] at 20170329 , time cost (ms) : {'parse_input': 19.97, 'solve': 119.66, 'output': 21.59}\n",
      "Done Optimize  250th [swalast.csi500.1 ] at 20170414 , time cost (ms) : {'parse_input': 20.27, 'solve': 118.15, 'output': 21.61}\n",
      "Done Optimize  300th [swalast.csi300   ] at 20170508 , time cost (ms) : {'parse_input': 19.94, 'solve': 144.58, 'output': 24.46}\n",
      "Done Optimize  350th [swabest.csi1000  ] at 20170531 , time cost (ms) : {'parse_input': 20.13, 'solve': 129.59, 'output': 23.63}\n",
      "Done Optimize  400th [best.csi500.1    ] at 20170621 , time cost (ms) : {'parse_input': 19.66, 'solve': 121.9, 'output': 21.84}\n",
      "Done Optimize  450th [best.csi300      ] at 20170712 , time cost (ms) : {'parse_input': 20.81, 'solve': 118.45, 'output': 20.55}\n",
      "Done Optimize  500th [swalast.csi1000  ] at 20170726 , time cost (ms) : {'parse_input': 22.33, 'solve': 138.97, 'output': 25.03}\n",
      "Done Optimize  550th [swabest.csi500.1 ] at 20170816 , time cost (ms) : {'parse_input': 20.95, 'solve': 111.92, 'output': 23.76}\n",
      "Done Optimize  600th [swabest.csi300   ] at 20170906 , time cost (ms) : {'parse_input': 21.58, 'solve': 112.53, 'output': 21.63}\n",
      "Done Optimize  650th [best.csi1000     ] at 20170927 , time cost (ms) : {'parse_input': 21.34, 'solve': 118.38, 'output': 23.43}\n",
      "Done Optimize  700th [swalast.csi500.1 ] at 20171018 , time cost (ms) : {'parse_input': 20.15, 'solve': 126.47, 'output': 22.11}\n",
      "Done Optimize  750th [swalast.csi300   ] at 20171108 , time cost (ms) : {'parse_input': 20.91, 'solve': 130.93, 'output': 24.11}\n",
      "Done Optimize  800th [swabest.csi1000  ] at 20171129 , time cost (ms) : {'parse_input': 20.41, 'solve': 138.82, 'output': 23.25}\n",
      "Done Optimize  850th [best.csi500.1    ] at 20171220 , time cost (ms) : {'parse_input': 20.9, 'solve': 130.56, 'output': 20.99}\n",
      "Done Optimize  900th [best.csi300      ] at 20180111 , time cost (ms) : {'parse_input': 21.72, 'solve': 136.19, 'output': 20.61}\n",
      "Done Optimize  950th [swalast.csi1000  ] at 20180125 , time cost (ms) : {'parse_input': 20.88, 'solve': 146.48, 'output': 21.21}\n",
      "Done Optimize 1000th [swabest.csi500.1 ] at 20180222 , time cost (ms) : {'parse_input': 20.78, 'solve': 132.75, 'output': 21.24}\n",
      "Done Optimize 1050th [swabest.csi300   ] at 20180315 , time cost (ms) : {'parse_input': 20.76, 'solve': 134.98, 'output': 20.59}\n",
      "Done Optimize 1100th [best.csi1000     ] at 20180409 , time cost (ms) : {'parse_input': 20.83, 'solve': 132.47, 'output': 21.17}\n",
      "Done Optimize 1150th [swalast.csi500.1 ] at 20180423 , time cost (ms) : {'parse_input': 21.03, 'solve': 135.22, 'output': 20.91}\n",
      "Done Optimize 1200th [swalast.csi300   ] at 20180516 , time cost (ms) : {'parse_input': 21.04, 'solve': 141.13, 'output': 20.51}\n",
      "Done Optimize 1250th [swabest.csi1000  ] at 20180606 , time cost (ms) : {'parse_input': 23.44, 'solve': 145.82, 'output': 21.3}\n",
      "Done Optimize 1300th [best.csi500.1    ] at 20180628 , time cost (ms) : {'parse_input': 21.34, 'solve': 152.66, 'output': 21.11}\n",
      "Not accurate but assessed as success at 20180712 for [swalast]!\n",
      "Accuarcy(Is Accurate=False,\n",
      "          lin_ub_bias=(√)-1.332367999928863e-06,\n",
      "          lin_lb_bias=(X)-2.098512348114312e-06,\n",
      "          bnd_ub_bias=(√)-5.439729083844824e-09,\n",
      "          bnd_lb_bias=(√)0.0,\n",
      "          excess_turn=(√)-1.2022482030471515e-06\n",
      "Done Optimize 1350th [best.csi300      ] at 20180719 , time cost (ms) : {'parse_input': 21.36, 'solve': 151.86, 'output': 23.41}\n",
      "Done Optimize 1400th [swalast.csi1000  ] at 20180802 , time cost (ms) : {'parse_input': 50.74, 'solve': 195.82, 'output': 45.15}\n",
      "Done Optimize 1450th [swabest.csi500.1 ] at 20180823 , time cost (ms) : {'parse_input': 20.96, 'solve': 162.4, 'output': 21.03}\n",
      "Done Optimize 1500th [swabest.csi300   ] at 20180913 , time cost (ms) : {'parse_input': 28.07, 'solve': 168.23, 'output': 20.34}\n",
      "Done Optimize 1550th [best.csi1000     ] at 20181012 , time cost (ms) : {'parse_input': 20.28, 'solve': 139.09, 'output': 22.38}\n",
      "Done Optimize 1600th [swalast.csi500.1 ] at 20181026 , time cost (ms) : {'parse_input': 21.46, 'solve': 151.8, 'output': 23.78}\n",
      "Done Optimize 1650th [swalast.csi300   ] at 20181116 , time cost (ms) : {'parse_input': 21.84, 'solve': 145.29, 'output': 22.77}\n",
      "Done Optimize 1700th [swabest.csi1000  ] at 20181207 , time cost (ms) : {'parse_input': 24.91, 'solve': 153.14, 'output': 23.21}\n",
      "Done Optimize 1750th [best.csi500.1    ] at 20181228 , time cost (ms) : {'parse_input': 21.57, 'solve': 159.24, 'output': 28.12}\n",
      "Done Optimize 1800th [best.csi300      ] at 20190122 , time cost (ms) : {'parse_input': 22.4, 'solve': 150.54, 'output': 28.2}\n",
      "Done Optimize 1850th [swalast.csi1000  ] at 20190212 , time cost (ms) : {'parse_input': 22.09, 'solve': 153.75, 'output': 22.82}\n",
      "Done Optimize 1900th [swabest.csi500.1 ] at 20190305 , time cost (ms) : {'parse_input': 21.87, 'solve': 136.29, 'output': 22.2}\n",
      "Done Optimize 1950th [swabest.csi300   ] at 20190326 , time cost (ms) : {'parse_input': 22.78, 'solve': 195.94, 'output': 21.44}\n",
      "Done Optimize 2000th [best.csi1000     ] at 20190417 , time cost (ms) : {'parse_input': 21.53, 'solve': 151.35, 'output': 24.34}\n",
      "Done Optimize 2050th [swalast.csi500.1 ] at 20190506 , time cost (ms) : {'parse_input': 23.96, 'solve': 149.64, 'output': 22.47}\n",
      "Done Optimize 2100th [swalast.csi300   ] at 20190527 , time cost (ms) : {'parse_input': 21.42, 'solve': 158.24, 'output': 21.57}\n",
      "Done Optimize 2150th [swabest.csi1000  ] at 20190618 , time cost (ms) : {'parse_input': 22.85, 'solve': 158.87, 'output': 22.1}\n",
      "Done Optimize 2200th [best.csi500.1    ] at 20190709 , time cost (ms) : {'parse_input': 22.59, 'solve': 160.19, 'output': 22.07}\n",
      "Done Optimize 2250th [best.csi300      ] at 20190730 , time cost (ms) : {'parse_input': 22.35, 'solve': 160.88, 'output': 21.04}\n",
      "Done Optimize 2300th [swalast.csi1000  ] at 20190813 , time cost (ms) : {'parse_input': 28.98, 'solve': 173.96, 'output': 45.61}\n",
      "Done Optimize 2350th [swabest.csi500.1 ] at 20190903 , time cost (ms) : {'parse_input': 24.71, 'solve': 143.02, 'output': 22.39}\n",
      "Done Optimize 2400th [swabest.csi300   ] at 20190925 , time cost (ms) : {'parse_input': 25.08, 'solve': 160.09, 'output': 22.43}\n",
      "Done Optimize 2450th [best.csi1000     ] at 20191023 , time cost (ms) : {'parse_input': 21.61, 'solve': 163.11, 'output': 23.66}\n",
      "Done Optimize 2500th [swalast.csi500.1 ] at 20191106 , time cost (ms) : {'parse_input': 23.05, 'solve': 144.46, 'output': 22.62}\n",
      "Done Optimize 2550th [swalast.csi300   ] at 20191127 , time cost (ms) : {'parse_input': 22.61, 'solve': 177.23, 'output': 30.55}\n",
      "Done Optimize 2600th [swabest.csi1000  ] at 20191218 , time cost (ms) : {'parse_input': 20.76, 'solve': 146.81, 'output': 22.52}\n",
      "Done Optimize 2650th [best.csi500.1    ] at 20200109 , time cost (ms) : {'parse_input': 23.56, 'solve': 136.23, 'output': 23.05}\n",
      "Done Optimize 2700th [best.csi300      ] at 20200207 , time cost (ms) : {'parse_input': 25.82, 'solve': 157.91, 'output': 21.03}\n",
      "Done Optimize 2750th [swalast.csi1000  ] at 20200221 , time cost (ms) : {'parse_input': 21.9, 'solve': 157.23, 'output': 22.73}\n",
      "Done Optimize 2800th [swabest.csi500.1 ] at 20200313 , time cost (ms) : {'parse_input': 21.63, 'solve': 165.92, 'output': 22.4}\n",
      "Done Optimize 2850th [swabest.csi300   ] at 20200403 , time cost (ms) : {'parse_input': 30.06, 'solve': 177.01, 'output': 23.11}\n",
      "Done Optimize 2900th [best.csi1000     ] at 20200427 , time cost (ms) : {'parse_input': 21.86, 'solve': 164.29, 'output': 23.76}\n",
      "Done Optimize 2950th [swalast.csi500.1 ] at 20200514 , time cost (ms) : {'parse_input': 23.89, 'solve': 153.14, 'output': 23.13}\n",
      "Done Optimize 3000th [swalast.csi300   ] at 20200604 , time cost (ms) : {'parse_input': 23.32, 'solve': 138.52, 'output': 32.43}\n",
      "Done Optimize 3050th [swabest.csi1000  ] at 20200629 , time cost (ms) : {'parse_input': 21.67, 'solve': 145.88, 'output': 22.69}\n",
      "Done Optimize 3100th [best.csi500.1    ] at 20200720 , time cost (ms) : {'parse_input': 22.77, 'solve': 158.84, 'output': 22.94}\n",
      "Done Optimize 3150th [best.csi300      ] at 20200810 , time cost (ms) : {'parse_input': 23.44, 'solve': 147.68, 'output': 21.45}\n",
      "Done Optimize 3200th [swalast.csi1000  ] at 20200824 , time cost (ms) : {'parse_input': 23.84, 'solve': 147.49, 'output': 23.07}\n",
      "Done Optimize 3250th [swabest.csi500.1 ] at 20200914 , time cost (ms) : {'parse_input': 25.1, 'solve': 144.04, 'output': 22.48}\n",
      "Failed optimization at 20200921, even with relax, use w0 instead.\n",
      "Done Optimize 3300th [swabest.csi300   ] at 20201013 , time cost (ms) : {'parse_input': 24.51, 'solve': 144.09, 'output': 22.66}\n",
      "Done Optimize 3350th [best.csi1000     ] at 20201103 , time cost (ms) : {'parse_input': 21.88, 'solve': 147.5, 'output': 22.56}\n",
      "Done Optimize 3400th [swalast.csi500.1 ] at 20201117 , time cost (ms) : {'parse_input': 22.1, 'solve': 154.03, 'output': 22.53}\n",
      "Done Optimize 3450th [swalast.csi300   ] at 20201208 , time cost (ms) : {'parse_input': 32.15, 'solve': 210.11, 'output': 21.13}\n",
      "Done Optimize 3500th [swabest.csi1000  ] at 20201229 , time cost (ms) : {'parse_input': 22.19, 'solve': 158.47, 'output': 22.57}\n",
      "Done Optimize 3550th [best.csi500.1    ] at 20210120 , time cost (ms) : {'parse_input': 22.23, 'solve': 155.85, 'output': 23.75}\n",
      "Done Optimize 3600th [best.csi300      ] at 20210210 , time cost (ms) : {'parse_input': 22.8, 'solve': 178.08, 'output': 21.03}\n",
      "Done Optimize 3650th [swalast.csi1000  ] at 20210303 , time cost (ms) : {'parse_input': 22.17, 'solve': 152.3, 'output': 25.56}\n",
      "Done Optimize 3700th [swabest.csi500.1 ] at 20210324 , time cost (ms) : {'parse_input': 25.15, 'solve': 167.03, 'output': 22.8}\n",
      "Done Optimize 3750th [swabest.csi300   ] at 20210415 , time cost (ms) : {'parse_input': 24.95, 'solve': 158.16, 'output': 22.81}\n",
      "Done Optimize 3800th [best.csi1000     ] at 20210511 , time cost (ms) : {'parse_input': 22.05, 'solve': 157.14, 'output': 20.98}\n",
      "Done Optimize 3850th [swalast.csi500.1 ] at 20210525 , time cost (ms) : {'parse_input': 22.06, 'solve': 166.86, 'output': 22.19}\n",
      "Done Optimize 3900th [swalast.csi300   ] at 20210616 , time cost (ms) : {'parse_input': 23.3, 'solve': 166.26, 'output': 21.7}\n",
      "Done Optimize 3950th [swabest.csi1000  ] at 20210707 , time cost (ms) : {'parse_input': 22.51, 'solve': 181.87, 'output': 21.45}\n",
      "Done Optimize 4000th [best.csi500.1    ] at 20210728 , time cost (ms) : {'parse_input': 22.5, 'solve': 179.66, 'output': 21.51}\n",
      "Done Optimize 4050th [best.csi300      ] at 20210818 , time cost (ms) : {'parse_input': 23.65, 'solve': 175.06, 'output': 21.58}\n",
      "Done Optimize 4100th [swalast.csi1000  ] at 20210901 , time cost (ms) : {'parse_input': 22.82, 'solve': 169.89, 'output': 21.53}\n",
      "Done Optimize 4150th [swabest.csi500.1 ] at 20210924 , time cost (ms) : {'parse_input': 23.52, 'solve': 170.89, 'output': 23.15}\n",
      "Done Optimize 4200th [swabest.csi300   ] at 20211022 , time cost (ms) : {'parse_input': 23.46, 'solve': 163.66, 'output': 21.13}\n",
      "Done Optimize 4250th [best.csi1000     ] at 20211112 , time cost (ms) : {'parse_input': 23.25, 'solve': 174.46, 'output': 22.38}\n",
      "Done Optimize 4300th [swalast.csi500.1 ] at 20211126 , time cost (ms) : {'parse_input': 25.36, 'solve': 170.21, 'output': 24.19}\n",
      "Done Optimize 4350th [swalast.csi300   ] at 20211217 , time cost (ms) : {'parse_input': 25.24, 'solve': 163.3, 'output': 21.73}\n",
      "Done Optimize 4400th [swabest.csi1000  ] at 20220110 , time cost (ms) : {'parse_input': 25.72, 'solve': 166.07, 'output': 24.69}\n",
      "Done Optimize 4450th [best.csi500.1    ] at 20220207 , time cost (ms) : {'parse_input': 24.5, 'solve': 169.25, 'output': 24.7}\n",
      "Done Optimize 4500th [best.csi300      ] at 20220228 , time cost (ms) : {'parse_input': 24.25, 'solve': 173.13, 'output': 45.6}\n",
      "Done Optimize 4550th [swalast.csi1000  ] at 20220314 , time cost (ms) : {'parse_input': 35.24, 'solve': 222.63, 'output': 22.8}\n",
      "Done Optimize 4600th [swabest.csi500.1 ] at 20220406 , time cost (ms) : {'parse_input': 24.09, 'solve': 186.58, 'output': 21.35}\n",
      "Done Optimize 4650th [swabest.csi300   ] at 20220427 , time cost (ms) : {'parse_input': 23.83, 'solve': 184.37, 'output': 22.65}\n",
      "Done Optimize 4700th [best.csi1000     ] at 20220523 , time cost (ms) : {'parse_input': 27.05, 'solve': 166.4, 'output': 23.64}\n",
      "Done Optimize 4750th [swalast.csi500.1 ] at 20220607 , time cost (ms) : {'parse_input': 24.39, 'solve': 186.46, 'output': 21.49}\n",
      "Done Optimize 4800th [swalast.csi300   ] at 20220628 , time cost (ms) : {'parse_input': 23.95, 'solve': 211.93, 'output': 22.15}\n",
      "Done Optimize 4850th [swabest.csi1000  ] at 20220719 , time cost (ms) : {'parse_input': 23.72, 'solve': 181.28, 'output': 22.43}\n",
      "Done Optimize 4900th [best.csi500.1    ] at 20220809 , time cost (ms) : {'parse_input': 26.2, 'solve': 174.49, 'output': 24.86}\n",
      "Done Optimize 4950th [best.csi300      ] at 20220830 , time cost (ms) : {'parse_input': 24.79, 'solve': 192.09, 'output': 21.08}\n",
      "Done Optimize 5000th [swalast.csi1000  ] at 20220914 , time cost (ms) : {'parse_input': 23.61, 'solve': 185.02, 'output': 23.27}\n",
      "Done Optimize 5050th [swabest.csi500.1 ] at 20221012 , time cost (ms) : {'parse_input': 24.27, 'solve': 190.6, 'output': 26.18}\n",
      "Done Optimize 5100th [swabest.csi300   ] at 20221102 , time cost (ms) : {'parse_input': 24.05, 'solve': 184.44, 'output': 21.26}\n",
      "Done Optimize 5150th [best.csi1000     ] at 20221123 , time cost (ms) : {'parse_input': 25.07, 'solve': 195.03, 'output': 25.48}\n",
      "Done Optimize 5200th [swalast.csi500.1 ] at 20221207 , time cost (ms) : {'parse_input': 23.64, 'solve': 191.13, 'output': 21.47}\n",
      "Done Optimize 5250th [swalast.csi300   ] at 20221228 , time cost (ms) : {'parse_input': 28.83, 'solve': 182.22, 'output': 23.28}\n",
      "Done Optimize 5300th [swabest.csi1000  ] at 20230119 , time cost (ms) : {'parse_input': 25.45, 'solve': 192.65, 'output': 23.89}\n",
      "Done Optimize 5350th [best.csi500.1    ] at 20230216 , time cost (ms) : {'parse_input': 31.43, 'solve': 232.78, 'output': 23.41}\n",
      "Done Optimize 5400th [best.csi300      ] at 20230309 , time cost (ms) : {'parse_input': 59.95, 'solve': 211.85, 'output': 23.06}\n",
      "Done Optimize 5450th [swalast.csi1000  ] at 20230323 , time cost (ms) : {'parse_input': 27.02, 'solve': 190.59, 'output': 23.87}\n",
      "Done Optimize 5500th [swabest.csi500.1 ] at 20230414 , time cost (ms) : {'parse_input': 61.58, 'solve': 246.29, 'output': 45.94}\n",
      "Done Optimize 5550th [swabest.csi300   ] at 20230510 , time cost (ms) : {'parse_input': 24.63, 'solve': 194.43, 'output': 23.23}\n",
      "Done Optimize 5600th [best.csi1000     ] at 20230531 , time cost (ms) : {'parse_input': 26.22, 'solve': 214.91, 'output': 30.78}\n",
      "Done Optimize 5650th [swalast.csi500.1 ] at 20230614 , time cost (ms) : {'parse_input': 26.54, 'solve': 194.23, 'output': 20.86}\n",
      "Done Optimize 5700th [swalast.csi300   ] at 20230707 , time cost (ms) : {'parse_input': 27.29, 'solve': 185.23, 'output': 25.89}\n",
      "Done Optimize 5750th [swabest.csi1000  ] at 20230728 , time cost (ms) : {'parse_input': 27.97, 'solve': 184.66, 'output': 24.27}\n",
      "Done Optimize 5800th [best.csi500.1    ] at 20230818 , time cost (ms) : {'parse_input': 27.11, 'solve': 207.09, 'output': 22.98}\n",
      "Done Optimize 5850th [best.csi300      ] at 20230908 , time cost (ms) : {'parse_input': 29.43, 'solve': 195.37, 'output': 23.06}\n",
      "Done Optimize 5900th [swalast.csi1000  ] at 20230922 , time cost (ms) : {'parse_input': 25.58, 'solve': 195.51, 'output': 24.34}\n",
      "Done Optimize 5950th [swabest.csi500.1 ] at 20231023 , time cost (ms) : {'parse_input': 26.1, 'solve': 199.19, 'output': 22.51}\n",
      "Done Optimize 6000th [swabest.csi300   ] at 20231113 , time cost (ms) : {'parse_input': 25.7, 'solve': 191.94, 'output': 23.17}\n",
      "Done Optimize 6050th [best.csi1000     ] at 20231204 , time cost (ms) : {'parse_input': 35.45, 'solve': 275.63, 'output': 23.4}\n",
      "Done Optimize 6100th [swalast.csi500.1 ] at 20231218 , time cost (ms) : {'parse_input': 26.51, 'solve': 193.66, 'output': 23.06}\n",
      "Done Optimize 6150th [swalast.csi300   ] at 20240109 , time cost (ms) : {'parse_input': 26.03, 'solve': 185.92, 'output': 23.28}\n",
      "Done Optimize 6200th [swabest.csi1000  ] at 20240130 , time cost (ms) : {'parse_input': 27.17, 'solve': 179.1, 'output': 23.89}\n",
      "Done Optimize 6250th [best.csi500.1    ] at 20240228 , time cost (ms) : {'parse_input': 29.06, 'solve': 212.25, 'output': 24.2}\n",
      "Done Optimize 6300th [best.csi300      ] at 20240320 , time cost (ms) : {'parse_input': 26.92, 'solve': 203.96, 'output': 24.29}\n",
      "Done Optimize 6350th [swalast.csi1000  ] at 20240403 , time cost (ms) : {'parse_input': 26.42, 'solve': 205.25, 'output': 23.64}\n",
      "Done Optimize 6400th [swabest.csi500.1 ] at 20240426 , time cost (ms) : {'parse_input': 26.62, 'solve': 199.14, 'output': 23.02}\n",
      "Done Optimize 6450th [swabest.csi300   ] at 20240522 , time cost (ms) : {'parse_input': 26.59, 'solve': 180.78, 'output': 22.33}\n",
      "Done Optimize 6500th [best.csi1000     ] at 20240613 , time cost (ms) : {'parse_input': 25.56, 'solve': 209.98, 'output': 25.61}\n",
      "Done Optimize 6550th [swalast.csi500.1 ] at 20240627 , time cost (ms) : {'parse_input': 58.73, 'solve': 213.13, 'output': 46.41}\n",
      "Group optimization Finished , Total time: 1545.00 secs, Setup time: 98.96 secs, Calc time: 1446.04 secs, Each optim time: 0.22\n",
      "FmpManager calc Finished!\n",
      "FmpManager plot Finished!\n",
      "FMP test Result:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>factor_name</th>\n",
       "      <th>benchmark</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>pf</th>\n",
       "      <th>bm</th>\n",
       "      <th>excess</th>\n",
       "      <th>annualized</th>\n",
       "      <th>mdd</th>\n",
       "      <th>te</th>\n",
       "      <th>ir</th>\n",
       "      <th>calmar</th>\n",
       "      <th>turnover</th>\n",
       "      <th>mdd_period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>best</td>\n",
       "      <td>csi300</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240705</td>\n",
       "      <td>129.60%</td>\n",
       "      <td>20.06%</td>\n",
       "      <td>109.53%</td>\n",
       "      <td>8.91%</td>\n",
       "      <td>4.61%</td>\n",
       "      <td>3.77%</td>\n",
       "      <td>2.364</td>\n",
       "      <td>1.932</td>\n",
       "      <td>146.683</td>\n",
       "      <td>20201103-20210113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>best</td>\n",
       "      <td>csi500</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240705</td>\n",
       "      <td>172.33%</td>\n",
       "      <td>-15.78%</td>\n",
       "      <td>188.11%</td>\n",
       "      <td>16.82%</td>\n",
       "      <td>8.27%</td>\n",
       "      <td>5.87%</td>\n",
       "      <td>2.863</td>\n",
       "      <td>2.035</td>\n",
       "      <td>146.788</td>\n",
       "      <td>20230922-20240206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best</td>\n",
       "      <td>csi1000</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240705</td>\n",
       "      <td>131.32%</td>\n",
       "      <td>-40.47%</td>\n",
       "      <td>171.79%</td>\n",
       "      <td>19.47%</td>\n",
       "      <td>8.53%</td>\n",
       "      <td>5.94%</td>\n",
       "      <td>3.279</td>\n",
       "      <td>2.283</td>\n",
       "      <td>146.885</td>\n",
       "      <td>20230922-20240306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>swabest</td>\n",
       "      <td>csi300</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240705</td>\n",
       "      <td>81.53%</td>\n",
       "      <td>20.06%</td>\n",
       "      <td>61.47%</td>\n",
       "      <td>5.64%</td>\n",
       "      <td>6.41%</td>\n",
       "      <td>3.64%</td>\n",
       "      <td>1.547</td>\n",
       "      <td>0.880</td>\n",
       "      <td>146.689</td>\n",
       "      <td>20221019-20240306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>swabest</td>\n",
       "      <td>csi500</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240705</td>\n",
       "      <td>121.36%</td>\n",
       "      <td>-15.78%</td>\n",
       "      <td>137.14%</td>\n",
       "      <td>13.80%</td>\n",
       "      <td>9.16%</td>\n",
       "      <td>6.35%</td>\n",
       "      <td>2.174</td>\n",
       "      <td>1.507</td>\n",
       "      <td>146.804</td>\n",
       "      <td>20230922-20240206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>swabest</td>\n",
       "      <td>csi1000</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240705</td>\n",
       "      <td>138.16%</td>\n",
       "      <td>-40.47%</td>\n",
       "      <td>178.63%</td>\n",
       "      <td>20.08%</td>\n",
       "      <td>9.07%</td>\n",
       "      <td>5.64%</td>\n",
       "      <td>3.562</td>\n",
       "      <td>2.214</td>\n",
       "      <td>146.837</td>\n",
       "      <td>20230922-20240515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>swalast</td>\n",
       "      <td>csi300</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240705</td>\n",
       "      <td>83.97%</td>\n",
       "      <td>20.06%</td>\n",
       "      <td>63.91%</td>\n",
       "      <td>5.87%</td>\n",
       "      <td>8.00%</td>\n",
       "      <td>3.85%</td>\n",
       "      <td>1.525</td>\n",
       "      <td>0.735</td>\n",
       "      <td>146.289</td>\n",
       "      <td>20221109-20240704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>swalast</td>\n",
       "      <td>csi500</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240705</td>\n",
       "      <td>104.48%</td>\n",
       "      <td>-15.78%</td>\n",
       "      <td>120.26%</td>\n",
       "      <td>12.69%</td>\n",
       "      <td>8.79%</td>\n",
       "      <td>6.29%</td>\n",
       "      <td>2.017</td>\n",
       "      <td>1.443</td>\n",
       "      <td>146.842</td>\n",
       "      <td>20231120-20240206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>swalast</td>\n",
       "      <td>csi1000</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240705</td>\n",
       "      <td>94.64%</td>\n",
       "      <td>-40.47%</td>\n",
       "      <td>135.11%</td>\n",
       "      <td>17.08%</td>\n",
       "      <td>10.48%</td>\n",
       "      <td>5.87%</td>\n",
       "      <td>2.911</td>\n",
       "      <td>1.631</td>\n",
       "      <td>146.888</td>\n",
       "      <td>20230407-20240419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  factor_name benchmark    start      end      pf       bm     excess  annualized   mdd     te     ir   calmar turnover     mdd_period    \n",
       "1       best     csi300  20170104  20240705  129.60%   20.06%  109.53%    8.91%     4.61%  3.77%  2.364  1.932  146.683  20201103-20210113\n",
       "2       best     csi500  20170104  20240705  172.33%  -15.78%  188.11%   16.82%     8.27%  5.87%  2.863  2.035  146.788  20230922-20240206\n",
       "0       best    csi1000  20170104  20240705  131.32%  -40.47%  171.79%   19.47%     8.53%  5.94%  3.279  2.283  146.885  20230922-20240306\n",
       "4    swabest     csi300  20170104  20240705   81.53%   20.06%   61.47%    5.64%     6.41%  3.64%  1.547  0.880  146.689  20221019-20240306\n",
       "5    swabest     csi500  20170104  20240705  121.36%  -15.78%  137.14%   13.80%     9.16%  6.35%  2.174  1.507  146.804  20230922-20240206\n",
       "3    swabest    csi1000  20170104  20240705  138.16%  -40.47%  178.63%   20.08%     9.07%  5.64%  3.562  2.214  146.837  20230922-20240515\n",
       "7    swalast     csi300  20170104  20240705   83.97%   20.06%   63.91%    5.87%     8.00%  3.85%  1.525  0.735  146.289  20221109-20240704\n",
       "8    swalast     csi500  20170104  20240705  104.48%  -15.78%  120.26%   12.69%     8.79%  6.29%  2.017  1.443  146.842  20231120-20240206\n",
       "6    swalast    csi1000  20170104  20240705   94.64%  -40.47%  135.11%   17.08%    10.48%  5.87%  2.911  1.631  146.888  20230407-20240419"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-07-14 05:24:00|MOD:time        |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 9 Hours 22 Minutes 36.9 Seconds\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytic datas are saved to /home/mengkjin/Workspace/learndl/model/resnet_gru_avg.3/detailed_analysis/data.xlsx\n",
      "Analytic plots are saved to /home/mengkjin/Workspace/learndl/model/resnet_gru_avg.3/detailed_analysis/plot.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<src.nn_model.trainer.trainer.NetTrainer at 0x7009616cfb50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.api import Trainer  \n",
    "app = Trainer.initialize(stage = 0 , resume = 0 , checkname= 1)\n",
    "app.go()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict is False , Data Processing start!\n",
      "6 datas :['y', 'day', '30m', 'style', 'indus', 'week']\n",
      "y blocks loading start!\n",
      " --> labels blocks reading [ret10_lag] DataBase...... finished! Cost 21.53 secs\n",
      " --> labels blocks reading [ret20_lag] DataBase...... finished! Cost 19.03 secs\n",
      " --> labels blocks merging (2)...... finished! Cost 3.28 secs\n",
      " --> models blocks reading [risk_exp] DataBase...... finished! Cost 66.12 secs\n",
      "y blocks loading finished! Cost 120.89 secs\n",
      "y blocks process...... finished! Cost 45.48 secs\n",
      "y blocks masking...... finished! Cost 0.75 secs\n",
      "y blocks saving ...... finished! Cost 3.65 secs\n",
      "y blocks norming...... finished! Cost 0.00 secs\n",
      "y finished! Cost 170.96 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "day blocks loading start!\n",
      " --> trade blocks reading [day] DataBase...... finished! Cost 28.90 secs\n",
      "day blocks loading finished! Cost 28.93 secs\n",
      "day blocks process...... finished! Cost 3.98 secs\n",
      "day blocks masking...... finished! Cost 0.82 secs\n",
      "day blocks saving ...... finished! Cost 3.54 secs\n",
      "day blocks norming...... finished! Cost 7.91 secs\n",
      "day finished! Cost 45.56 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "30m blocks loading start!\n",
      " --> trade blocks reading [30min] DataBase...... finished! Cost 68.91 secs\n",
      " --> trade blocks reading [day] DataBase...... finished! Cost 25.55 secs\n",
      "30m blocks loading finished! Cost 95.36 secs\n",
      "30m blocks process...... finished! Cost 37.15 secs\n",
      "30m blocks masking...... finished! Cost 2.38 secs\n",
      "30m blocks saving ...... finished! Cost 40.96 secs\n",
      "30m blocks norming...... finished! Cost 3.78 secs\n",
      "30m finished! Cost 179.84 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "style blocks loading start!\n",
      " --> models blocks reading [risk_exp] DataBase...... finished! Cost 32.29 secs\n",
      "style blocks loading finished! Cost 32.32 secs\n",
      "style blocks process...... finished! Cost 0.00 secs\n",
      "style blocks masking...... finished! Cost 0.89 secs\n",
      "style blocks saving ...... finished! Cost 5.00 secs\n",
      "style blocks norming...... finished! Cost 0.00 secs\n",
      "style finished! Cost 38.37 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "indus blocks loading start!\n",
      " --> models blocks reading [risk_exp] DataBase...... finished! Cost 59.12 secs\n",
      "indus blocks loading finished! Cost 59.13 secs\n",
      "indus blocks process...... finished! Cost 0.00 secs\n",
      "indus blocks masking...... finished! Cost 2.54 secs\n",
      "indus blocks saving ...... finished! Cost 28.21 secs\n",
      "indus blocks norming...... finished! Cost 0.00 secs\n",
      "indus finished! Cost 90.06 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "week blocks loading start!\n",
      " --> trade blocks reading [day] DataBase...... finished! Cost 26.11 secs\n",
      "week blocks loading finished! Cost 26.14 secs\n",
      "week blocks process...... finished! Cost 29.50 secs\n",
      "week blocks masking...... finished! Cost 2.47 secs\n",
      "week blocks saving ...... finished! Cost 24.22 secs\n",
      "week blocks norming...... finished! Cost 5.89 secs\n",
      "week finished! Cost 88.40 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "Data Processing Finished! Cost 613.18 Seconds\n",
      "predict is True , Data Processing start!\n",
      "6 datas :['y', 'day', '30m', 'style', 'indus', 'week']\n",
      "y blocks loading start!\n",
      " --> labels blocks reading [ret10_lag] DataBase...... finished! Cost 0.72 secs\n",
      " --> labels blocks reading [ret20_lag] DataBase...... finished! Cost 0.66 secs\n",
      " --> labels blocks merging (2)...... finished! Cost 0.10 secs\n",
      " --> models blocks reading [risk_exp] DataBase...... finished! Cost 2.93 secs\n",
      "y blocks loading finished! Cost 4.62 secs\n",
      "y blocks process...... finished! Cost 2.13 secs\n",
      "y blocks masking...... finished! Cost 0.07 secs\n",
      "y blocks saving ...... finished! Cost 0.13 secs\n",
      "y blocks norming...... finished! Cost 0.00 secs\n",
      "y finished! Cost 7.21 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "day blocks loading start!\n",
      " --> trade blocks reading [day] DataBase...... finished! Cost 1.20 secs\n",
      "day blocks loading finished! Cost 1.20 secs\n",
      "day blocks process...... finished! Cost 0.15 secs\n",
      "day blocks masking...... finished! Cost 0.08 secs\n",
      "day blocks saving ...... finished! Cost 0.15 secs\n",
      "day blocks norming...... finished! Cost 0.00 secs\n",
      "day finished! Cost 1.85 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "30m blocks loading start!\n",
      " --> trade blocks reading [30min] DataBase...... finished! Cost 5.75 secs\n",
      " --> trade blocks reading [day] DataBase...... finished! Cost 1.00 secs\n",
      "30m blocks loading finished! Cost 6.79 secs\n",
      "30m blocks process...... finished! Cost 1.56 secs\n",
      "30m blocks masking...... finished! Cost 0.12 secs\n",
      "30m blocks saving ...... finished! Cost 2.05 secs\n",
      "30m blocks norming...... finished! Cost 0.00 secs\n",
      "30m finished! Cost 10.72 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "style blocks loading start!\n",
      " --> models blocks reading [risk_exp] DataBase...... finished! Cost 1.49 secs\n",
      "style blocks loading finished! Cost 1.49 secs\n",
      "style blocks process...... finished! Cost 0.00 secs\n",
      "style blocks masking...... finished! Cost 0.07 secs\n",
      "style blocks saving ...... finished! Cost 0.20 secs\n",
      "style blocks norming...... finished! Cost 0.00 secs\n",
      "style finished! Cost 1.91 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "indus blocks loading start!\n",
      " --> models blocks reading [risk_exp] DataBase...... finished! Cost 2.83 secs\n",
      "indus blocks loading finished! Cost 2.83 secs\n",
      "indus blocks process...... finished! Cost 0.00 secs\n",
      "indus blocks masking...... finished! Cost 0.10 secs\n",
      "indus blocks saving ...... finished! Cost 1.05 secs\n",
      "indus blocks norming...... finished! Cost 0.00 secs\n",
      "indus finished! Cost 4.13 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "week blocks loading start!\n",
      " --> trade blocks reading [day] DataBase...... finished! Cost 2.12 secs\n",
      "week blocks loading finished! Cost 2.12 secs\n",
      "week blocks process...... finished! Cost 1.69 secs\n",
      "week blocks masking...... finished! Cost 0.14 secs\n",
      "week blocks saving ..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[44m24-07-06 20:18:14|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-07-06 20:18:14|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Sat Jul  6 20:18:14 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... finished! Cost 2.74 secs\n",
      "week blocks norming...... finished! Cost 0.00 secs\n",
      "week finished! Cost 6.86 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "Data Processing Finished! Cost 32.67 Seconds\n",
      "--------------------------------------------------------------------------------\n",
      "--Process Queue : Data + Fit + Test\n",
      "--Confirm Resume Training!\n",
      "--Model_name is set to gru_day!\n",
      "Callback : ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True) , reset optimizer on some epoch (can speedup scheduler)\n",
      "Callback : CallbackTimer(verbosity=2) , record time cost of callback hooks\n",
      "Callback : EarlyStoppage(patience=20) , stop fitting when validation score cease to improve\n",
      "Callback : ValidationConverge(patience=5,eps=1e-05) , stop fitting when valid_score converge\n",
      "Callback : EarlyExitRetrain(earliest=5,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100]) , retrain with new lr if fitting stopped too early\n",
      "Callback : NanLossRetrain(max_attempt=4) , retrain if fitting encounters nan loss\n",
      "Callback : BatchDisplay(verbosity=2) , display batch progress bar\n",
      "Callback : StatusDisplay(verbosity=2) , display epoch and event information\n",
      "Callback : DetailedAlphaAnalysis(which_alpha=avg) , record and concat each model to Alpha model instance\n",
      "{'random_seed': None,\n",
      " 'model_name': 'gru_day',\n",
      " 'model_module': 'gru',\n",
      " 'model_data_type': 'day',\n",
      " 'model_types': ['best', 'swalast', 'swabest'],\n",
      " 'labels': ['std_lag1_10'],\n",
      " 'beg_date': 20170103,\n",
      " 'end_date': 99991231,\n",
      " 'sample_method': 'train_shuffle',\n",
      " 'shuffle_option': 'epoch',\n",
      " 'lgbm_ensembler': False}\n",
      "{'hidden_dim': [32, 64],\n",
      " 'seqlens': [{'day': 30, '30m': 30, 'dms': 30}],\n",
      " 'tra_seqlens': [{'hist_loss': 40}],\n",
      " 'dropout': [0.1],\n",
      " 'enc_in': [True],\n",
      " 'enc_att': [False],\n",
      " 'rnn_type': ['lstm'],\n",
      " 'rnn_att': [False],\n",
      " 'rnn_layers': [2],\n",
      " 'dec_mlp_layers': [2],\n",
      " 'num_output': [1],\n",
      " 'kernel_size': [3],\n",
      " 'hidden_as_factor': [False],\n",
      " 'ordered_param_group': [False],\n",
      " 'tra_num_states': [5]}\n",
      "Load  2 DataBlocks...... finished! Cost 1.03 secs\n",
      "Align 2 DataBlocks...... finished! Cost 2.35 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-07-06 20:18:19|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 5.0 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-07-06 20:18:19|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Sat Jul  6 20:18:19 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-07-06 20:18:19|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Fit], Cost 0.0 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-07-06 20:18:19|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Sat Jul  6 20:18:19 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n",
      "score function of [spearman] calculated and success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[44m24-07-06 20:19:06|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mTesting Mean Score(spearman):\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">gru.0</th>\n",
       "      <th colspan=\"3\" halign=\"left\">gru.1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>best</th>\n",
       "      <th>swalast</th>\n",
       "      <th>swabest</th>\n",
       "      <th>best</th>\n",
       "      <th>swalast</th>\n",
       "      <th>swabest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20170103</th>\n",
       "      <td>0.1536</td>\n",
       "      <td>0.1525</td>\n",
       "      <td>0.1520</td>\n",
       "      <td>0.1525</td>\n",
       "      <td>0.1521</td>\n",
       "      <td>0.1520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20170704</th>\n",
       "      <td>0.1373</td>\n",
       "      <td>0.1327</td>\n",
       "      <td>0.1365</td>\n",
       "      <td>0.1413</td>\n",
       "      <td>0.1373</td>\n",
       "      <td>0.1405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20171226</th>\n",
       "      <td>0.1305</td>\n",
       "      <td>0.1323</td>\n",
       "      <td>0.1329</td>\n",
       "      <td>0.1291</td>\n",
       "      <td>0.1298</td>\n",
       "      <td>0.1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180627</th>\n",
       "      <td>0.1225</td>\n",
       "      <td>0.1186</td>\n",
       "      <td>0.1223</td>\n",
       "      <td>0.1226</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.1236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181220</th>\n",
       "      <td>0.0998</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1004</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.0987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190624</th>\n",
       "      <td>0.0970</td>\n",
       "      <td>0.0955</td>\n",
       "      <td>0.0972</td>\n",
       "      <td>0.0920</td>\n",
       "      <td>0.0908</td>\n",
       "      <td>0.0924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20191217</th>\n",
       "      <td>0.1011</td>\n",
       "      <td>0.0991</td>\n",
       "      <td>0.1005</td>\n",
       "      <td>0.1042</td>\n",
       "      <td>0.1048</td>\n",
       "      <td>0.1047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20200617</th>\n",
       "      <td>0.0970</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.0978</td>\n",
       "      <td>0.0947</td>\n",
       "      <td>0.0927</td>\n",
       "      <td>0.0946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201214</th>\n",
       "      <td>0.0837</td>\n",
       "      <td>0.0792</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.0759</td>\n",
       "      <td>0.0766</td>\n",
       "      <td>0.0739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20210615</th>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.0572</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211209</th>\n",
       "      <td>0.0935</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0962</td>\n",
       "      <td>0.1084</td>\n",
       "      <td>0.1106</td>\n",
       "      <td>0.1084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20220613</th>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.0909</td>\n",
       "      <td>0.0813</td>\n",
       "      <td>0.0857</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.0876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20221206</th>\n",
       "      <td>0.0650</td>\n",
       "      <td>0.0616</td>\n",
       "      <td>0.0635</td>\n",
       "      <td>0.0580</td>\n",
       "      <td>0.0576</td>\n",
       "      <td>0.0497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20230606</th>\n",
       "      <td>0.0906</td>\n",
       "      <td>0.0857</td>\n",
       "      <td>0.0872</td>\n",
       "      <td>0.0868</td>\n",
       "      <td>0.0885</td>\n",
       "      <td>0.0856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20231201</th>\n",
       "      <td>0.1217</td>\n",
       "      <td>0.1221</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>0.1082</td>\n",
       "      <td>0.1025</td>\n",
       "      <td>0.1008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20240604</th>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.0847</td>\n",
       "      <td>0.0923</td>\n",
       "      <td>0.0953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg</th>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.1005</td>\n",
       "      <td>0.1011</td>\n",
       "      <td>0.1014</td>\n",
       "      <td>0.1007</td>\n",
       "      <td>0.1005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sum</th>\n",
       "      <td>185.7499</td>\n",
       "      <td>183.1368</td>\n",
       "      <td>184.2437</td>\n",
       "      <td>184.7413</td>\n",
       "      <td>183.4767</td>\n",
       "      <td>183.1322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Std</th>\n",
       "      <td>0.0669</td>\n",
       "      <td>0.0665</td>\n",
       "      <td>0.0670</td>\n",
       "      <td>0.0663</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>0.0663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>65.0745</td>\n",
       "      <td>64.5593</td>\n",
       "      <td>64.4564</td>\n",
       "      <td>65.2390</td>\n",
       "      <td>65.7981</td>\n",
       "      <td>64.6863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IR</th>\n",
       "      <td>7.4687</td>\n",
       "      <td>7.4095</td>\n",
       "      <td>7.3977</td>\n",
       "      <td>7.4875</td>\n",
       "      <td>7.5517</td>\n",
       "      <td>7.4241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             gru.0                         gru.1                    \n",
       "              best   swalast   swabest      best   swalast   swabest\n",
       "20170103    0.1536    0.1525    0.1520    0.1525    0.1521    0.1520\n",
       "20170704    0.1373    0.1327    0.1365    0.1413    0.1373    0.1405\n",
       "20171226    0.1305    0.1323    0.1329    0.1291    0.1298    0.1300\n",
       "20180627    0.1225    0.1186    0.1223    0.1226    0.1210    0.1236\n",
       "20181220    0.0998    0.0990    0.1004    0.0958    0.0981    0.0987\n",
       "20190624    0.0970    0.0955    0.0972    0.0920    0.0908    0.0924\n",
       "20191217    0.1011    0.0991    0.1005    0.1042    0.1048    0.1047\n",
       "20200617    0.0970    0.0963    0.0978    0.0947    0.0927    0.0946\n",
       "20201214    0.0837    0.0792    0.0842    0.0759    0.0766    0.0739\n",
       "20210615    0.0605    0.0572    0.0608    0.0689    0.0666    0.0662\n",
       "20211209    0.0935    0.0954    0.0962    0.1084    0.1106    0.1084\n",
       "20220613    0.0931    0.0909    0.0813    0.0857    0.0829    0.0876\n",
       "20221206    0.0650    0.0616    0.0635    0.0580    0.0576    0.0497\n",
       "20230606    0.0906    0.0857    0.0872    0.0868    0.0885    0.0856\n",
       "20231201    0.1217    0.1221    0.1235    0.1082    0.1025    0.1008\n",
       "20240604    0.0052    0.0437   -0.0050    0.0847    0.0923    0.0953\n",
       "Avg         0.1019    0.1005    0.1011    0.1014    0.1007    0.1005\n",
       "Sum       185.7499  183.1368  184.2437  184.7413  183.4767  183.1322\n",
       "Std         0.0669    0.0665    0.0670    0.0663    0.0653    0.0663\n",
       "T          65.0745   64.5593   64.4564   65.2390   65.7981   64.6863\n",
       "IR          7.4687    7.4095    7.3977    7.4875    7.5517    7.4241"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-07-06 20:19:06|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 46.6 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-07-06 20:19:10|MOD:test        |\u001b[0m: \u001b[1m\u001b[34mPerforming Factor and FMP test!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-07-06 20:20:51|MOD:time        |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 2 Minutes 37.1 Seconds\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataAPI , Trainer\n\u001b[1;32m      2\u001b[0m DataAPI\u001b[38;5;241m.\u001b[39mreconstruct_train_data()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/learndl/src/nn_model/trainer/trainer.py:66\u001b[0m, in \u001b[0;36mTrainer.update_models\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m REG_MODELS:\n\u001b[1;32m     65\u001b[0m     config_path \u001b[38;5;241m=\u001b[39m TrainConfig\u001b[38;5;241m.\u001b[39mget_config_path(model\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresume\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/learndl/src/nn_model/trainer/trainer.py:56\u001b[0m, in \u001b[0;36mTrainer.go\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgo\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m BigTimer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mcritical , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMain Process\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 56\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Workspace/learndl/src/nn_model/classes/mod.py:328\u001b[0m, in \u001b[0;36mBaseTrainer.main_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_configure_model()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage_queue: \n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstage_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_summarize_model()\n",
      "File \u001b[0;32m~/Workspace/learndl/src/nn_model/classes/mod.py:354\u001b[0m, in \u001b[0;36mBaseTrainer.stage_test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;241m.\u001b[39mmodel_date , \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;241m.\u001b[39mmodel_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_iter:\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_model()\n\u001b[0;32m--> 354\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_test_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/learndl/src/nn_model/classes/mod.py:44\u001b[0m, in \u001b[0;36mBaseCB.hook_wrapper.<locals>.wrapper_with\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mat_enter(hook_name)\n\u001b[1;32m     43\u001b[0m hook()\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mat_exit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/learndl/src/nn_model/callback/api.py:20\u001b[0m, in \u001b[0;36mCallBackManager.at_exit\u001b[0;34m(self, hook_name)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mat_exit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hook_name):\n\u001b[0;32m---> 20\u001b[0m     [cb\u001b[38;5;241m.\u001b[39mat_exit(hook_name) \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks]\n",
      "File \u001b[0;32m~/Workspace/learndl/src/nn_model/callback/api.py:20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mat_exit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hook_name):\n\u001b[0;32m---> 20\u001b[0m     [\u001b[43mcb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mat_exit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks]\n",
      "File \u001b[0;32m~/Workspace/learndl/src/nn_model/classes/mod.py:32\u001b[0m, in \u001b[0;36mBaseCB.at_exit\u001b[0;34m(self, hook_name)\u001b[0m\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mat_exit\u001b[39m(\u001b[38;5;28mself\u001b[39m , hook_name): \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/learndl/src/nn_model/callback/test.py:87\u001b[0m, in \u001b[0;36mDetailedAlphaAnalysis.on_test_end\u001b[0;34m(self)\u001b[0m\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_test_end\u001b[39m(\u001b[38;5;28mself\u001b[39m):   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_pred_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/learndl/src/nn_model/callback/test.py:59\u001b[0m, in \u001b[0;36mDetailedAlphaAnalysis.export_pred_df\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfactor_name\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mpivot_table(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m'\u001b[39m,[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecid\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfactor_name\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPerforming Factor and FMP test!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfac_man \u001b[38;5;241m=\u001b[39m \u001b[43mPerfManager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmp_man \u001b[38;5;241m=\u001b[39m FmpManager\u001b[38;5;241m.\u001b[39mrun_test(df , verbosity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     62\u001b[0m rslts : \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m , pd\u001b[38;5;241m.\u001b[39mDataFrame] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfmp_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m:v \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmp_man\u001b[38;5;241m.\u001b[39mget_rslts()\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/Workspace/learndl/src/factor/perf/api.py:106\u001b[0m, in \u001b[0;36mPerfManager.run_test\u001b[0;34m(cls, factor_val, benchmark, all, verbosity, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_test\u001b[39m(\u001b[38;5;28mcls\u001b[39m , factor_val : pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m|\u001b[39m DataBlock , benchmark : \u001b[38;5;28mlist\u001b[39m[Benchmark\u001b[38;5;241m|\u001b[39mAny] \u001b[38;5;241m|\u001b[39m Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m ,\n\u001b[1;32m    104\u001b[0m              \u001b[38;5;28mall\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m , verbosity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m , \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    105\u001b[0m     pm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mall\u001b[39m , \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfactor_val\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mplot(show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m , verbosity \u001b[38;5;241m=\u001b[39m verbosity)\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pm\n",
      "File \u001b[0;32m~/Workspace/learndl/src/factor/perf/api.py:37\u001b[0m, in \u001b[0;36mPerfManager.calc\u001b[0;34m(self, factor_val, benchmarks, verbosity)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc\u001b[39m(\u001b[38;5;28mself\u001b[39m , factor_val: DataBlock \u001b[38;5;241m|\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame, benchmarks: Optional[\u001b[38;5;28mlist\u001b[39m[Benchmark\u001b[38;5;241m|\u001b[39mAny]] \u001b[38;5;241m|\u001b[39m Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m , verbosity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m perf_name , perf_calc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperf_calc_dict\u001b[38;5;241m.\u001b[39mitems(): \n\u001b[0;32m---> 37\u001b[0m         \u001b[43mperf_calc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfactor_val\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbenchmarks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m verbosity \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m calc of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Finished!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbosity \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m calc Finished!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Workspace/learndl/src/factor/perf/calculator.py:36\u001b[0m, in \u001b[0;36mBasePerfCalc.calc\u001b[0;34m(self, factor_val, benchmarks)\u001b[0m\n\u001b[1;32m     33\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculator()\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m#self.benchmark_names = [(bm.name if bm else None) for bm in benchmarks]\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_rslt : pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(\n\u001b[0;32m---> 36\u001b[0m         [func(factor_val,benchmark\u001b[38;5;241m=\u001b[39mbm,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\u001b[38;5;241m.\u001b[39massign(benchmark\u001b[38;5;241m=\u001b[39m(bm\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mif\u001b[39;00m bm \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m bm \u001b[38;5;129;01min\u001b[39;00m benchmarks])\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Workspace/learndl/src/factor/perf/calculator.py:36\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculator()\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m#self.benchmark_names = [(bm.name if bm else None) for bm in benchmarks]\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_rslt : pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(\n\u001b[0;32m---> 36\u001b[0m         [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfactor_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbm\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39massign(benchmark\u001b[38;5;241m=\u001b[39m(bm\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mif\u001b[39;00m bm \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m bm \u001b[38;5;129;01min\u001b[39;00m benchmarks])\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Workspace/learndl/src/factor/perf/stat.py:293\u001b[0m, in \u001b[0;36mcalc_industry_ic\u001b[0;34m(factor_val, benchmark, nday, lag, ic_type, ret_type)\u001b[0m\n\u001b[1;32m    291\u001b[0m factor_val , secid , date \u001b[38;5;241m=\u001b[39m factor_val_breakdown(factor_val , benchmark)\n\u001b[1;32m    292\u001b[0m factor_ind \u001b[38;5;241m=\u001b[39m get_industry_exp(factor_val)\u001b[38;5;241m.\u001b[39mreset_index()\u001b[38;5;241m.\u001b[39mset_index([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecid\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindustry\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 293\u001b[0m industry_ic \u001b[38;5;241m=\u001b[39m \u001b[43mfactor_ind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindustry\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43meval_ic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnday\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnday\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mic_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mic_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mret_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mret_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m ic_mean \u001b[38;5;241m=\u001b[39m industry_ic\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindustry\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mstack()\n\u001b[1;32m    296\u001b[0m ic_std  \u001b[38;5;241m=\u001b[39m industry_ic\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindustry\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;241m.\u001b[39mstack()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1824\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[0;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1822\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1823\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1824\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1825\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1826\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, Series)\n\u001b[1;32m   1827\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1828\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1829\u001b[0m         ):\n\u001b[1;32m   1830\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1831\u001b[0m                 message\u001b[38;5;241m=\u001b[39m_apply_groupings_depr\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1832\u001b[0m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1835\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   1836\u001b[0m             )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1885\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[0;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[1;32m   1852\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     is_agg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[1;32m   1859\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1860\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[1;32m   1861\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1883\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1885\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_groupwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1887\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/groupby/ops.py:919\u001b[0m, in \u001b[0;36mBaseGrouper.apply_groupwise\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[1;32m    918\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[0;32m--> 919\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[1;32m    921\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Workspace/learndl/src/factor/perf/stat.py:294\u001b[0m, in \u001b[0;36mcalc_industry_ic.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    291\u001b[0m factor_val , secid , date \u001b[38;5;241m=\u001b[39m factor_val_breakdown(factor_val , benchmark)\n\u001b[1;32m    292\u001b[0m factor_ind \u001b[38;5;241m=\u001b[39m get_industry_exp(factor_val)\u001b[38;5;241m.\u001b[39mreset_index()\u001b[38;5;241m.\u001b[39mset_index([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecid\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindustry\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    293\u001b[0m industry_ic \u001b[38;5;241m=\u001b[39m factor_ind\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindustry\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m--> 294\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x:\u001b[43meval_ic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnday\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnday\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mic_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mic_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mret_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mret_type\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    295\u001b[0m ic_mean \u001b[38;5;241m=\u001b[39m industry_ic\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindustry\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mstack()\n\u001b[1;32m    296\u001b[0m ic_std  \u001b[38;5;241m=\u001b[39m industry_ic\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindustry\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;241m.\u001b[39mstack()\n",
      "File \u001b[0;32m~/Workspace/learndl/src/factor/perf/stat.py:39\u001b[0m, in \u001b[0;36meval_ic\u001b[0;34m(factor_val, nday, lag, ic_type, ret_type)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_ic\u001b[39m(factor_val : DataBlock \u001b[38;5;241m|\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame , nday : \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m , lag : \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m , \n\u001b[1;32m     37\u001b[0m             ic_type  : Literal[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpearson\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspearman\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpearson\u001b[39m\u001b[38;5;124m'\u001b[39m ,\n\u001b[1;32m     38\u001b[0m             ret_type : Literal[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvwap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m---> 39\u001b[0m     factor_val \u001b[38;5;241m=\u001b[39m \u001b[43mget_fut_ret\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfactor_val\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnday\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlag\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mret_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mret_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     factor_list \u001b[38;5;241m=\u001b[39m factor_val\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mvalues[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     41\u001b[0m     ic \u001b[38;5;241m=\u001b[39m factor_val\u001b[38;5;241m.\u001b[39mgroupby(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m], as_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x:x[factor_list]\u001b[38;5;241m.\u001b[39mcorrwith(x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mret\u001b[39m\u001b[38;5;124m'\u001b[39m], method\u001b[38;5;241m=\u001b[39mic_type))\n",
      "File \u001b[0;32m~/Workspace/learndl/src/factor/perf/stat.py:24\u001b[0m, in \u001b[0;36mget_fut_ret\u001b[0;34m(factor_val, nday, lag, ret_type)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_fut_ret\u001b[39m(factor_val : DataBlock \u001b[38;5;241m|\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame , nday : \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m , lag : \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m , ret_type : Literal[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvwap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     23\u001b[0m     factor_val , secid , date \u001b[38;5;241m=\u001b[39m factor_val_breakdown(factor_val)\n\u001b[0;32m---> 24\u001b[0m     factor_ret \u001b[38;5;241m=\u001b[39m \u001b[43mDATAVENDOR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnday_fut_ret\u001b[49m\u001b[43m(\u001b[49m\u001b[43msecid\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnday\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlag\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mret_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mret_type\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_dataframe()\n\u001b[1;32m     25\u001b[0m     factor_ret \u001b[38;5;241m=\u001b[39m factor_val\u001b[38;5;241m.\u001b[39mjoin(factor_ret , on \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecid\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m factor_ret\n",
      "File \u001b[0;32m~/Workspace/learndl/src/data/vendor.py:122\u001b[0m, in \u001b[0;36mDataVendor.nday_fut_ret\u001b[0;34m(self, secid, date, nday, lag, ret_type)\u001b[0m\n\u001b[1;32m    120\u001b[0m date_min \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtd_offset(date\u001b[38;5;241m.\u001b[39mmin() , \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m    121\u001b[0m date_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtd_offset(\u001b[38;5;28mint\u001b[39m(date\u001b[38;5;241m.\u001b[39mmax()) , nday \u001b[38;5;241m+\u001b[39m lag \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_returns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate_min\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m full_date \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtd_within(date_min , date_max)\n\u001b[1;32m    125\u001b[0m block \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mday_ret\u001b[38;5;241m.\u001b[39malign(secid , full_date , [ret_type] , inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mas_tensor()\n",
      "File \u001b[0;32m~/Workspace/learndl/src/data/vendor.py:58\u001b[0m, in \u001b[0;36mDataVendor.get_returns\u001b[0;34m(self, start_dt, end_dt)\u001b[0m\n\u001b[1;32m     56\u001b[0m td_within \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtd_within(start_dt , end_dt)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday_ret\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misin(td_within , \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mday_ret\u001b[38;5;241m.\u001b[39mdate)\u001b[38;5;241m.\u001b[39mall()):\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mday_ret  \u001b[38;5;241m=\u001b[39m \u001b[43mGetData\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdaily_returns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_dt\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_dt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/learndl/src/data/core.py:79\u001b[0m, in \u001b[0;36mGetData.daily_returns\u001b[0;34m(cls, start_dt, end_dt)\u001b[0m\n\u001b[1;32m     77\u001b[0m pre_start_dt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(date_offset(start_dt , \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m20\u001b[39m))\n\u001b[1;32m     78\u001b[0m feature \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvwap\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 79\u001b[0m block \u001b[38;5;241m=\u001b[39m \u001b[43mBlockLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrade\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mday\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvwap\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madjfactor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpre_start_dt\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_dt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mas_tensor()\n\u001b[1;32m     80\u001b[0m block \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39madjust_price()\u001b[38;5;241m.\u001b[39malign_feature(feature)\n\u001b[1;32m     81\u001b[0m values \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mvalues[:,\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m/\u001b[39m block\u001b[38;5;241m.\u001b[39mvalues[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Workspace/learndl/src/data/core.py:111\u001b[0m, in \u001b[0;36mBlockLoader.load_block\u001b[0;34m(self, start_dt, end_dt, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m db_key \u001b[38;5;129;01min\u001b[39;00m db_keys:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Timer(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m --> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_src\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m blocks reading [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] DataBase\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 111\u001b[0m         blk \u001b[38;5;241m=\u001b[39m \u001b[43mDataBlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_db\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdb_src\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_key\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_dt\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_dt\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m         sub_blocks\u001b[38;5;241m.\u001b[39mappend(blk)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sub_blocks) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:  \n",
      "File \u001b[0;32m~/Workspace/learndl/src/data/core.py:251\u001b[0m, in \u001b[0;36mDataBlock.load_db\u001b[0;34m(cls, db_src, db_key, start_dt, end_dt, feature, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     feature \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m feature \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecid\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminute\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfactor_name\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m    250\u001b[0m dfs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfile_preprocess(load_target_file(db_src,db_key,date),date,feature) \u001b[38;5;28;01mfor\u001b[39;00m date \u001b[38;5;129;01min\u001b[39;00m target_dates]\n\u001b[0;32m--> 251\u001b[0m dfs \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m use_index \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecid\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminute\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminute\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dfs\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mvalues) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfactor_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfactor_name\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dfs\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(use_index) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m , use_index\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/reshape/concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    383\u001b[0m     objs,\n\u001b[1;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    393\u001b[0m )\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/reshape/concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[1;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[0;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/concat.py:177\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m    167\u001b[0m vals \u001b[38;5;241m=\u001b[39m [ju\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units]\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_extension:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# expected \"Union[_SupportsArray[dtype[Any]],\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;66;03m# _NestedSequence[_SupportsArray[dtype[Any]]]]\"\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_1d_only_ea_dtype(blk\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     values \u001b[38;5;241m=\u001b[39m concat_compat(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ea_compat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.api import DataAPI , Trainer\n",
    "DataAPI.reconstruct_train_data()\n",
    "Trainer.update_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device name: NVIDIA GeForce RTX 4090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:10:43|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:10:43|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Mon Jun 24 01:10:43 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Process Queue : Data + Fit + Test\n",
      "--Confirm Resume Training!\n",
      "--Model_name is set to gru_day!\n",
      "Callback : ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True) , reset optimizer on some epoch (can speedup scheduler)\n",
      "Callback : CallbackTimer(verbosity=2) , record time cost of callback hooks\n",
      "Callback : EarlyStoppage(patience=20) , stop fitting when validation score cease to improve\n",
      "Callback : ValidationConverge(patience=5,eps=1e-05) , stop fitting when valid_score converge\n",
      "Callback : EarlyExitRetrain(earliest=5,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100]) , retrain with new lr if fitting stopped too early\n",
      "Callback : NanLossRetrain(max_attempt=4) , retrain if fitting encounters nan loss\n",
      "Callback : BatchDisplay(verbosity=2) , display batch progress bar\n",
      "Callback : StatusDisplay(verbosity=2) , display epoch and event information\n",
      "{'random_seed': None,\n",
      " 'model_name': 'gruRES_day',\n",
      " 'model_module': 'gru',\n",
      " 'model_data_type': 'day',\n",
      " 'model_types': ['best', 'swalast', 'swabest'],\n",
      " 'labels': ['std_lag1_10'],\n",
      " 'beg_date': 20170103,\n",
      " 'end_date': 99991231,\n",
      " 'sample_method': 'train_shuffle',\n",
      " 'shuffle_option': 'epoch',\n",
      " 'lgbm_ensembler': False}\n",
      "{'hidden_dim': [32, 64],\n",
      " 'seqlens': [{'day': 30, '30m': 30, 'dms': 30}],\n",
      " 'tra_seqlens': [{'hist_loss': 40}],\n",
      " 'dropout': [0.1],\n",
      " 'enc_in': [True],\n",
      " 'enc_att': [False],\n",
      " 'rnn_type': ['lstm'],\n",
      " 'rnn_att': [False],\n",
      " 'rnn_layers': [2],\n",
      " 'dec_mlp_layers': [2],\n",
      " 'num_output': [1],\n",
      " 'kernel_size': [3],\n",
      " 'hidden_as_factor': [False],\n",
      " 'ordered_param_group': [False],\n",
      " 'tra_num_states': [5]}\n",
      "Load  2 DataBlocks...... finished! Cost 2.40 secs\n",
      "Align 2 DataBlocks...... finished! Cost 2.86 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:10:51|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 7.9 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:10:51|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Mon Jun 24 01:10:51 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n",
      "score function of [spearman] calculated and success!\n",
      "loss function of [pearson] calculated and success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mFirstBite Ep#  0 : loss  0.99770, train 0.00391, valid 0.03668, best 0.0367, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.87023, train 0.12798, valid 0.09558, best 0.0956, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.85418, train 0.14255, valid 0.08890, best 0.0956, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.84138, train 0.15293, valid 0.08662, best 0.0956, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.83995, train 0.15505, valid 0.09025, best 0.0956, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.83040, train 0.16270, valid 0.08154, best 0.0956, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.83040, train 0.16270, valid 0.08154, best 0.0956, lr3.1e-04, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  0 : loss  0.99907, train 0.00307, valid-0.00235, best-0.0024, lr1.3e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  5 : loss  0.89798, train 0.10238, valid 0.07134, best 0.0839, lr2.5e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 10 : loss  0.88484, train 0.11407, valid 0.08712, best 0.0892, lr1.9e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 15 : loss  0.87704, train 0.12033, valid 0.08682, best 0.0892, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 20 : loss  0.87598, train 0.12115, valid 0.08665, best 0.0892, lr9.4e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 25 : loss  0.87235, train 0.12489, valid 0.08559, best 0.0892, lr3.1e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 25 : loss  0.87235, train 0.12489, valid 0.08559, best 0.0892, lr3.1e-05, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  0 : loss  1.00802, train-0.01361, valid-0.04341, best-0.0434, lr1.3e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  5 : loss  0.90600, train 0.09749, valid 0.08519, best 0.0985, lr2.5e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 10 : loss  0.88704, train 0.11519, valid 0.10017, best 0.1002, lr1.9e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 15 : loss  0.88873, train 0.11359, valid 0.09794, best 0.1002, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 20 : loss  0.89135, train 0.11055, valid 0.09254, best 0.1002, lr9.4e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 25 : loss  0.88877, train 0.11064, valid 0.09957, best 0.1021, lr3.1e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 30 : loss  0.88839, train 0.10964, valid 0.08501, best 0.1021, lr1.6e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 35 : loss  0.88399, train 0.11418, valid 0.08975, best 0.1021, lr3.1e-03\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 39 , effective at epoch 40\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 40 : loss  0.88123, train 0.11494, valid 0.09168, best 0.1021, lr1.3e-02\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:16:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #1 @20231201|Retrain#2 Ep# 96 EarlyStop|Train 0.1037 Valid 0.0934 BestVal 0.0956|Cost  5.7Min,  3.5Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  1.00213, train-0.00346, valid-0.02870, best-0.0287, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.88101, train 0.11998, valid 0.10566, best 0.1118, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.86625, train 0.13116, valid 0.09471, best 0.1118, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.85764, train 0.13868, valid 0.09772, best 0.1118, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.85723, train 0.13898, valid 0.10404, best 0.1118, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 24 : loss  0.85347, train 0.14217, valid 0.09795, best 0.1118, lr1.6e-04, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  0 : loss  0.99619, train 0.00640, valid 0.02634, best 0.0263, lr1.3e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  5 : loss  0.91904, train 0.07677, valid 0.07904, best 0.0790, lr2.5e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 10 : loss  0.90607, train 0.09211, valid 0.09041, best 0.0904, lr1.9e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 15 : loss  0.89860, train 0.10379, valid 0.09553, best 0.0956, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 20 : loss  0.89763, train 0.10523, valid 0.09337, best 0.0956, lr9.4e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 24 : loss  0.89522, train 0.10913, valid 0.09689, best 0.0970, lr1.6e-05, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  0 : loss  1.00127, train-0.00532, valid-0.02776, best-0.0278, lr1.3e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  5 : loss  0.90841, train 0.09496, valid 0.09364, best 0.1105, lr2.5e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 10 : loss  0.88931, train 0.11243, valid 0.09934, best 0.1105, lr1.9e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 15 : loss  0.88411, train 0.11520, valid 0.09632, best 0.1105, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 20 : loss  0.88303, train 0.11682, valid 0.10441, best 0.1105, lr9.4e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 24 : loss  0.87640, train 0.12191, valid 0.10175, best 0.1105, lr1.6e-03, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  0 : loss  0.99942, train 0.00330, valid 0.03781, best 0.0378, lr1.3e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  5 : loss  0.96270, train 0.04376, valid 0.07316, best 0.0732, lr2.5e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 10 : loss  0.95829, train 0.04798, valid 0.07296, best 0.0736, lr1.9e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 15 : loss  0.95190, train 0.05224, valid 0.07273, best 0.0736, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 20 : loss  0.95024, train 0.05357, valid 0.07160, best 0.0736, lr9.5e-06\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 24 : loss  0.94829, train 0.05503, valid 0.07283, best 0.0736, lr1.7e-06, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  0 : loss  0.99623, train 0.00749, valid 0.03126, best 0.0313, lr1.3e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  5 : loss  0.97201, train 0.03669, valid 0.04230, best 0.0540, lr2.5e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 10 : loss  0.97588, train 0.03329, valid 0.03690, best 0.0540, lr1.9e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 15 : loss  0.97586, train 0.03358, valid 0.04005, best 0.0540, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 20 : loss  0.97032, train 0.04237, valid 0.04644, best 0.0540, lr9.4e-02\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:22:42|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #0 @20240604|Retrain#4 Ep#125 EarlyStop|Train 0.0369 Valid 0.0350 BestVal 0.1118|Cost  6.0Min,  2.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:24:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #1 @20240604|FirstBite Ep# 34 EarlyStop|Train 0.1667 Valid 0.0877 BestVal 0.0877|Cost  2.1Min,  3.6Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:24:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Fit], Cost 0.2 Hours, 4.7 Min/model, 3.3 Sec/Epoch\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:24:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Mon Jun 24 01:24:48 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:24:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mEach Model Date Testing Mean Score(spearman):\u001b[0m\n",
      "\u001b[32mModels            0       0       0       1       1       1\u001b[0m\n",
      "\u001b[32mOutput         best swalast swabest    best swalast swabest\u001b[0m\n",
      "\u001b[32m20170103     0.1536  0.1525  0.1520  0.1525  0.1521  0.1520\u001b[0m\n",
      "\u001b[32m20170704     0.1373  0.1327  0.1365  0.1413  0.1373  0.1405\u001b[0m\n",
      "\u001b[32m20171226     0.1305  0.1323  0.1329  0.1291  0.1298  0.1300\u001b[0m\n",
      "\u001b[32m20180627     0.1225  0.1186  0.1223  0.1226  0.1210  0.1236\u001b[0m\n",
      "\u001b[32m20181220     0.0998  0.0990  0.1004  0.0958  0.0981  0.0987\u001b[0m\n",
      "\u001b[32m20190624     0.0970  0.0955  0.0972  0.0920  0.0908  0.0924\u001b[0m\n",
      "\u001b[32m20191217     0.1011  0.0991  0.1005  0.1042  0.1048  0.1047\u001b[0m\n",
      "\u001b[32m20200617     0.0970  0.0963  0.0978  0.0947  0.0927  0.0946\u001b[0m\n",
      "\u001b[32m20201214     0.0837  0.0792  0.0842  0.0759  0.0766  0.0739\u001b[0m\n",
      "\u001b[32m20210615     0.0605  0.0572  0.0608  0.0689  0.0666  0.0662\u001b[0m\n",
      "\u001b[32m20211209     0.0935  0.0954  0.0962  0.1084  0.1106  0.1084\u001b[0m\n",
      "\u001b[32m20220613     0.0931  0.0909  0.0813  0.0857  0.0829  0.0876\u001b[0m\n",
      "\u001b[32m20221206     0.0650  0.0616  0.0635  0.0580  0.0576  0.0497\u001b[0m\n",
      "\u001b[32m20230606     0.0906  0.0857  0.0872  0.0868  0.0885  0.0856\u001b[0m\n",
      "\u001b[32m20231201     0.1217  0.1221  0.1235  0.1082  0.1025  0.1008\u001b[0m\n",
      "\u001b[32m20240604    -0.0130  0.0400 -0.0266  0.0401  0.0503  0.0572\u001b[0m\n",
      "\u001b[32mAllTimeAvg   0.1024  0.1008  0.1016  0.1012  0.1005  0.1003\u001b[0m\n",
      "\u001b[32mAllTimeSum   185.48  182.66  184.04  183.36  182.05  181.72\u001b[0m\n",
      "\u001b[32mStd          0.0668  0.0665  0.0669  0.0666  0.0656  0.0665\u001b[0m\n",
      "\u001b[32mTValue        65.23   64.56   64.64   64.69   65.24   64.18\u001b[0m\n",
      "\u001b[32mAnnIR        7.5077  7.4304  7.4389  7.4445  7.5079  7.3860\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:25:46|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 57.6 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:25:46|MOD:time        |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 15 Minutes 3.3 Seconds\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:25:46|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:25:46|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Mon Jun 24 01:25:46 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Process Queue : Data + Fit + Test\n",
      "--Confirm Resume Training!\n",
      "--Model_name is set to gruRTN_day!\n",
      "Callback : ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True) , reset optimizer on some epoch (can speedup scheduler)\n",
      "Callback : CallbackTimer(verbosity=2) , record time cost of callback hooks\n",
      "Callback : EarlyStoppage(patience=20) , stop fitting when validation score cease to improve\n",
      "Callback : ValidationConverge(patience=5,eps=1e-05) , stop fitting when valid_score converge\n",
      "Callback : EarlyExitRetrain(earliest=5,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100]) , retrain with new lr if fitting stopped too early\n",
      "Callback : NanLossRetrain(max_attempt=4) , retrain if fitting encounters nan loss\n",
      "Callback : BatchDisplay(verbosity=2) , display batch progress bar\n",
      "Callback : StatusDisplay(verbosity=2) , display epoch and event information\n",
      "{'random_seed': None,\n",
      " 'model_name': 'gruRTN_day',\n",
      " 'model_module': 'gru',\n",
      " 'model_data_type': 'day',\n",
      " 'model_types': ['best', 'swalast', 'swabest'],\n",
      " 'labels': ['rtn_lag1_10'],\n",
      " 'beg_date': 20170103,\n",
      " 'end_date': 99991231,\n",
      " 'sample_method': 'train_shuffle',\n",
      " 'shuffle_option': 'epoch',\n",
      " 'lgbm_ensembler': False}\n",
      "{'hidden_dim': [32, 64],\n",
      " 'seqlens': [{'day': 30, '30m': 30, 'dms': 30}],\n",
      " 'tra_seqlens': [{'hist_loss': 40}],\n",
      " 'dropout': [0.1],\n",
      " 'enc_in': [True],\n",
      " 'enc_att': [False],\n",
      " 'rnn_type': ['lstm'],\n",
      " 'rnn_att': [False],\n",
      " 'rnn_layers': [2],\n",
      " 'dec_mlp_layers': [2],\n",
      " 'num_output': [1],\n",
      " 'kernel_size': [3],\n",
      " 'hidden_as_factor': [False],\n",
      " 'ordered_param_group': [False],\n",
      " 'tra_num_states': [5]}\n",
      "try using /home/mengkjin/Workspace/learndl/data/DataSet/day.20240605.pt , success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:25:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 2.1 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:25:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Mon Jun 24 01:25:48 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mFirstBite Ep#  0 : loss  1.00382, train-0.00516, valid-0.03649, best-0.0365, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.86311, train 0.13764, valid 0.09228, best 0.0923, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.84547, train 0.15544, valid 0.09630, best 0.0963, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.83038, train 0.17066, valid 0.08826, best 0.0963, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.82732, train 0.17363, valid 0.08575, best 0.0963, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.81729, train 0.18370, valid 0.08407, best 0.0963, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 30 : loss  0.81421, train 0.18647, valid 0.08262, best 0.0963, lr1.6e-04\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:27:46|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRTN_day #1 @20231201|FirstBite Ep# 31 EarlyStop|Train 0.1865 Valid 0.0826 BestVal 0.0826|Cost  1.9Min,  3.6Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  1.00293, train-0.00341, valid-0.00344, best-0.0034, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.87709, train 0.12551, valid 0.08796, best 0.1129, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.86180, train 0.13731, valid 0.10436, best 0.1129, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.85287, train 0.14587, valid 0.10502, best 0.1129, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.85180, train 0.14778, valid 0.09660, best 0.1129, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 23 : loss  0.84699, train 0.15223, valid 0.10093, best 0.1129, lr1.0e-07, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  0 : loss  0.99854, train 0.00457, valid 0.01988, best 0.0199, lr1.3e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  5 : loss  0.92030, train 0.08103, valid 0.08255, best 0.0825, lr2.5e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 10 : loss  0.90702, train 0.09389, valid 0.08568, best 0.0857, lr1.9e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 15 : loss  0.89605, train 0.10568, valid 0.10030, best 0.1003, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 20 : loss  0.89414, train 0.10746, valid 0.09676, best 0.1003, lr9.4e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 23 : loss  0.89076, train 0.11050, valid 0.09963, best 0.1003, lr1.0e-07, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  0 : loss  0.99821, train 0.00317, valid 0.04276, best 0.0428, lr1.3e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  5 : loss  0.91434, train 0.07999, valid 0.07219, best 0.1080, lr2.5e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 10 : loss  0.89327, train 0.10612, valid 0.08880, best 0.1080, lr1.9e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 15 : loss  0.88616, train 0.11392, valid 0.08959, best 0.1080, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 20 : loss  0.88396, train 0.11643, valid 0.09105, best 0.1080, lr9.4e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 23 : loss  0.87573, train 0.12499, valid 0.08870, best 0.1080, lr1.0e-07, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  0 : loss  1.00152, train 0.00047, valid 0.02760, best 0.0276, lr1.3e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  5 : loss  0.96801, train 0.04157, valid 0.06108, best 0.0611, lr2.5e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 10 : loss  0.95807, train 0.04909, valid 0.05827, best 0.0611, lr1.9e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 15 : loss  0.95065, train 0.05443, valid 0.05588, best 0.0611, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 20 : loss  0.94877, train 0.05559, valid 0.05718, best 0.0611, lr9.5e-06\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 23 : loss  0.94672, train 0.05714, valid 0.05652, best 0.0611, lr1.0e-07, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  0 : loss  1.00047, train-0.00169, valid-0.00940, best-0.0094, lr1.3e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  5 : loss  0.97745, train 0.03642, valid 0.05397, best 0.0540, lr2.5e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 10 : loss  0.96087, train 0.04016, valid 0.04625, best 0.0540, lr1.9e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 15 : loss  0.97887, train 0.02974, valid 0.04515, best 0.0592, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 20 : loss  0.97797, train 0.03105, valid 0.02864, best 0.0592, lr9.4e-02\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:33:42|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRTN_day #0 @20240604|Retrain#4 Ep#120 EarlyStop|Train 0.0324 Valid 0.0466 BestVal 0.1129|Cost  5.9Min,  2.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:35:37|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRTN_day #1 @20240604|FirstBite Ep# 31 EarlyStop|Train 0.1916 Valid 0.0938 BestVal 0.0938|Cost  1.9Min,  3.6Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:35:37|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Fit], Cost 0.2 Hours, 3.3 Min/model, 3.2 Sec/Epoch\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:35:37|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Mon Jun 24 01:35:37 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:35:37|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mEach Model Date Testing Mean Score(spearman):\u001b[0m\n",
      "\u001b[32mModels            0       0       0       1       1       1\u001b[0m\n",
      "\u001b[32mOutput         best swalast swabest    best swalast swabest\u001b[0m\n",
      "\u001b[32m20170103     0.1415  0.1404  0.1427  0.1449  0.1435  0.1447\u001b[0m\n",
      "\u001b[32m20170704     0.1344  0.1262  0.1345  0.1305  0.1318  0.1309\u001b[0m\n",
      "\u001b[32m20171226     0.1443  0.1441  0.1454  0.1443  0.1440  0.1482\u001b[0m\n",
      "\u001b[32m20180627     0.1204  0.1149  0.1163  0.1097  0.1025  0.1077\u001b[0m\n",
      "\u001b[32m20181220     0.1134  0.1100  0.1116  0.1024  0.1004  0.1035\u001b[0m\n",
      "\u001b[32m20190624     0.0928  0.0923  0.0936  0.0977  0.0973  0.0969\u001b[0m\n",
      "\u001b[32m20191217     0.1172  0.1181  0.1180  0.1052  0.1068  0.1165\u001b[0m\n",
      "\u001b[32m20200617     0.0920  0.0939  0.0939  0.0902  0.0826  0.0929\u001b[0m\n",
      "\u001b[32m20201214     0.0986  0.0984  0.0981  0.0939  0.0944  0.0957\u001b[0m\n",
      "\u001b[32m20210615     0.0696  0.0720  0.0718  0.0754  0.0735  0.0734\u001b[0m\n",
      "\u001b[32m20211209     0.1126  0.1152  0.1148  0.1102  0.1128  0.1136\u001b[0m\n",
      "\u001b[32m20220613     0.1110  0.1046  0.1063  0.1074  0.1058  0.1127\u001b[0m\n",
      "\u001b[32m20221206     0.0574  0.0559  0.0579  0.0602  0.0553  0.0578\u001b[0m\n",
      "\u001b[32m20230606     0.0824  0.0776  0.0835  0.0856  0.0771  0.0824\u001b[0m\n",
      "\u001b[32m20231201     0.1327  0.1357  0.1289  0.1284  0.1280  0.1267\u001b[0m\n",
      "\u001b[32m20240604    -0.0291  0.0381  0.0338  0.0062  0.0266  0.0194\u001b[0m\n",
      "\u001b[32mAllTimeAvg   0.1071  0.1062  0.1073  0.1051  0.1032  0.1063\u001b[0m\n",
      "\u001b[32mAllTimeSum   194.08  192.38  194.48  190.38  187.02  192.67\u001b[0m\n",
      "\u001b[32mStd          0.0877  0.0881  0.0874  0.0877  0.0868  0.0861\u001b[0m\n",
      "\u001b[32mTValue        51.97   51.28   52.26   51.01   50.60   52.58\u001b[0m\n",
      "\u001b[32mAnnIR        5.9810  5.9022  6.0149  5.8711  5.8230  6.0509\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:36:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 54.9 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:36:32|MOD:time        |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 10 Minutes 45.7 Seconds\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:36:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:36:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Mon Jun 24 01:36:32 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Process Queue : Data + Fit + Test\n",
      "--Confirm Resume Training!\n",
      "--Model_name is set to gruRES_day!\n",
      "Callback : ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True) , reset optimizer on some epoch (can speedup scheduler)\n",
      "Callback : CallbackTimer(verbosity=2) , record time cost of callback hooks\n",
      "Callback : EarlyStoppage(patience=20) , stop fitting when validation score cease to improve\n",
      "Callback : ValidationConverge(patience=5,eps=1e-05) , stop fitting when valid_score converge\n",
      "Callback : EarlyExitRetrain(earliest=5,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100]) , retrain with new lr if fitting stopped too early\n",
      "Callback : NanLossRetrain(max_attempt=4) , retrain if fitting encounters nan loss\n",
      "Callback : BatchDisplay(verbosity=2) , display batch progress bar\n",
      "Callback : StatusDisplay(verbosity=2) , display epoch and event information\n",
      "{'random_seed': None,\n",
      " 'model_name': 'gruRES_day',\n",
      " 'model_module': 'gru',\n",
      " 'model_data_type': 'day',\n",
      " 'model_types': ['best', 'swalast', 'swabest'],\n",
      " 'labels': ['res_lag1_10'],\n",
      " 'beg_date': 20170103,\n",
      " 'end_date': 99991231,\n",
      " 'sample_method': 'train_shuffle',\n",
      " 'shuffle_option': 'epoch',\n",
      " 'lgbm_ensembler': False}\n",
      "{'hidden_dim': [32, 64],\n",
      " 'seqlens': [{'day': 30, '30m': 30, 'dms': 30}],\n",
      " 'tra_seqlens': [{'hist_loss': 40}],\n",
      " 'dropout': [0.1],\n",
      " 'enc_in': [True],\n",
      " 'enc_att': [False],\n",
      " 'rnn_type': ['lstm'],\n",
      " 'rnn_att': [False],\n",
      " 'rnn_layers': [2],\n",
      " 'dec_mlp_layers': [2],\n",
      " 'num_output': [1],\n",
      " 'kernel_size': [3],\n",
      " 'hidden_as_factor': [False],\n",
      " 'ordered_param_group': [False],\n",
      " 'tra_num_states': [5]}\n",
      "try using /home/mengkjin/Workspace/learndl/data/DataSet/day.20240605.pt , success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:36:34|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 2.0 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:36:34|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Mon Jun 24 01:36:34 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mFirstBite Ep#  0 : loss  1.01252, train-0.01521, valid-0.03742, best-0.0374, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.91026, train 0.08135, valid 0.03185, best 0.0599, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.89291, train 0.09436, valid 0.04522, best 0.0599, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.87755, train 0.10648, valid 0.03959, best 0.0599, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.87426, train 0.10831, valid 0.03543, best 0.0599, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 21 : loss  0.87082, train 0.11088, valid 0.03980, best 0.0599, lr6.3e-04, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  0 : loss  0.99637, train 0.00462, valid 0.03106, best 0.0311, lr1.3e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  5 : loss  0.94196, train 0.05698, valid 0.03578, best 0.0560, lr2.5e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 10 : loss  0.92553, train 0.06750, valid 0.03080, best 0.0560, lr1.9e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 15 : loss  0.91653, train 0.07499, valid 0.03537, best 0.0560, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 20 : loss  0.91509, train 0.07561, valid 0.04185, best 0.0560, lr9.4e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 21 : loss  0.91403, train 0.07721, valid 0.04641, best 0.0560, lr6.3e-05, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  0 : loss  1.00549, train-0.00683, valid-0.02416, best-0.0242, lr1.3e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  5 : loss  0.94359, train 0.04867, valid 0.02798, best 0.0514, lr2.5e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 10 : loss  0.93368, train 0.05600, valid 0.02787, best 0.0514, lr1.9e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 15 : loss  0.92716, train 0.06278, valid 0.03398, best 0.0514, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 20 : loss  0.92646, train 0.06384, valid 0.04303, best 0.0514, lr9.4e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 21 : loss  0.92585, train 0.06408, valid 0.03492, best 0.0514, lr6.3e-03, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  0 : loss  1.01260, train-0.01493, valid-0.03974, best-0.0397, lr1.3e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  5 : loss  0.96876, train 0.02948, valid 0.04494, best 0.0477, lr2.5e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 10 : loss  0.96445, train 0.03233, valid 0.04554, best 0.0477, lr1.9e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 15 : loss  0.96098, train 0.03497, valid 0.04404, best 0.0477, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 20 : loss  0.96022, train 0.03574, valid 0.04354, best 0.0477, lr9.5e-06\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 21 : loss  0.95992, train 0.03628, valid 0.04487, best 0.0477, lr6.3e-06, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  0 : loss  0.99746, train 0.00225, valid-0.00306, best-0.0031, lr1.3e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  5 : loss  0.97511, train 0.02839, valid 0.01301, best 0.0537, lr2.5e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 10 : loss  0.96875, train 0.03070, valid 0.01638, best 0.0537, lr1.9e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 15 : loss  0.96160, train 0.03236, valid 0.03529, best 0.0537, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 20 : loss  0.96655, train 0.02830, valid 0.04237, best 0.0537, lr9.4e-02\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:43:22|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #1 @20231201|Retrain#4 Ep#110 EarlyStop|Train 0.0293 Valid 0.0368 BestVal 0.0599|Cost  6.7Min,  3.6Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  0.99752, train 0.00292, valid 0.02356, best 0.0236, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.92738, train 0.06719, valid 0.04180, best 0.0533, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.90904, train 0.07964, valid 0.02435, best 0.0533, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.89805, train 0.08672, valid 0.03391, best 0.0533, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.89733, train 0.08698, valid 0.03455, best 0.0533, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 21 : loss  0.89541, train 0.08806, valid 0.04139, best 0.0533, lr6.3e-04, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  0 : loss  1.00733, train-0.00926, valid-0.03680, best-0.0368, lr1.3e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  5 : loss  0.95510, train 0.04285, valid 0.05432, best 0.0549, lr2.5e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 10 : loss  0.94688, train 0.04934, valid 0.04339, best 0.0549, lr1.9e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 15 : loss  0.93907, train 0.05436, valid 0.03791, best 0.0549, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 20 : loss  0.93641, train 0.05594, valid 0.03422, best 0.0549, lr9.4e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 22 : loss  0.93590, train 0.05702, valid 0.04172, best 0.0549, lr3.1e-05, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  0 : loss  0.99719, train 0.00375, valid 0.01564, best 0.0156, lr1.3e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  5 : loss  0.94177, train 0.04812, valid 0.03631, best 0.0465, lr2.5e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 10 : loss  0.93093, train 0.05774, valid 0.03864, best 0.0465, lr1.9e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 15 : loss  0.92785, train 0.05940, valid 0.02965, best 0.0465, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 20 : loss  0.92459, train 0.06304, valid 0.03018, best 0.0465, lr9.4e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 22 : loss  0.92154, train 0.06476, valid 0.03223, best 0.0465, lr3.1e-03, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  0 : loss  0.99855, train 0.00161, valid 0.01776, best 0.0178, lr1.3e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  5 : loss  0.97299, train 0.02829, valid 0.05143, best 0.0514, lr2.5e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 10 : loss  0.96918, train 0.02884, valid 0.05178, best 0.0518, lr1.9e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 15 : loss  0.96640, train 0.02942, valid 0.05233, best 0.0523, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 20 : loss  0.96524, train 0.02948, valid 0.05201, best 0.0523, lr9.5e-06\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 22 : loss  0.96442, train 0.03008, valid 0.05196, best 0.0523, lr3.2e-06, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  0 : loss  1.00164, train-0.00223, valid-0.02355, best-0.0236, lr1.3e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  5 : loss  0.97856, train 0.02428, valid 0.02481, best 0.0474, lr2.5e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 10 : loss  0.97707, train 0.02539, valid 0.04388, best 0.0474, lr1.9e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 15 : loss  0.97920, train 0.02491, valid 0.01606, best 0.0474, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 20 : loss  0.97573, train 0.02385, valid 0.01877, best 0.0474, lr9.4e-02\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:48:54|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #0 @20240604|Retrain#4 Ep#114 EarlyStop|Train 0.0265 Valid 0.0202 BestVal 0.0549|Cost  5.5Min,  2.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:56:07|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #1 @20240604|Retrain#4 Ep#115 EarlyStop|Train 0.0274 Valid 0.0224 BestVal 0.0568|Cost  7.2Min,  3.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:56:07|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Fit], Cost 0.3 Hours, 6.5 Min/model, 3.5 Sec/Epoch\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:56:07|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Mon Jun 24 01:56:07 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:56:07|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mEach Model Date Testing Mean Score(spearman):\u001b[0m\n",
      "\u001b[32mModels            0       0       0       1       1       1\u001b[0m\n",
      "\u001b[32mOutput         best swalast swabest    best swalast swabest\u001b[0m\n",
      "\u001b[32m20170103     0.1107  0.1119  0.1114  0.1060  0.1068  0.1081\u001b[0m\n",
      "\u001b[32m20170704     0.0937  0.0946  0.0972  0.0970  0.0989  0.0993\u001b[0m\n",
      "\u001b[32m20171226     0.0889  0.0892  0.0893  0.0901  0.0905  0.0905\u001b[0m\n",
      "\u001b[32m20180627     0.0713  0.0712  0.0713  0.0738  0.0715  0.0736\u001b[0m\n",
      "\u001b[32m20181220     0.0733  0.0742  0.0746  0.0740  0.0741  0.0728\u001b[0m\n",
      "\u001b[32m20190624     0.0702  0.0701  0.0703  0.0663  0.0672  0.0681\u001b[0m\n",
      "\u001b[32m20191217     0.0774  0.0745  0.0763  0.0789  0.0759  0.0750\u001b[0m\n",
      "\u001b[32m20200617     0.0555  0.0563  0.0570  0.0597  0.0579  0.0592\u001b[0m\n",
      "\u001b[32m20201214     0.0610  0.0605  0.0609  0.0619  0.0606  0.0642\u001b[0m\n",
      "\u001b[32m20210615     0.0406  0.0406  0.0438  0.0459  0.0475  0.0477\u001b[0m\n",
      "\u001b[32m20211209     0.0568  0.0572  0.0567  0.0561  0.0556  0.0573\u001b[0m\n",
      "\u001b[32m20220613     0.0457  0.0450  0.0465  0.0447  0.0461  0.0462\u001b[0m\n",
      "\u001b[32m20221206     0.0241  0.0235  0.0277  0.0287  0.0243  0.0296\u001b[0m\n",
      "\u001b[32m20230606     0.0589  0.0451  0.0425  0.0361  0.0324  0.0353\u001b[0m\n",
      "\u001b[32m20231201     0.0291  0.0323  0.0298  0.0140  0.0078  0.0160\u001b[0m\n",
      "\u001b[32m20240604    -0.0728 -0.0680 -0.0678 -0.0540  0.0016 -0.0326\u001b[0m\n",
      "\u001b[32mAllTimeAvg   0.0629  0.0622  0.0628  0.0614  0.0608  0.0622\u001b[0m\n",
      "\u001b[32mAllTimeSum   113.98  112.74  113.82  111.32  110.09  112.77\u001b[0m\n",
      "\u001b[32mStd          0.0537  0.0507  0.0500  0.0504  0.0493  0.0490\u001b[0m\n",
      "\u001b[32mTValue        49.87   52.28   53.42   51.92   52.51   54.01\u001b[0m\n",
      "\u001b[32mAnnIR        5.7394  6.0170  6.1485  5.9749  6.0436  6.2161\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:57:02|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 55.3 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:57:02|MOD:time        |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 20 Minutes 30.0 Seconds\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from src.api import Trainer\n",
    "Trainer.update_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
