{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 2.1.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available() , torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device name: NVIDIA GeForce RTX 4090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 07:59:31|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-08-26 07:59:31|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Mon Aug 26 07:59:31 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Process Queue : Data + Fit + Test\n",
      "--Model_name is set to gru_5year_day!\n",
      "Callback : ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True,kwargs={}) , reset optimizer on some epoch (can speedup scheduler)\n",
      "Callback : CallbackTimer(verbosity=2,kwargs={}) , record time cost of callback hooks\n",
      "Callback : EarlyStoppage(patience=20,kwargs={}) , stop fitting when validation score cease to improve\n",
      "Callback : ValidationConverge(patience=5,eps=1e-05,kwargs={}) , stop fitting when valid_score converge\n",
      "Callback : EarlyExitRetrain(earliest=10,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100, 1],kwargs={}) , retrain with new lr if fitting stopped too early\n",
      "Callback : NanLossRetrain(max_attempt=4,kwargs={}) , retrain if fitting encounters nan loss\n",
      "Callback : BatchDisplay(verbosity=2,kwargs={}) , display batch progress bar\n",
      "Callback : StatusDisplay(verbosity=2,kwargs={}) , display epoch and event information\n",
      "Callback : GroupReturnAnalysis(group_num=20,kwargs={}) , record and concat each model to Alpha model instance\n",
      "Callback : DetailedAlphaAnalysis(use_num=avg,kwargs={}) , record and concat each model to Alpha model instance\n",
      "{'model_name': 'gru_5year_day',\n",
      " 'model_module': 'gru',\n",
      " 'model.types': ['best', 'swalast', 'swabest'],\n",
      " 'model.booster_type': 'lgbm',\n",
      " 'model.booster_head': False,\n",
      " 'data.types': 'day',\n",
      " 'data.labels': ['std_lag1_10', 'rtn_lag1_10'],\n",
      " 'random_seed': None,\n",
      " 'beg_date': 20170103,\n",
      " 'end_date': 99991231,\n",
      " 'sample_method': 'sequential',\n",
      " 'shuffle_option': 'epoch'}\n",
      "{'hidden_dim': [32, 32, 32, 32, 32],\n",
      " 'seqlens': [{'day': 30, '30m': 30}],\n",
      " 'dropout': [0.1],\n",
      " 'enc_in': [True],\n",
      " 'enc_att': [False],\n",
      " 'rnn_type': ['gru'],\n",
      " 'rnn_att': [False],\n",
      " 'rnn_layers': [2],\n",
      " 'dec_mlp_layers': [2],\n",
      " 'num_output': [1],\n",
      " 'which_output': [0],\n",
      " 'kernel_size': [3],\n",
      " 'hidden_as_factor': [True],\n",
      " 'ordered_param_group': [False],\n",
      " 'verbosity': 2}\n",
      "Load  2 DataBlocks...... finished! Cost 2.49 secs\n",
      "Align 2 DataBlocks...... finished! Cost 2.84 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-08-26 07:59:40|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 8.4 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-08-26 07:59:40|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Mon Aug 26 07:59:40 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 07:59:40|MOD:mod         |\u001b[0m: \u001b[1m\u001b[34mFirst Iterance: (20170103 , 0)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n",
      "score function of [spearman] calculated and success!\n",
      "loss function of [pearson] calculated and success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mFirstBite Ep#  0 : loss  0.99168, train 0.01574, valid 0.03110, best 0.0311, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.89389, train 0.11484, valid 0.11039, best 0.1244, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.87245, train 0.13974, valid 0.13167, best 0.1333, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.86008, train 0.15132, valid 0.13084, best 0.1399, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.85873, train 0.15380, valid 0.13010, best 0.1399, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.85076, train 0.16153, valid 0.13461, best 0.1399, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 30 : loss  0.84930, train 0.16284, valid 0.13873, best 0.1428, lr1.6e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 35 : loss  0.84721, train 0.16530, valid 0.13469, best 0.1428, lr3.1e-04\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 39 , effective at epoch 40, and will speedup2x\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 40 : loss  0.84414, train 0.16815, valid 0.13819, best 0.1428, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 45 : loss  0.85999, train 0.15321, valid 0.12533, best 0.1483, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 50 : loss  0.84240, train 0.17036, valid 0.13767, best 0.1483, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 55 : loss  0.83564, train 0.17725, valid 0.14068, best 0.1483, lr1.0e-07\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 59 , effective at epoch 60, and will speedup2x\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 60 : loss  0.83299, train 0.17475, valid 0.12964, best 0.1505, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 65 : loss  0.84357, train 0.16995, valid 0.14568, best 0.1505, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 70 : loss  0.82820, train 0.18386, valid 0.12946, best 0.1505, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 75 : loss  0.82131, train 0.19062, valid 0.14798, best 0.1505, lr1.0e-07\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 08:02:50|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #0 @20170103|FirstBite Ep# 79 EarlyStop|Train 0.1909 Valid 0.1448 BestVal 0.1448|Cost  3.1Min,  2.3Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  1.00431, train-0.00679, valid-0.03157, best-0.0316, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.89891, train 0.11500, valid 0.12184, best 0.1218, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.87510, train 0.13807, valid 0.11829, best 0.1285, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.86046, train 0.15325, valid 0.13122, best 0.1334, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.86120, train 0.15239, valid 0.12810, best 0.1348, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.85265, train 0.16231, valid 0.13326, best 0.1363, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 30 : loss  0.84987, train 0.16333, valid 0.13646, best 0.1365, lr1.6e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 35 : loss  0.84899, train 0.16427, valid 0.13546, best 0.1365, lr3.1e-04\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 39 , effective at epoch 40, and will speedup2x\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 40 : loss  0.84675, train 0.16778, valid 0.13476, best 0.1387, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 45 : loss  0.85809, train 0.15618, valid 0.12957, best 0.1387, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 50 : loss  0.84509, train 0.16816, valid 0.13879, best 0.1451, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 55 : loss  0.83803, train 0.17494, valid 0.13885, best 0.1481, lr1.0e-07\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 59 , effective at epoch 60, and will speedup2x\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 60 : loss  0.83436, train 0.17998, valid 0.14204, best 0.1481, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 65 : loss  0.84712, train 0.16611, valid 0.11729, best 0.1481, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 70 : loss  0.83097, train 0.18174, valid 0.12355, best 0.1481, lr1.9e-03\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 08:05:43|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #1 @20170103|FirstBite Ep# 72 EarlyStop|Train 0.1780 Valid 0.1315 BestVal 0.1315|Cost  2.9Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  1.01212, train-0.01846, valid-0.03596, best-0.0360, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.90081, train 0.11116, valid 0.12831, best 0.1285, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.87655, train 0.13698, valid 0.13540, best 0.1414, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.86182, train 0.15231, valid 0.12636, best 0.1414, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.86130, train 0.15311, valid 0.12326, best 0.1431, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.85217, train 0.16185, valid 0.13722, best 0.1461, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 30 : loss  0.85096, train 0.16264, valid 0.14322, best 0.1461, lr1.6e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 35 : loss  0.84975, train 0.16391, valid 0.13433, best 0.1461, lr3.1e-04\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 39 , effective at epoch 40, and will speedup2x\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 40 : loss  0.84709, train 0.16765, valid 0.14284, best 0.1461, lr1.3e-03\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 08:07:24|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #2 @20170103|FirstBite Ep# 42 EarlyStop|Train 0.1582 Valid 0.1415 BestVal 0.1415|Cost  1.7Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  1.00733, train-0.01314, valid-0.02511, best-0.0251, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.91338, train 0.10474, valid 0.11651, best 0.1165, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.88054, train 0.13257, valid 0.13797, best 0.1407, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.86351, train 0.14932, valid 0.13609, best 0.1407, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.86409, train 0.14669, valid 0.13067, best 0.1407, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.85358, train 0.16106, valid 0.13318, best 0.1445, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 30 : loss  0.85158, train 0.16145, valid 0.13422, best 0.1445, lr1.6e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 35 : loss  0.84959, train 0.16339, valid 0.14002, best 0.1445, lr3.1e-04\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 39 , effective at epoch 40, and will speedup2x\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 40 : loss  0.84697, train 0.16580, valid 0.13816, best 0.1445, lr1.3e-03\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 08:09:05|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #3 @20170103|FirstBite Ep# 42 EarlyStop|Train 0.1590 Valid 0.1261 BestVal 0.1261|Cost  1.7Min,  2.3Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  0.99583, train 0.00703, valid 0.03880, best 0.0388, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.89806, train 0.11589, valid 0.11058, best 0.1368, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.87511, train 0.13886, valid 0.12069, best 0.1371, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.86117, train 0.15293, valid 0.13729, best 0.1420, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.86053, train 0.15269, valid 0.13331, best 0.1420, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.85117, train 0.16384, valid 0.13262, best 0.1420, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 30 : loss  0.84961, train 0.16482, valid 0.13259, best 0.1420, lr1.6e-04\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 08:10:25|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #4 @20170103|FirstBite Ep# 33 EarlyStop|Train 0.1656 Valid 0.1345 BestVal 0.1345|Cost  1.3Min,  2.3Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 08:13:10|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #0 @20170704|FirstBite Ep# 68 EarlyStop|Train 0.1833 Valid 0.1493 BestVal 0.1493|Cost  2.7Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 08:15:11|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #1 @20170704|FirstBite Ep# 50 EarlyStop|Train 0.1729 Valid 0.1397 BestVal 0.1397|Cost  2.0Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 08:19:13|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #2 @20170704|FirstBite Ep#101 EarlyStop|Train 0.2069 Valid 0.1675 BestVal 0.1675|Cost  4.0Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 08:23:15|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #3 @20170704|FirstBite Ep#101 EarlyStop|Train 0.2067 Valid 0.1580 BestVal 0.1580|Cost  4.0Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 08:25:55|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #4 @20170704|FirstBite Ep# 65 EarlyStop|Train 0.1625 Valid 0.1309 BestVal 0.1309|Cost  2.6Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 08:29:29|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #0 @20171226|FirstBite Ep# 85 EarlyStop|Train 0.2038 Valid 0.1475 BestVal 0.1475|Cost  3.5Min,  2.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 08:32:57|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #1 @20171226|FirstBite Ep# 85 EarlyStop|Train 0.2113 Valid 0.1496 BestVal 0.1496|Cost  3.5Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 08:37:10|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #2 @20171226|FirstBite Ep#100 EarlyStop|Train 0.2076 Valid 0.1499 BestVal 0.1499|Cost  4.2Min,  2.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 08:41:06|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #3 @20171226|FirstBite Ep# 94 EarlyStop|Train 0.2097 Valid 0.1526 BestVal 0.1526|Cost  3.9Min,  2.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 08:42:52|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #4 @20171226|FirstBite Ep# 44 EarlyStop|Train 0.1604 Valid 0.1473 BestVal 0.1473|Cost  1.8Min,  2.3Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 08:47:27|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #0 @20180627|FirstBite Ep#108 EarlyStop|Train 0.2185 Valid 0.1440 BestVal 0.1440|Cost  4.5Min,  2.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 08:50:43|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #1 @20180627|FirstBite Ep# 79 EarlyStop|Train 0.2080 Valid 0.1413 BestVal 0.1413|Cost  3.3Min,  2.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 08:54:24|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #2 @20180627|FirstBite Ep# 89 EarlyStop|Train 0.2104 Valid 0.1369 BestVal 0.1369|Cost  3.7Min,  2.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 08:58:35|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #3 @20180627|FirstBite Ep#102 EarlyStop|Train 0.2145 Valid 0.1403 BestVal 0.1403|Cost  4.2Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 09:01:44|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #4 @20180627|FirstBite Ep# 78 EarlyStop|Train 0.2077 Valid 0.1435 BestVal 0.1435|Cost  3.1Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 09:04:51|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #0 @20181220|FirstBite Ep# 76 EarlyStop|Train 0.2076 Valid 0.1404 BestVal 0.1404|Cost  3.1Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 09:08:08|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #1 @20181220|FirstBite Ep# 78 EarlyStop|Train 0.2085 Valid 0.1381 BestVal 0.1381|Cost  3.3Min,  2.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 09:09:51|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #2 @20181220|FirstBite Ep# 42 EarlyStop|Train 0.1840 Valid 0.1324 BestVal 0.1324|Cost  1.7Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 09:12:35|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #3 @20181220|FirstBite Ep# 68 EarlyStop|Train 0.2015 Valid 0.1356 BestVal 0.1356|Cost  2.7Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 09:14:34|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #4 @20181220|FirstBite Ep# 49 EarlyStop|Train 0.1969 Valid 0.1380 BestVal 0.1380|Cost  2.0Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 09:16:31|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #0 @20190624|FirstBite Ep# 48 EarlyStop|Train 0.1880 Valid 0.1373 BestVal 0.1373|Cost  1.9Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 09:19:22|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #1 @20190624|FirstBite Ep# 71 EarlyStop|Train 0.1951 Valid 0.1379 BestVal 0.1379|Cost  2.8Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 09:22:04|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #2 @20190624|FirstBite Ep# 67 EarlyStop|Train 0.1904 Valid 0.1307 BestVal 0.1307|Cost  2.7Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 09:24:39|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #3 @20190624|FirstBite Ep# 64 EarlyStop|Train 0.1819 Valid 0.1371 BestVal 0.1371|Cost  2.6Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 09:26:21|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #4 @20190624|FirstBite Ep# 42 EarlyStop|Train 0.1763 Valid 0.1468 BestVal 0.1468|Cost  1.7Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 09:28:55|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #0 @20191217|FirstBite Ep# 63 EarlyStop|Train 0.1915 Valid 0.1247 BestVal 0.1247|Cost  2.5Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 09:32:19|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #1 @20191217|FirstBite Ep# 85 EarlyStop|Train 0.2145 Valid 0.1154 BestVal 0.1154|Cost  3.4Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 09:37:22|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #2 @20191217|Retrain#4 Ep#125 EarlyStop|Train 0.0278 Valid 0.0588 BestVal 0.1163|Cost  5.0Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 09:39:04|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #3 @20191217|FirstBite Ep# 42 EarlyStop|Train 0.1797 Valid 0.1137 BestVal 0.1137|Cost  1.7Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 09:42:30|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #4 @20191217|FirstBite Ep# 85 EarlyStop|Train 0.2106 Valid 0.1167 BestVal 0.1167|Cost  3.4Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 09:47:20|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #0 @20200617|FirstBite Ep#102 EarlyStop|Train 0.2072 Valid 0.1020 BestVal 0.1020|Cost  4.8Min,  2.8Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 09:50:51|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #1 @20200617|FirstBite Ep# 84 EarlyStop|Train 0.2052 Valid 0.1001 BestVal 0.1001|Cost  3.5Min,  2.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 09:53:50|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #2 @20200617|FirstBite Ep# 90 EarlyStop|Train 0.2101 Valid 0.1037 BestVal 0.1037|Cost  3.0Min,  2.0Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 09:57:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #3 @20200617|FirstBite Ep#101 EarlyStop|Train 0.2055 Valid 0.1005 BestVal 0.1005|Cost  4.0Min,  2.3Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 09:59:27|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #4 @20200617|FirstBite Ep# 35 EarlyStop|Train 0.1835 Valid 0.0943 BestVal 0.0943|Cost  1.6Min,  2.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 10:01:51|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #0 @20201214|FirstBite Ep# 57 EarlyStop|Train 0.1805 Valid 0.0962 BestVal 0.0962|Cost  2.4Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 10:05:50|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #1 @20201214|FirstBite Ep# 88 EarlyStop|Train 0.1930 Valid 0.0894 BestVal 0.0894|Cost  4.0Min,  2.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 10:09:11|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #2 @20201214|FirstBite Ep# 81 EarlyStop|Train 0.1915 Valid 0.0923 BestVal 0.0923|Cost  3.3Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 10:10:29|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #3 @20201214|FirstBite Ep# 32 EarlyStop|Train 0.1674 Valid 0.0919 BestVal 0.0919|Cost  1.3Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 10:13:31|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #4 @20201214|FirstBite Ep# 74 EarlyStop|Train 0.1833 Valid 0.0879 BestVal 0.0879|Cost  3.0Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 10:17:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #0 @20210615|FirstBite Ep# 94 EarlyStop|Train 0.1774 Valid 0.0925 BestVal 0.0925|Cost  4.0Min,  2.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 10:21:08|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #1 @20210615|FirstBite Ep# 87 EarlyStop|Train 0.1759 Valid 0.0946 BestVal 0.0946|Cost  3.6Min,  2.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 10:24:04|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #2 @20210615|FirstBite Ep# 72 EarlyStop|Train 0.1682 Valid 0.0918 BestVal 0.0918|Cost  2.9Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 10:26:06|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #3 @20210615|FirstBite Ep# 50 EarlyStop|Train 0.1614 Valid 0.0908 BestVal 0.0908|Cost  2.0Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 10:28:52|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #4 @20210615|FirstBite Ep# 68 EarlyStop|Train 0.1651 Valid 0.0934 BestVal 0.0934|Cost  2.8Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 10:30:12|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #0 @20211209|FirstBite Ep# 32 EarlyStop|Train 0.1331 Valid 0.0731 BestVal 0.0731|Cost  1.3Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 10:32:10|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #1 @20211209|FirstBite Ep# 49 EarlyStop|Train 0.1430 Valid 0.0705 BestVal 0.0705|Cost  2.0Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 10:36:02|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #2 @20211209|FirstBite Ep# 95 EarlyStop|Train 0.1592 Valid 0.0727 BestVal 0.0727|Cost  3.9Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 10:37:44|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #3 @20211209|FirstBite Ep# 48 EarlyStop|Train 0.1407 Valid 0.0771 BestVal 0.0771|Cost  1.7Min,  2.1Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 10:41:08|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #4 @20211209|FirstBite Ep# 83 EarlyStop|Train 0.1577 Valid 0.0762 BestVal 0.0762|Cost  3.4Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 10:42:33|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #0 @20220613|FirstBite Ep# 32 EarlyStop|Train 0.1236 Valid 0.0728 BestVal 0.0728|Cost  1.4Min,  2.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 10:44:15|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #1 @20220613|FirstBite Ep# 40 EarlyStop|Train 0.1269 Valid 0.0750 BestVal 0.0750|Cost  1.7Min,  2.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 10:45:52|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #2 @20220613|FirstBite Ep# 39 EarlyStop|Train 0.1261 Valid 0.0744 BestVal 0.0744|Cost  1.6Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 10:48:42|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #3 @20220613|FirstBite Ep# 67 EarlyStop|Train 0.1326 Valid 0.0789 BestVal 0.0789|Cost  2.8Min,  2.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 10:51:20|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #4 @20220613|FirstBite Ep# 64 EarlyStop|Train 0.1235 Valid 0.0768 BestVal 0.0768|Cost  2.6Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 10:52:49|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #0 @20221206|FirstBite Ep# 34 EarlyStop|Train 0.1130 Valid 0.0783 BestVal 0.0783|Cost  1.4Min,  2.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 10:54:33|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #1 @20221206|FirstBite Ep# 41 EarlyStop|Train 0.1126 Valid 0.0791 BestVal 0.0791|Cost  1.7Min,  2.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 11:00:14|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #2 @20221206|Retrain#4 Ep#135 EarlyStop|Train 0.0334 Valid 0.0467 BestVal 0.0887|Cost  5.7Min,  2.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 11:03:38|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #3 @20221206|FirstBite Ep# 85 EarlyStop|Train 0.1339 Valid 0.0741 BestVal 0.0741|Cost  3.4Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 11:08:44|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #4 @20221206|Retrain#4 Ep#128 EarlyStop|Train 0.0330 Valid 0.0522 BestVal 0.0912|Cost  5.1Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 11:10:18|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #0 @20230606|FirstBite Ep# 39 EarlyStop|Train 0.1060 Valid 0.0736 BestVal 0.0736|Cost  1.5Min,  2.3Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 11:11:42|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #1 @20230606|FirstBite Ep# 33 EarlyStop|Train 0.1087 Valid 0.0757 BestVal 0.0757|Cost  1.4Min,  2.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 11:17:00|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #2 @20230606|Retrain#4 Ep#130 EarlyStop|Train 0.0353 Valid 0.0558 BestVal 0.1100|Cost  5.3Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 11:18:23|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #3 @20230606|FirstBite Ep# 34 EarlyStop|Train 0.1052 Valid 0.0681 BestVal 0.0681|Cost  1.4Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 11:24:01|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #4 @20230606|Retrain#4 Ep#135 EarlyStop|Train 0.0394 Valid 0.0617 BestVal 0.1074|Cost  5.6Min,  2.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 11:30:25|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #0 @20231201|Retrain#4 Ep#150 EarlyStop|Train 0.0388 Valid 0.0413 BestVal 0.1025|Cost  6.4Min,  2.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 11:36:37|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #1 @20231201|Retrain#4 Ep#135 EarlyStop|Train 0.0424 Valid 0.0536 BestVal 0.1014|Cost  6.2Min,  2.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 11:39:16|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #2 @20231201|Retrain#1 Ep# 59 EarlyStop|Train 0.0871 Valid 0.0954 BestVal 0.0961|Cost  2.6Min,  2.6Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 11:45:50|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #3 @20231201|Retrain#4 Ep#145 EarlyStop|Train 0.0423 Valid 0.0616 BestVal 0.1059|Cost  6.6Min,  2.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 11:47:28|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #4 @20231201|FirstBite Ep# 35 EarlyStop|Train 0.1018 Valid 0.0746 BestVal 0.0746|Cost  1.6Min,  2.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 11:48:56|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #0 @20240604|FirstBite Ep# 34 EarlyStop|Train 0.0995 Valid 0.0896 BestVal 0.0896|Cost  1.4Min,  2.4Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 11:54:38|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #1 @20240604|Retrain#4 Ep#135 EarlyStop|Train 0.0471 Valid 0.0514 BestVal 0.1197|Cost  5.7Min,  2.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 11:56:00|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #2 @20240604|FirstBite Ep# 32 EarlyStop|Train 0.1005 Valid 0.1007 BestVal 0.1007|Cost  1.4Min,  2.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 12:02:19|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #3 @20240604|Retrain#4 Ep#150 EarlyStop|Train 0.0359 Valid 0.0321 BestVal 0.1080|Cost  6.3Min,  2.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 12:03:51|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgru_5year_day #4 @20240604|FirstBite Ep# 36 EarlyStop|Train 0.1028 Valid 0.1046 BestVal 0.1046|Cost  1.5Min,  2.5Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-08-26 12:03:51|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Fit], Cost 4.1 Hours, 3.1 Min/model, 2.5 Sec/Epoch\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-08-26 12:03:51|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Mon Aug 26 12:03:51 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-08-26 12:06:10|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mTesting Mean Score(spearman):\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">gru.0</th>\n",
       "      <th colspan=\"3\" halign=\"left\">gru.1</th>\n",
       "      <th colspan=\"3\" halign=\"left\">gru.2</th>\n",
       "      <th colspan=\"3\" halign=\"left\">gru.3</th>\n",
       "      <th colspan=\"3\" halign=\"left\">gru.4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>best</th>\n",
       "      <th>swalast</th>\n",
       "      <th>swabest</th>\n",
       "      <th>best</th>\n",
       "      <th>swalast</th>\n",
       "      <th>swabest</th>\n",
       "      <th>best</th>\n",
       "      <th>swalast</th>\n",
       "      <th>swabest</th>\n",
       "      <th>best</th>\n",
       "      <th>swalast</th>\n",
       "      <th>swabest</th>\n",
       "      <th>best</th>\n",
       "      <th>swalast</th>\n",
       "      <th>swabest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20170103</th>\n",
       "      <td>0.135</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20170704</th>\n",
       "      <td>0.140</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20171226</th>\n",
       "      <td>0.122</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180627</th>\n",
       "      <td>0.141</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181220</th>\n",
       "      <td>0.109</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190624</th>\n",
       "      <td>0.097</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20191217</th>\n",
       "      <td>0.083</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20200617</th>\n",
       "      <td>0.106</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201214</th>\n",
       "      <td>0.087</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20210615</th>\n",
       "      <td>0.081</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211209</th>\n",
       "      <td>0.101</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20220613</th>\n",
       "      <td>0.085</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20221206</th>\n",
       "      <td>0.054</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20230606</th>\n",
       "      <td>0.116</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20231201</th>\n",
       "      <td>0.089</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20240604</th>\n",
       "      <td>0.085</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg</th>\n",
       "      <td>0.103</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sum</th>\n",
       "      <td>188.294</td>\n",
       "      <td>184.877</td>\n",
       "      <td>186.534</td>\n",
       "      <td>189.304</td>\n",
       "      <td>186.114</td>\n",
       "      <td>188.201</td>\n",
       "      <td>184.212</td>\n",
       "      <td>179.445</td>\n",
       "      <td>183.941</td>\n",
       "      <td>183.395</td>\n",
       "      <td>180.088</td>\n",
       "      <td>188.296</td>\n",
       "      <td>187.491</td>\n",
       "      <td>185.068</td>\n",
       "      <td>186.677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Std</th>\n",
       "      <td>0.074</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>59.740</td>\n",
       "      <td>59.853</td>\n",
       "      <td>59.813</td>\n",
       "      <td>61.005</td>\n",
       "      <td>61.101</td>\n",
       "      <td>60.879</td>\n",
       "      <td>59.067</td>\n",
       "      <td>57.394</td>\n",
       "      <td>59.351</td>\n",
       "      <td>58.635</td>\n",
       "      <td>57.974</td>\n",
       "      <td>60.292</td>\n",
       "      <td>58.663</td>\n",
       "      <td>59.310</td>\n",
       "      <td>60.053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IR</th>\n",
       "      <td>6.840</td>\n",
       "      <td>6.853</td>\n",
       "      <td>6.848</td>\n",
       "      <td>6.984</td>\n",
       "      <td>6.995</td>\n",
       "      <td>6.970</td>\n",
       "      <td>6.762</td>\n",
       "      <td>6.571</td>\n",
       "      <td>6.795</td>\n",
       "      <td>6.713</td>\n",
       "      <td>6.637</td>\n",
       "      <td>6.903</td>\n",
       "      <td>6.716</td>\n",
       "      <td>6.790</td>\n",
       "      <td>6.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          gru.0                      gru.1                      gru.2                      gru.3                      gru.4                    \n",
       "           best   swalast  swabest    best   swalast  swabest    best   swalast  swabest    best   swalast  swabest    best   swalast  swabest \n",
       "20170103    0.135    0.132    0.137    0.149    0.137    0.139    0.127    0.118    0.125    0.127    0.127    0.139    0.123    0.117    0.128\n",
       "20170704    0.140    0.135    0.141    0.135    0.132    0.136    0.138    0.133    0.138    0.145    0.141    0.146    0.123    0.138    0.137\n",
       "20171226    0.122    0.130    0.128    0.123    0.135    0.130    0.131    0.132    0.131    0.123    0.129    0.129    0.138    0.138    0.137\n",
       "20180627    0.141    0.142    0.141    0.151    0.147    0.149    0.147    0.139    0.143    0.135    0.137    0.140    0.153    0.149    0.150\n",
       "20181220    0.109    0.105    0.112    0.109    0.111    0.110    0.107    0.105    0.107    0.110    0.111    0.113    0.106    0.113    0.113\n",
       "20190624    0.097    0.098    0.099    0.099    0.100    0.099    0.102    0.095    0.098    0.102    0.102    0.102    0.099    0.095    0.099\n",
       "20191217    0.083    0.088    0.089    0.092    0.095    0.092    0.070    0.073    0.081    0.078    0.084    0.084    0.075    0.095    0.088\n",
       "20200617    0.106    0.102    0.103    0.106    0.105    0.107    0.103    0.105    0.105    0.102    0.101    0.104    0.100    0.100    0.101\n",
       "20201214    0.087    0.088    0.088    0.090    0.090    0.090    0.091    0.092    0.089    0.083    0.085    0.087    0.089    0.090    0.092\n",
       "20210615    0.081    0.075    0.076    0.080    0.074    0.077    0.072    0.072    0.074    0.071    0.070    0.073    0.070    0.068    0.069\n",
       "20211209    0.101    0.099    0.101    0.098    0.096    0.099    0.098    0.094    0.098    0.102    0.105    0.103    0.095    0.096    0.099\n",
       "20220613    0.085    0.091    0.091    0.083    0.087    0.088    0.086    0.084    0.086    0.095    0.081    0.089    0.084    0.087    0.085\n",
       "20221206    0.054    0.061    0.054    0.077    0.056    0.070    0.066    0.069    0.057    0.054    0.044    0.052    0.082    0.040    0.056\n",
       "20230606    0.116    0.113    0.107    0.096    0.096    0.098    0.102    0.095    0.105    0.113    0.107    0.103    0.109    0.109    0.111\n",
       "20231201    0.089    0.064    0.067    0.071    0.075    0.066    0.074    0.070    0.073    0.070    0.065    0.085    0.096    0.088    0.068\n",
       "20240604    0.085    0.073    0.083    0.067    0.062    0.074    0.084    0.075    0.087    0.077    0.059    0.077    0.087    0.082    0.090\n",
       "Avg         0.103    0.101    0.102    0.103    0.102    0.103    0.101    0.098    0.101    0.100    0.098    0.103    0.102    0.101    0.102\n",
       "Sum       188.294  184.877  186.534  189.304  186.114  188.201  184.212  179.445  183.941  183.395  180.088  188.296  187.491  185.068  186.677\n",
       "Std         0.074    0.072    0.073    0.072    0.071    0.072    0.073    0.073    0.072    0.073    0.073    0.073    0.075    0.073    0.073\n",
       "T          59.740   59.853   59.813   61.005   61.101   60.879   59.067   57.394   59.351   58.635   57.974   60.292   58.663   59.310   60.053\n",
       "IR          6.840    6.853    6.848    6.984    6.995    6.970    6.762    6.571    6.795    6.713    6.637    6.903    6.716    6.790    6.875"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-08-26 12:06:11|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 140.0 Secs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results are saved to /home/mengkjin/Workspace/learndl/models/gru_5year_day/detailed_analysis/test.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num</th>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>best</th>\n",
       "      <td>-1.993%</td>\n",
       "      <td>-0.718%</td>\n",
       "      <td>-0.404%</td>\n",
       "      <td>-0.211%</td>\n",
       "      <td>-0.093%</td>\n",
       "      <td>-0.034%</td>\n",
       "      <td>-0.015%</td>\n",
       "      <td>0.058%</td>\n",
       "      <td>0.120%</td>\n",
       "      <td>0.153%</td>\n",
       "      <td>0.189%</td>\n",
       "      <td>0.242%</td>\n",
       "      <td>0.191%</td>\n",
       "      <td>0.258%</td>\n",
       "      <td>0.308%</td>\n",
       "      <td>0.320%</td>\n",
       "      <td>0.407%</td>\n",
       "      <td>0.404%</td>\n",
       "      <td>0.386%</td>\n",
       "      <td>0.495%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swabest</th>\n",
       "      <td>-2.007%</td>\n",
       "      <td>-0.733%</td>\n",
       "      <td>-0.418%</td>\n",
       "      <td>-0.259%</td>\n",
       "      <td>-0.159%</td>\n",
       "      <td>-0.066%</td>\n",
       "      <td>0.002%</td>\n",
       "      <td>0.021%</td>\n",
       "      <td>0.065%</td>\n",
       "      <td>0.119%</td>\n",
       "      <td>0.254%</td>\n",
       "      <td>0.221%</td>\n",
       "      <td>0.255%</td>\n",
       "      <td>0.267%</td>\n",
       "      <td>0.335%</td>\n",
       "      <td>0.334%</td>\n",
       "      <td>0.406%</td>\n",
       "      <td>0.450%</td>\n",
       "      <td>0.483%</td>\n",
       "      <td>0.499%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swalast</th>\n",
       "      <td>-1.981%</td>\n",
       "      <td>-0.743%</td>\n",
       "      <td>-0.419%</td>\n",
       "      <td>-0.264%</td>\n",
       "      <td>-0.227%</td>\n",
       "      <td>-0.061%</td>\n",
       "      <td>-0.044%</td>\n",
       "      <td>0.069%</td>\n",
       "      <td>0.074%</td>\n",
       "      <td>0.077%</td>\n",
       "      <td>0.184%</td>\n",
       "      <td>0.240%</td>\n",
       "      <td>0.264%</td>\n",
       "      <td>0.248%</td>\n",
       "      <td>0.361%</td>\n",
       "      <td>0.346%</td>\n",
       "      <td>0.416%</td>\n",
       "      <td>0.445%</td>\n",
       "      <td>0.508%</td>\n",
       "      <td>0.573%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>best</th>\n",
       "      <td>-1.911%</td>\n",
       "      <td>-0.771%</td>\n",
       "      <td>-0.403%</td>\n",
       "      <td>-0.257%</td>\n",
       "      <td>-0.137%</td>\n",
       "      <td>-0.044%</td>\n",
       "      <td>0.032%</td>\n",
       "      <td>0.029%</td>\n",
       "      <td>0.107%</td>\n",
       "      <td>0.164%</td>\n",
       "      <td>0.144%</td>\n",
       "      <td>0.255%</td>\n",
       "      <td>0.239%</td>\n",
       "      <td>0.245%</td>\n",
       "      <td>0.290%</td>\n",
       "      <td>0.329%</td>\n",
       "      <td>0.391%</td>\n",
       "      <td>0.425%</td>\n",
       "      <td>0.445%</td>\n",
       "      <td>0.496%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swabest</th>\n",
       "      <td>-1.974%</td>\n",
       "      <td>-0.768%</td>\n",
       "      <td>-0.389%</td>\n",
       "      <td>-0.252%</td>\n",
       "      <td>-0.174%</td>\n",
       "      <td>-0.073%</td>\n",
       "      <td>0.037%</td>\n",
       "      <td>0.054%</td>\n",
       "      <td>0.105%</td>\n",
       "      <td>0.094%</td>\n",
       "      <td>0.191%</td>\n",
       "      <td>0.194%</td>\n",
       "      <td>0.245%</td>\n",
       "      <td>0.271%</td>\n",
       "      <td>0.333%</td>\n",
       "      <td>0.356%</td>\n",
       "      <td>0.359%</td>\n",
       "      <td>0.450%</td>\n",
       "      <td>0.470%</td>\n",
       "      <td>0.535%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swalast</th>\n",
       "      <td>-1.986%</td>\n",
       "      <td>-0.797%</td>\n",
       "      <td>-0.446%</td>\n",
       "      <td>-0.232%</td>\n",
       "      <td>-0.166%</td>\n",
       "      <td>-0.123%</td>\n",
       "      <td>-0.029%</td>\n",
       "      <td>0.047%</td>\n",
       "      <td>0.083%</td>\n",
       "      <td>0.174%</td>\n",
       "      <td>0.141%</td>\n",
       "      <td>0.199%</td>\n",
       "      <td>0.247%</td>\n",
       "      <td>0.327%</td>\n",
       "      <td>0.356%</td>\n",
       "      <td>0.386%</td>\n",
       "      <td>0.385%</td>\n",
       "      <td>0.469%</td>\n",
       "      <td>0.481%</td>\n",
       "      <td>0.555%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>best</th>\n",
       "      <td>-1.826%</td>\n",
       "      <td>-0.798%</td>\n",
       "      <td>-0.381%</td>\n",
       "      <td>-0.275%</td>\n",
       "      <td>-0.112%</td>\n",
       "      <td>0.000%</td>\n",
       "      <td>-0.019%</td>\n",
       "      <td>0.064%</td>\n",
       "      <td>0.123%</td>\n",
       "      <td>0.063%</td>\n",
       "      <td>0.185%</td>\n",
       "      <td>0.240%</td>\n",
       "      <td>0.285%</td>\n",
       "      <td>0.258%</td>\n",
       "      <td>0.315%</td>\n",
       "      <td>0.319%</td>\n",
       "      <td>0.367%</td>\n",
       "      <td>0.418%</td>\n",
       "      <td>0.418%</td>\n",
       "      <td>0.416%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swabest</th>\n",
       "      <td>-1.935%</td>\n",
       "      <td>-0.693%</td>\n",
       "      <td>-0.412%</td>\n",
       "      <td>-0.297%</td>\n",
       "      <td>-0.174%</td>\n",
       "      <td>-0.009%</td>\n",
       "      <td>0.044%</td>\n",
       "      <td>0.060%</td>\n",
       "      <td>0.097%</td>\n",
       "      <td>0.134%</td>\n",
       "      <td>0.181%</td>\n",
       "      <td>0.227%</td>\n",
       "      <td>0.251%</td>\n",
       "      <td>0.298%</td>\n",
       "      <td>0.307%</td>\n",
       "      <td>0.338%</td>\n",
       "      <td>0.339%</td>\n",
       "      <td>0.400%</td>\n",
       "      <td>0.454%</td>\n",
       "      <td>0.454%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swalast</th>\n",
       "      <td>-1.911%</td>\n",
       "      <td>-0.730%</td>\n",
       "      <td>-0.416%</td>\n",
       "      <td>-0.319%</td>\n",
       "      <td>-0.137%</td>\n",
       "      <td>-0.026%</td>\n",
       "      <td>-0.022%</td>\n",
       "      <td>0.090%</td>\n",
       "      <td>0.109%</td>\n",
       "      <td>0.174%</td>\n",
       "      <td>0.198%</td>\n",
       "      <td>0.225%</td>\n",
       "      <td>0.218%</td>\n",
       "      <td>0.348%</td>\n",
       "      <td>0.324%</td>\n",
       "      <td>0.329%</td>\n",
       "      <td>0.379%</td>\n",
       "      <td>0.362%</td>\n",
       "      <td>0.440%</td>\n",
       "      <td>0.428%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">3</th>\n",
       "      <th>best</th>\n",
       "      <td>-1.918%</td>\n",
       "      <td>-0.655%</td>\n",
       "      <td>-0.391%</td>\n",
       "      <td>-0.178%</td>\n",
       "      <td>-0.093%</td>\n",
       "      <td>-0.034%</td>\n",
       "      <td>0.063%</td>\n",
       "      <td>0.026%</td>\n",
       "      <td>0.187%</td>\n",
       "      <td>0.129%</td>\n",
       "      <td>0.163%</td>\n",
       "      <td>0.173%</td>\n",
       "      <td>0.210%</td>\n",
       "      <td>0.218%</td>\n",
       "      <td>0.289%</td>\n",
       "      <td>0.355%</td>\n",
       "      <td>0.380%</td>\n",
       "      <td>0.362%</td>\n",
       "      <td>0.387%</td>\n",
       "      <td>0.398%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swabest</th>\n",
       "      <td>-1.947%</td>\n",
       "      <td>-0.690%</td>\n",
       "      <td>-0.348%</td>\n",
       "      <td>-0.242%</td>\n",
       "      <td>-0.138%</td>\n",
       "      <td>-0.049%</td>\n",
       "      <td>0.049%</td>\n",
       "      <td>0.052%</td>\n",
       "      <td>0.095%</td>\n",
       "      <td>0.142%</td>\n",
       "      <td>0.160%</td>\n",
       "      <td>0.182%</td>\n",
       "      <td>0.207%</td>\n",
       "      <td>0.246%</td>\n",
       "      <td>0.294%</td>\n",
       "      <td>0.342%</td>\n",
       "      <td>0.383%</td>\n",
       "      <td>0.386%</td>\n",
       "      <td>0.487%</td>\n",
       "      <td>0.462%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swalast</th>\n",
       "      <td>-1.917%</td>\n",
       "      <td>-0.721%</td>\n",
       "      <td>-0.352%</td>\n",
       "      <td>-0.228%</td>\n",
       "      <td>-0.134%</td>\n",
       "      <td>-0.057%</td>\n",
       "      <td>0.022%</td>\n",
       "      <td>0.074%</td>\n",
       "      <td>0.041%</td>\n",
       "      <td>0.138%</td>\n",
       "      <td>0.198%</td>\n",
       "      <td>0.184%</td>\n",
       "      <td>0.219%</td>\n",
       "      <td>0.247%</td>\n",
       "      <td>0.326%</td>\n",
       "      <td>0.318%</td>\n",
       "      <td>0.397%</td>\n",
       "      <td>0.426%</td>\n",
       "      <td>0.443%</td>\n",
       "      <td>0.447%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4</th>\n",
       "      <th>best</th>\n",
       "      <td>-1.815%</td>\n",
       "      <td>-0.747%</td>\n",
       "      <td>-0.502%</td>\n",
       "      <td>-0.255%</td>\n",
       "      <td>-0.171%</td>\n",
       "      <td>-0.078%</td>\n",
       "      <td>-0.024%</td>\n",
       "      <td>0.135%</td>\n",
       "      <td>0.114%</td>\n",
       "      <td>0.137%</td>\n",
       "      <td>0.179%</td>\n",
       "      <td>0.208%</td>\n",
       "      <td>0.221%</td>\n",
       "      <td>0.314%</td>\n",
       "      <td>0.272%</td>\n",
       "      <td>0.337%</td>\n",
       "      <td>0.388%</td>\n",
       "      <td>0.394%</td>\n",
       "      <td>0.461%</td>\n",
       "      <td>0.480%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swabest</th>\n",
       "      <td>-1.953%</td>\n",
       "      <td>-0.789%</td>\n",
       "      <td>-0.444%</td>\n",
       "      <td>-0.304%</td>\n",
       "      <td>-0.163%</td>\n",
       "      <td>-0.046%</td>\n",
       "      <td>0.008%</td>\n",
       "      <td>0.054%</td>\n",
       "      <td>0.139%</td>\n",
       "      <td>0.072%</td>\n",
       "      <td>0.178%</td>\n",
       "      <td>0.253%</td>\n",
       "      <td>0.233%</td>\n",
       "      <td>0.274%</td>\n",
       "      <td>0.326%</td>\n",
       "      <td>0.360%</td>\n",
       "      <td>0.395%</td>\n",
       "      <td>0.440%</td>\n",
       "      <td>0.508%</td>\n",
       "      <td>0.513%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swalast</th>\n",
       "      <td>-1.927%</td>\n",
       "      <td>-0.806%</td>\n",
       "      <td>-0.402%</td>\n",
       "      <td>-0.334%</td>\n",
       "      <td>-0.176%</td>\n",
       "      <td>-0.118%</td>\n",
       "      <td>-0.020%</td>\n",
       "      <td>-0.003%</td>\n",
       "      <td>0.101%</td>\n",
       "      <td>0.118%</td>\n",
       "      <td>0.201%</td>\n",
       "      <td>0.208%</td>\n",
       "      <td>0.261%</td>\n",
       "      <td>0.287%</td>\n",
       "      <td>0.348%</td>\n",
       "      <td>0.327%</td>\n",
       "      <td>0.420%</td>\n",
       "      <td>0.481%</td>\n",
       "      <td>0.518%</td>\n",
       "      <td>0.570%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "group          1        2        3        4        5        6        7        8        9       10      11      12      13      14      15      16      17      18      19      20  \n",
       "num type                                                                                                                                                                           \n",
       "0   best     -1.993%  -0.718%  -0.404%  -0.211%  -0.093%  -0.034%  -0.015%   0.058%  0.120%  0.153%  0.189%  0.242%  0.191%  0.258%  0.308%  0.320%  0.407%  0.404%  0.386%  0.495%\n",
       "    swabest  -2.007%  -0.733%  -0.418%  -0.259%  -0.159%  -0.066%   0.002%   0.021%  0.065%  0.119%  0.254%  0.221%  0.255%  0.267%  0.335%  0.334%  0.406%  0.450%  0.483%  0.499%\n",
       "    swalast  -1.981%  -0.743%  -0.419%  -0.264%  -0.227%  -0.061%  -0.044%   0.069%  0.074%  0.077%  0.184%  0.240%  0.264%  0.248%  0.361%  0.346%  0.416%  0.445%  0.508%  0.573%\n",
       "1   best     -1.911%  -0.771%  -0.403%  -0.257%  -0.137%  -0.044%   0.032%   0.029%  0.107%  0.164%  0.144%  0.255%  0.239%  0.245%  0.290%  0.329%  0.391%  0.425%  0.445%  0.496%\n",
       "    swabest  -1.974%  -0.768%  -0.389%  -0.252%  -0.174%  -0.073%   0.037%   0.054%  0.105%  0.094%  0.191%  0.194%  0.245%  0.271%  0.333%  0.356%  0.359%  0.450%  0.470%  0.535%\n",
       "    swalast  -1.986%  -0.797%  -0.446%  -0.232%  -0.166%  -0.123%  -0.029%   0.047%  0.083%  0.174%  0.141%  0.199%  0.247%  0.327%  0.356%  0.386%  0.385%  0.469%  0.481%  0.555%\n",
       "2   best     -1.826%  -0.798%  -0.381%  -0.275%  -0.112%   0.000%  -0.019%   0.064%  0.123%  0.063%  0.185%  0.240%  0.285%  0.258%  0.315%  0.319%  0.367%  0.418%  0.418%  0.416%\n",
       "    swabest  -1.935%  -0.693%  -0.412%  -0.297%  -0.174%  -0.009%   0.044%   0.060%  0.097%  0.134%  0.181%  0.227%  0.251%  0.298%  0.307%  0.338%  0.339%  0.400%  0.454%  0.454%\n",
       "    swalast  -1.911%  -0.730%  -0.416%  -0.319%  -0.137%  -0.026%  -0.022%   0.090%  0.109%  0.174%  0.198%  0.225%  0.218%  0.348%  0.324%  0.329%  0.379%  0.362%  0.440%  0.428%\n",
       "3   best     -1.918%  -0.655%  -0.391%  -0.178%  -0.093%  -0.034%   0.063%   0.026%  0.187%  0.129%  0.163%  0.173%  0.210%  0.218%  0.289%  0.355%  0.380%  0.362%  0.387%  0.398%\n",
       "    swabest  -1.947%  -0.690%  -0.348%  -0.242%  -0.138%  -0.049%   0.049%   0.052%  0.095%  0.142%  0.160%  0.182%  0.207%  0.246%  0.294%  0.342%  0.383%  0.386%  0.487%  0.462%\n",
       "    swalast  -1.917%  -0.721%  -0.352%  -0.228%  -0.134%  -0.057%   0.022%   0.074%  0.041%  0.138%  0.198%  0.184%  0.219%  0.247%  0.326%  0.318%  0.397%  0.426%  0.443%  0.447%\n",
       "4   best     -1.815%  -0.747%  -0.502%  -0.255%  -0.171%  -0.078%  -0.024%   0.135%  0.114%  0.137%  0.179%  0.208%  0.221%  0.314%  0.272%  0.337%  0.388%  0.394%  0.461%  0.480%\n",
       "    swabest  -1.953%  -0.789%  -0.444%  -0.304%  -0.163%  -0.046%   0.008%   0.054%  0.139%  0.072%  0.178%  0.253%  0.233%  0.274%  0.326%  0.360%  0.395%  0.440%  0.508%  0.513%\n",
       "    swalast  -1.927%  -0.806%  -0.402%  -0.334%  -0.176%  -0.118%  -0.020%  -0.003%  0.101%  0.118%  0.201%  0.208%  0.261%  0.287%  0.348%  0.327%  0.420%  0.481%  0.518%  0.570%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped Return Results are saved to /home/mengkjin/Workspace/learndl/models/gru_5year_day/detailed_analysis/group.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n",
      "/home/mengkjin/Workspace/learndl/src/data/core.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs) if len(dfs) else pd.DataFrame()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>factor_name</th>\n",
       "      <th>benchmark</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>pf</th>\n",
       "      <th>bm</th>\n",
       "      <th>excess</th>\n",
       "      <th>annualized</th>\n",
       "      <th>mdd</th>\n",
       "      <th>te</th>\n",
       "      <th>ir</th>\n",
       "      <th>calmar</th>\n",
       "      <th>turnover</th>\n",
       "      <th>mdd_period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>best</td>\n",
       "      <td>csi300</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240726</td>\n",
       "      <td>90.19%</td>\n",
       "      <td>20.59%</td>\n",
       "      <td>69.60%</td>\n",
       "      <td>6.01%</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>4.21%</td>\n",
       "      <td>1.427</td>\n",
       "      <td>0.717</td>\n",
       "      <td>147.506</td>\n",
       "      <td>20190603-20200228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>best</td>\n",
       "      <td>csi500</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240726</td>\n",
       "      <td>96.04%</td>\n",
       "      <td>-17.81%</td>\n",
       "      <td>113.85%</td>\n",
       "      <td>12.00%</td>\n",
       "      <td>10.78%</td>\n",
       "      <td>6.83%</td>\n",
       "      <td>1.756</td>\n",
       "      <td>1.113</td>\n",
       "      <td>147.690</td>\n",
       "      <td>20210429-20210915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best</td>\n",
       "      <td>csi1000</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240726</td>\n",
       "      <td>64.77%</td>\n",
       "      <td>-41.69%</td>\n",
       "      <td>106.46%</td>\n",
       "      <td>14.30%</td>\n",
       "      <td>14.56%</td>\n",
       "      <td>6.59%</td>\n",
       "      <td>2.171</td>\n",
       "      <td>0.982</td>\n",
       "      <td>147.879</td>\n",
       "      <td>20210429-20210915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>swabest</td>\n",
       "      <td>csi300</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240726</td>\n",
       "      <td>87.94%</td>\n",
       "      <td>20.59%</td>\n",
       "      <td>67.35%</td>\n",
       "      <td>5.85%</td>\n",
       "      <td>7.44%</td>\n",
       "      <td>4.04%</td>\n",
       "      <td>1.447</td>\n",
       "      <td>0.786</td>\n",
       "      <td>147.549</td>\n",
       "      <td>20190603-20210113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>swabest</td>\n",
       "      <td>csi500</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240726</td>\n",
       "      <td>104.63%</td>\n",
       "      <td>-17.81%</td>\n",
       "      <td>122.44%</td>\n",
       "      <td>12.64%</td>\n",
       "      <td>9.57%</td>\n",
       "      <td>6.51%</td>\n",
       "      <td>1.941</td>\n",
       "      <td>1.320</td>\n",
       "      <td>147.759</td>\n",
       "      <td>20210415-20210915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>swabest</td>\n",
       "      <td>csi1000</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240726</td>\n",
       "      <td>75.49%</td>\n",
       "      <td>-41.69%</td>\n",
       "      <td>117.18%</td>\n",
       "      <td>15.23%</td>\n",
       "      <td>13.27%</td>\n",
       "      <td>6.39%</td>\n",
       "      <td>2.383</td>\n",
       "      <td>1.147</td>\n",
       "      <td>147.933</td>\n",
       "      <td>20210429-20210915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>swalast</td>\n",
       "      <td>csi300</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240726</td>\n",
       "      <td>90.76%</td>\n",
       "      <td>20.59%</td>\n",
       "      <td>70.17%</td>\n",
       "      <td>6.02%</td>\n",
       "      <td>6.28%</td>\n",
       "      <td>4.16%</td>\n",
       "      <td>1.448</td>\n",
       "      <td>0.958</td>\n",
       "      <td>147.547</td>\n",
       "      <td>20190603-20200228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>swalast</td>\n",
       "      <td>csi500</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240726</td>\n",
       "      <td>91.06%</td>\n",
       "      <td>-17.81%</td>\n",
       "      <td>108.86%</td>\n",
       "      <td>11.61%</td>\n",
       "      <td>10.62%</td>\n",
       "      <td>6.60%</td>\n",
       "      <td>1.759</td>\n",
       "      <td>1.093</td>\n",
       "      <td>147.774</td>\n",
       "      <td>20210429-20210915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>swalast</td>\n",
       "      <td>csi1000</td>\n",
       "      <td>20170104</td>\n",
       "      <td>20240726</td>\n",
       "      <td>77.21%</td>\n",
       "      <td>-41.69%</td>\n",
       "      <td>118.90%</td>\n",
       "      <td>15.40%</td>\n",
       "      <td>12.76%</td>\n",
       "      <td>6.40%</td>\n",
       "      <td>2.407</td>\n",
       "      <td>1.207</td>\n",
       "      <td>147.965</td>\n",
       "      <td>20210429-20210915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  factor_name benchmark    start      end      pf       bm     excess  annualized   mdd     te     ir   calmar turnover     mdd_period    \n",
       "1       best     csi300  20170104  20240726   90.19%   20.59%   69.60%    6.01%     8.39%  4.21%  1.427  0.717  147.506  20190603-20200228\n",
       "2       best     csi500  20170104  20240726   96.04%  -17.81%  113.85%   12.00%    10.78%  6.83%  1.756  1.113  147.690  20210429-20210915\n",
       "0       best    csi1000  20170104  20240726   64.77%  -41.69%  106.46%   14.30%    14.56%  6.59%  2.171  0.982  147.879  20210429-20210915\n",
       "4    swabest     csi300  20170104  20240726   87.94%   20.59%   67.35%    5.85%     7.44%  4.04%  1.447  0.786  147.549  20190603-20210113\n",
       "5    swabest     csi500  20170104  20240726  104.63%  -17.81%  122.44%   12.64%     9.57%  6.51%  1.941  1.320  147.759  20210415-20210915\n",
       "3    swabest    csi1000  20170104  20240726   75.49%  -41.69%  117.18%   15.23%    13.27%  6.39%  2.383  1.147  147.933  20210429-20210915\n",
       "7    swalast     csi300  20170104  20240726   90.76%   20.59%   70.17%    6.02%     6.28%  4.16%  1.448  0.958  147.547  20190603-20200228\n",
       "8    swalast     csi500  20170104  20240726   91.06%  -17.81%  108.86%   11.61%    10.62%  6.60%  1.759  1.093  147.774  20210429-20210915\n",
       "6    swalast    csi1000  20170104  20240726   77.21%  -41.69%  118.90%   15.40%    12.76%  6.40%  2.407  1.207  147.965  20210429-20210915"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-08-26 12:58:22|MOD:time        |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 4 Hours 58 Minutes 50.5 Seconds\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytic datas are saved to /home/mengkjin/Workspace/learndl/models/gru_5year_day/detailed_analysis/perf.xlsx\n",
      "Analytic plots are saved to /home/mengkjin/Workspace/learndl/models/gru_5year_day/detailed_analysis/plot.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<src.model.trainer.trainers.net.NetTrainer at 0x767d1953dd50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.api import Trainer  \n",
    "app = Trainer.initialize(stage = 0 , resume = 0 , checkname= 1)\n",
    "app.go()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device name: NVIDIA GeForce RTX 4090\n",
      "try using /home/mengkjin/Workspace/learndl/data/DataSet/30m.20240703.pt , success!\n",
      "Load  2 DataBlocks...... finished! Cost 1.08 secs\n",
      "Align 2 DataBlocks...... finished! Cost 0.76 secs\n",
      "Pre-Norming method of [30m] : {'divlast': False, 'histnorm': True}\n"
     ]
    }
   ],
   "source": [
    "from src.api import HiddenExtractor\n",
    "extractor = HiddenExtractor('resnet_gru_30m' , model_nums=[1] , model_types=['best'])\n",
    "extractor.extract_hidden('update' , deploy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataAPI , Trainer\n\u001b[1;32m----> 2\u001b[0m \u001b[43mDataAPI\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruct_train_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m Trainer\u001b[38;5;241m.\u001b[39mupdate_models()\n",
      "File \u001b[1;32md:\\Coding\\learndl\\learndl\\src\\model\\api\\data.py:38\u001b[0m, in \u001b[0;36mDataAPI.reconstruct_train_data\u001b[1;34m(data_types)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# assert THIS_IS_SERVER\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_types , \u001b[38;5;28mstr\u001b[39m): data_types \u001b[38;5;241m=\u001b[39m data_types\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m \u001b[43mNetDataModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n",
      "File \u001b[1;32md:\\Coding\\learndl\\learndl\\src\\model\\trainer\\trainers\\basic.py:70\u001b[0m, in \u001b[0;36mDataModule.prepare_data\u001b[1;34m(data_types)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_data\u001b[39m(data_types : Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 70\u001b[0m     \u001b[43mDataProcessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_types\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     DataProcessor\u001b[38;5;241m.\u001b[39mmain(predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m , data_types \u001b[38;5;241m=\u001b[39m data_types)\n",
      "File \u001b[1;32md:\\Coding\\learndl\\learndl\\src\\data\\process.py:50\u001b[0m, in \u001b[0;36mDataProcessor.main\u001b[1;34m(cls, predict, confirm, parser, data_types)\u001b[0m\n\u001b[0;32m     47\u001b[0m     args , _ \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_known_args()\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mconfirm \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mConfirm update data? print \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m to confirm!\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     52\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredict\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m , Data Processing start!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "from src.api import DataAPI , Trainer\n",
    "DataAPI.reconstruct_train_data()\n",
    "Trainer.update_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device name: NVIDIA GeForce RTX 4090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:10:43|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:10:43|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Mon Jun 24 01:10:43 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Process Queue : Data + Fit + Test\n",
      "--Confirm Resume Training!\n",
      "--Model_name is set to gru_day!\n",
      "Callback : ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True) , reset optimizer on some epoch (can speedup scheduler)\n",
      "Callback : CallbackTimer(verbosity=2) , record time cost of callback hooks\n",
      "Callback : EarlyStoppage(patience=20) , stop fitting when validation score cease to improve\n",
      "Callback : ValidationConverge(patience=5,eps=1e-05) , stop fitting when valid_score converge\n",
      "Callback : EarlyExitRetrain(earliest=5,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100]) , retrain with new lr if fitting stopped too early\n",
      "Callback : NanLossRetrain(max_attempt=4) , retrain if fitting encounters nan loss\n",
      "Callback : BatchDisplay(verbosity=2) , display batch progress bar\n",
      "Callback : StatusDisplay(verbosity=2) , display epoch and event information\n",
      "{'random_seed': None,\n",
      " 'model_name': 'gruRES_day',\n",
      " 'model_module': 'gru',\n",
      " 'model_data_type': 'day',\n",
      " 'model_types': ['best', 'swalast', 'swabest'],\n",
      " 'labels': ['std_lag1_10'],\n",
      " 'beg_date': 20170103,\n",
      " 'end_date': 99991231,\n",
      " 'sample_method': 'train_shuffle',\n",
      " 'shuffle_option': 'epoch',\n",
      " 'lgbm_ensembler': False}\n",
      "{'hidden_dim': [32, 64],\n",
      " 'seqlens': [{'day': 30, '30m': 30, 'dms': 30}],\n",
      " 'tra_seqlens': [{'hist_loss': 40}],\n",
      " 'dropout': [0.1],\n",
      " 'enc_in': [True],\n",
      " 'enc_att': [False],\n",
      " 'rnn_type': ['lstm'],\n",
      " 'rnn_att': [False],\n",
      " 'rnn_layers': [2],\n",
      " 'dec_mlp_layers': [2],\n",
      " 'num_output': [1],\n",
      " 'kernel_size': [3],\n",
      " 'hidden_as_factor': [False],\n",
      " 'ordered_param_group': [False],\n",
      " 'tra_num_states': [5]}\n",
      "Load  2 DataBlocks...... finished! Cost 2.40 secs\n",
      "Align 2 DataBlocks...... finished! Cost 2.86 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:10:51|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 7.9 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:10:51|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Mon Jun 24 01:10:51 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n",
      "score function of [spearman] calculated and success!\n",
      "loss function of [pearson] calculated and success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mFirstBite Ep#  0 : loss  0.99770, train 0.00391, valid 0.03668, best 0.0367, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.87023, train 0.12798, valid 0.09558, best 0.0956, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.85418, train 0.14255, valid 0.08890, best 0.0956, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.84138, train 0.15293, valid 0.08662, best 0.0956, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.83995, train 0.15505, valid 0.09025, best 0.0956, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.83040, train 0.16270, valid 0.08154, best 0.0956, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.83040, train 0.16270, valid 0.08154, best 0.0956, lr3.1e-04, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  0 : loss  0.99907, train 0.00307, valid-0.00235, best-0.0024, lr1.3e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  5 : loss  0.89798, train 0.10238, valid 0.07134, best 0.0839, lr2.5e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 10 : loss  0.88484, train 0.11407, valid 0.08712, best 0.0892, lr1.9e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 15 : loss  0.87704, train 0.12033, valid 0.08682, best 0.0892, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 20 : loss  0.87598, train 0.12115, valid 0.08665, best 0.0892, lr9.4e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 25 : loss  0.87235, train 0.12489, valid 0.08559, best 0.0892, lr3.1e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 25 : loss  0.87235, train 0.12489, valid 0.08559, best 0.0892, lr3.1e-05, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  0 : loss  1.00802, train-0.01361, valid-0.04341, best-0.0434, lr1.3e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  5 : loss  0.90600, train 0.09749, valid 0.08519, best 0.0985, lr2.5e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 10 : loss  0.88704, train 0.11519, valid 0.10017, best 0.1002, lr1.9e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 15 : loss  0.88873, train 0.11359, valid 0.09794, best 0.1002, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 20 : loss  0.89135, train 0.11055, valid 0.09254, best 0.1002, lr9.4e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 25 : loss  0.88877, train 0.11064, valid 0.09957, best 0.1021, lr3.1e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 30 : loss  0.88839, train 0.10964, valid 0.08501, best 0.1021, lr1.6e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 35 : loss  0.88399, train 0.11418, valid 0.08975, best 0.1021, lr3.1e-03\u001b[0m\n",
      "\u001b[32mReset learn rate and scheduler at the end of epoch 39 , effective at epoch 40\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 40 : loss  0.88123, train 0.11494, valid 0.09168, best 0.1021, lr1.3e-02\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:16:36|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #1 @20231201|Retrain#2 Ep# 96 EarlyStop|Train 0.1037 Valid 0.0934 BestVal 0.0956|Cost  5.7Min,  3.5Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  1.00213, train-0.00346, valid-0.02870, best-0.0287, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.88101, train 0.11998, valid 0.10566, best 0.1118, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.86625, train 0.13116, valid 0.09471, best 0.1118, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.85764, train 0.13868, valid 0.09772, best 0.1118, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.85723, train 0.13898, valid 0.10404, best 0.1118, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 24 : loss  0.85347, train 0.14217, valid 0.09795, best 0.1118, lr1.6e-04, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  0 : loss  0.99619, train 0.00640, valid 0.02634, best 0.0263, lr1.3e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  5 : loss  0.91904, train 0.07677, valid 0.07904, best 0.0790, lr2.5e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 10 : loss  0.90607, train 0.09211, valid 0.09041, best 0.0904, lr1.9e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 15 : loss  0.89860, train 0.10379, valid 0.09553, best 0.0956, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 20 : loss  0.89763, train 0.10523, valid 0.09337, best 0.0956, lr9.4e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 24 : loss  0.89522, train 0.10913, valid 0.09689, best 0.0970, lr1.6e-05, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  0 : loss  1.00127, train-0.00532, valid-0.02776, best-0.0278, lr1.3e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  5 : loss  0.90841, train 0.09496, valid 0.09364, best 0.1105, lr2.5e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 10 : loss  0.88931, train 0.11243, valid 0.09934, best 0.1105, lr1.9e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 15 : loss  0.88411, train 0.11520, valid 0.09632, best 0.1105, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 20 : loss  0.88303, train 0.11682, valid 0.10441, best 0.1105, lr9.4e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 24 : loss  0.87640, train 0.12191, valid 0.10175, best 0.1105, lr1.6e-03, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  0 : loss  0.99942, train 0.00330, valid 0.03781, best 0.0378, lr1.3e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  5 : loss  0.96270, train 0.04376, valid 0.07316, best 0.0732, lr2.5e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 10 : loss  0.95829, train 0.04798, valid 0.07296, best 0.0736, lr1.9e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 15 : loss  0.95190, train 0.05224, valid 0.07273, best 0.0736, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 20 : loss  0.95024, train 0.05357, valid 0.07160, best 0.0736, lr9.5e-06\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 24 : loss  0.94829, train 0.05503, valid 0.07283, best 0.0736, lr1.7e-06, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  0 : loss  0.99623, train 0.00749, valid 0.03126, best 0.0313, lr1.3e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  5 : loss  0.97201, train 0.03669, valid 0.04230, best 0.0540, lr2.5e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 10 : loss  0.97588, train 0.03329, valid 0.03690, best 0.0540, lr1.9e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 15 : loss  0.97586, train 0.03358, valid 0.04005, best 0.0540, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 20 : loss  0.97032, train 0.04237, valid 0.04644, best 0.0540, lr9.4e-02\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:22:42|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #0 @20240604|Retrain#4 Ep#125 EarlyStop|Train 0.0369 Valid 0.0350 BestVal 0.1118|Cost  6.0Min,  2.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:24:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #1 @20240604|FirstBite Ep# 34 EarlyStop|Train 0.1667 Valid 0.0877 BestVal 0.0877|Cost  2.1Min,  3.6Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:24:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Fit], Cost 0.2 Hours, 4.7 Min/model, 3.3 Sec/Epoch\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:24:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Mon Jun 24 01:24:48 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:24:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mEach Model Date Testing Mean Score(spearman):\u001b[0m\n",
      "\u001b[32mModels            0       0       0       1       1       1\u001b[0m\n",
      "\u001b[32mOutput         best swalast swabest    best swalast swabest\u001b[0m\n",
      "\u001b[32m20170103     0.1536  0.1525  0.1520  0.1525  0.1521  0.1520\u001b[0m\n",
      "\u001b[32m20170704     0.1373  0.1327  0.1365  0.1413  0.1373  0.1405\u001b[0m\n",
      "\u001b[32m20171226     0.1305  0.1323  0.1329  0.1291  0.1298  0.1300\u001b[0m\n",
      "\u001b[32m20180627     0.1225  0.1186  0.1223  0.1226  0.1210  0.1236\u001b[0m\n",
      "\u001b[32m20181220     0.0998  0.0990  0.1004  0.0958  0.0981  0.0987\u001b[0m\n",
      "\u001b[32m20190624     0.0970  0.0955  0.0972  0.0920  0.0908  0.0924\u001b[0m\n",
      "\u001b[32m20191217     0.1011  0.0991  0.1005  0.1042  0.1048  0.1047\u001b[0m\n",
      "\u001b[32m20200617     0.0970  0.0963  0.0978  0.0947  0.0927  0.0946\u001b[0m\n",
      "\u001b[32m20201214     0.0837  0.0792  0.0842  0.0759  0.0766  0.0739\u001b[0m\n",
      "\u001b[32m20210615     0.0605  0.0572  0.0608  0.0689  0.0666  0.0662\u001b[0m\n",
      "\u001b[32m20211209     0.0935  0.0954  0.0962  0.1084  0.1106  0.1084\u001b[0m\n",
      "\u001b[32m20220613     0.0931  0.0909  0.0813  0.0857  0.0829  0.0876\u001b[0m\n",
      "\u001b[32m20221206     0.0650  0.0616  0.0635  0.0580  0.0576  0.0497\u001b[0m\n",
      "\u001b[32m20230606     0.0906  0.0857  0.0872  0.0868  0.0885  0.0856\u001b[0m\n",
      "\u001b[32m20231201     0.1217  0.1221  0.1235  0.1082  0.1025  0.1008\u001b[0m\n",
      "\u001b[32m20240604    -0.0130  0.0400 -0.0266  0.0401  0.0503  0.0572\u001b[0m\n",
      "\u001b[32mAllTimeAvg   0.1024  0.1008  0.1016  0.1012  0.1005  0.1003\u001b[0m\n",
      "\u001b[32mAllTimeSum   185.48  182.66  184.04  183.36  182.05  181.72\u001b[0m\n",
      "\u001b[32mStd          0.0668  0.0665  0.0669  0.0666  0.0656  0.0665\u001b[0m\n",
      "\u001b[32mTValue        65.23   64.56   64.64   64.69   65.24   64.18\u001b[0m\n",
      "\u001b[32mAnnIR        7.5077  7.4304  7.4389  7.4445  7.5079  7.3860\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:25:46|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 57.6 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:25:46|MOD:time        |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 15 Minutes 3.3 Seconds\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:25:46|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:25:46|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Mon Jun 24 01:25:46 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Process Queue : Data + Fit + Test\n",
      "--Confirm Resume Training!\n",
      "--Model_name is set to gruRTN_day!\n",
      "Callback : ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True) , reset optimizer on some epoch (can speedup scheduler)\n",
      "Callback : CallbackTimer(verbosity=2) , record time cost of callback hooks\n",
      "Callback : EarlyStoppage(patience=20) , stop fitting when validation score cease to improve\n",
      "Callback : ValidationConverge(patience=5,eps=1e-05) , stop fitting when valid_score converge\n",
      "Callback : EarlyExitRetrain(earliest=5,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100]) , retrain with new lr if fitting stopped too early\n",
      "Callback : NanLossRetrain(max_attempt=4) , retrain if fitting encounters nan loss\n",
      "Callback : BatchDisplay(verbosity=2) , display batch progress bar\n",
      "Callback : StatusDisplay(verbosity=2) , display epoch and event information\n",
      "{'random_seed': None,\n",
      " 'model_name': 'gruRTN_day',\n",
      " 'model_module': 'gru',\n",
      " 'model_data_type': 'day',\n",
      " 'model_types': ['best', 'swalast', 'swabest'],\n",
      " 'labels': ['rtn_lag1_10'],\n",
      " 'beg_date': 20170103,\n",
      " 'end_date': 99991231,\n",
      " 'sample_method': 'train_shuffle',\n",
      " 'shuffle_option': 'epoch',\n",
      " 'lgbm_ensembler': False}\n",
      "{'hidden_dim': [32, 64],\n",
      " 'seqlens': [{'day': 30, '30m': 30, 'dms': 30}],\n",
      " 'tra_seqlens': [{'hist_loss': 40}],\n",
      " 'dropout': [0.1],\n",
      " 'enc_in': [True],\n",
      " 'enc_att': [False],\n",
      " 'rnn_type': ['lstm'],\n",
      " 'rnn_att': [False],\n",
      " 'rnn_layers': [2],\n",
      " 'dec_mlp_layers': [2],\n",
      " 'num_output': [1],\n",
      " 'kernel_size': [3],\n",
      " 'hidden_as_factor': [False],\n",
      " 'ordered_param_group': [False],\n",
      " 'tra_num_states': [5]}\n",
      "try using /home/mengkjin/Workspace/learndl/data/DataSet/day.20240605.pt , success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:25:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 2.1 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:25:48|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Mon Jun 24 01:25:48 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mFirstBite Ep#  0 : loss  1.00382, train-0.00516, valid-0.03649, best-0.0365, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.86311, train 0.13764, valid 0.09228, best 0.0923, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.84547, train 0.15544, valid 0.09630, best 0.0963, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.83038, train 0.17066, valid 0.08826, best 0.0963, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.82732, train 0.17363, valid 0.08575, best 0.0963, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 25 : loss  0.81729, train 0.18370, valid 0.08407, best 0.0963, lr3.1e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 30 : loss  0.81421, train 0.18647, valid 0.08262, best 0.0963, lr1.6e-04\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:27:46|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRTN_day #1 @20231201|FirstBite Ep# 31 EarlyStop|Train 0.1865 Valid 0.0826 BestVal 0.0826|Cost  1.9Min,  3.6Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  1.00293, train-0.00341, valid-0.00344, best-0.0034, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.87709, train 0.12551, valid 0.08796, best 0.1129, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.86180, train 0.13731, valid 0.10436, best 0.1129, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.85287, train 0.14587, valid 0.10502, best 0.1129, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.85180, train 0.14778, valid 0.09660, best 0.1129, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 23 : loss  0.84699, train 0.15223, valid 0.10093, best 0.1129, lr1.0e-07, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  0 : loss  0.99854, train 0.00457, valid 0.01988, best 0.0199, lr1.3e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  5 : loss  0.92030, train 0.08103, valid 0.08255, best 0.0825, lr2.5e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 10 : loss  0.90702, train 0.09389, valid 0.08568, best 0.0857, lr1.9e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 15 : loss  0.89605, train 0.10568, valid 0.10030, best 0.1003, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 20 : loss  0.89414, train 0.10746, valid 0.09676, best 0.1003, lr9.4e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 23 : loss  0.89076, train 0.11050, valid 0.09963, best 0.1003, lr1.0e-07, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  0 : loss  0.99821, train 0.00317, valid 0.04276, best 0.0428, lr1.3e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  5 : loss  0.91434, train 0.07999, valid 0.07219, best 0.1080, lr2.5e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 10 : loss  0.89327, train 0.10612, valid 0.08880, best 0.1080, lr1.9e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 15 : loss  0.88616, train 0.11392, valid 0.08959, best 0.1080, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 20 : loss  0.88396, train 0.11643, valid 0.09105, best 0.1080, lr9.4e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 23 : loss  0.87573, train 0.12499, valid 0.08870, best 0.1080, lr1.0e-07, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  0 : loss  1.00152, train 0.00047, valid 0.02760, best 0.0276, lr1.3e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  5 : loss  0.96801, train 0.04157, valid 0.06108, best 0.0611, lr2.5e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 10 : loss  0.95807, train 0.04909, valid 0.05827, best 0.0611, lr1.9e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 15 : loss  0.95065, train 0.05443, valid 0.05588, best 0.0611, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 20 : loss  0.94877, train 0.05559, valid 0.05718, best 0.0611, lr9.5e-06\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 23 : loss  0.94672, train 0.05714, valid 0.05652, best 0.0611, lr1.0e-07, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  0 : loss  1.00047, train-0.00169, valid-0.00940, best-0.0094, lr1.3e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  5 : loss  0.97745, train 0.03642, valid 0.05397, best 0.0540, lr2.5e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 10 : loss  0.96087, train 0.04016, valid 0.04625, best 0.0540, lr1.9e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 15 : loss  0.97887, train 0.02974, valid 0.04515, best 0.0592, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 20 : loss  0.97797, train 0.03105, valid 0.02864, best 0.0592, lr9.4e-02\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:33:42|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRTN_day #0 @20240604|Retrain#4 Ep#120 EarlyStop|Train 0.0324 Valid 0.0466 BestVal 0.1129|Cost  5.9Min,  2.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:35:37|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRTN_day #1 @20240604|FirstBite Ep# 31 EarlyStop|Train 0.1916 Valid 0.0938 BestVal 0.0938|Cost  1.9Min,  3.6Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:35:37|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Fit], Cost 0.2 Hours, 3.3 Min/model, 3.2 Sec/Epoch\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:35:37|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Mon Jun 24 01:35:37 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:35:37|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mEach Model Date Testing Mean Score(spearman):\u001b[0m\n",
      "\u001b[32mModels            0       0       0       1       1       1\u001b[0m\n",
      "\u001b[32mOutput         best swalast swabest    best swalast swabest\u001b[0m\n",
      "\u001b[32m20170103     0.1415  0.1404  0.1427  0.1449  0.1435  0.1447\u001b[0m\n",
      "\u001b[32m20170704     0.1344  0.1262  0.1345  0.1305  0.1318  0.1309\u001b[0m\n",
      "\u001b[32m20171226     0.1443  0.1441  0.1454  0.1443  0.1440  0.1482\u001b[0m\n",
      "\u001b[32m20180627     0.1204  0.1149  0.1163  0.1097  0.1025  0.1077\u001b[0m\n",
      "\u001b[32m20181220     0.1134  0.1100  0.1116  0.1024  0.1004  0.1035\u001b[0m\n",
      "\u001b[32m20190624     0.0928  0.0923  0.0936  0.0977  0.0973  0.0969\u001b[0m\n",
      "\u001b[32m20191217     0.1172  0.1181  0.1180  0.1052  0.1068  0.1165\u001b[0m\n",
      "\u001b[32m20200617     0.0920  0.0939  0.0939  0.0902  0.0826  0.0929\u001b[0m\n",
      "\u001b[32m20201214     0.0986  0.0984  0.0981  0.0939  0.0944  0.0957\u001b[0m\n",
      "\u001b[32m20210615     0.0696  0.0720  0.0718  0.0754  0.0735  0.0734\u001b[0m\n",
      "\u001b[32m20211209     0.1126  0.1152  0.1148  0.1102  0.1128  0.1136\u001b[0m\n",
      "\u001b[32m20220613     0.1110  0.1046  0.1063  0.1074  0.1058  0.1127\u001b[0m\n",
      "\u001b[32m20221206     0.0574  0.0559  0.0579  0.0602  0.0553  0.0578\u001b[0m\n",
      "\u001b[32m20230606     0.0824  0.0776  0.0835  0.0856  0.0771  0.0824\u001b[0m\n",
      "\u001b[32m20231201     0.1327  0.1357  0.1289  0.1284  0.1280  0.1267\u001b[0m\n",
      "\u001b[32m20240604    -0.0291  0.0381  0.0338  0.0062  0.0266  0.0194\u001b[0m\n",
      "\u001b[32mAllTimeAvg   0.1071  0.1062  0.1073  0.1051  0.1032  0.1063\u001b[0m\n",
      "\u001b[32mAllTimeSum   194.08  192.38  194.48  190.38  187.02  192.67\u001b[0m\n",
      "\u001b[32mStd          0.0877  0.0881  0.0874  0.0877  0.0868  0.0861\u001b[0m\n",
      "\u001b[32mTValue        51.97   51.28   52.26   51.01   50.60   52.58\u001b[0m\n",
      "\u001b[32mAnnIR        5.9810  5.9022  6.0149  5.8711  5.8230  6.0509\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:36:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 54.9 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:36:32|MOD:time        |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 10 Minutes 45.7 Seconds\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:36:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mModel Specifics:\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:36:32|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Data] at Mon Jun 24 01:36:32 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Process Queue : Data + Fit + Test\n",
      "--Confirm Resume Training!\n",
      "--Model_name is set to gruRES_day!\n",
      "Callback : ResetOptimizer(num_reset=2,trigger=40,recover_level=1.0,speedup2x=True) , reset optimizer on some epoch (can speedup scheduler)\n",
      "Callback : CallbackTimer(verbosity=2) , record time cost of callback hooks\n",
      "Callback : EarlyStoppage(patience=20) , stop fitting when validation score cease to improve\n",
      "Callback : ValidationConverge(patience=5,eps=1e-05) , stop fitting when valid_score converge\n",
      "Callback : EarlyExitRetrain(earliest=5,max_attempt=4,lr_multiplier=[1, 0.1, 10, 0.01, 100]) , retrain with new lr if fitting stopped too early\n",
      "Callback : NanLossRetrain(max_attempt=4) , retrain if fitting encounters nan loss\n",
      "Callback : BatchDisplay(verbosity=2) , display batch progress bar\n",
      "Callback : StatusDisplay(verbosity=2) , display epoch and event information\n",
      "{'random_seed': None,\n",
      " 'model_name': 'gruRES_day',\n",
      " 'model_module': 'gru',\n",
      " 'model_data_type': 'day',\n",
      " 'model_types': ['best', 'swalast', 'swabest'],\n",
      " 'labels': ['res_lag1_10'],\n",
      " 'beg_date': 20170103,\n",
      " 'end_date': 99991231,\n",
      " 'sample_method': 'train_shuffle',\n",
      " 'shuffle_option': 'epoch',\n",
      " 'lgbm_ensembler': False}\n",
      "{'hidden_dim': [32, 64],\n",
      " 'seqlens': [{'day': 30, '30m': 30, 'dms': 30}],\n",
      " 'tra_seqlens': [{'hist_loss': 40}],\n",
      " 'dropout': [0.1],\n",
      " 'enc_in': [True],\n",
      " 'enc_att': [False],\n",
      " 'rnn_type': ['lstm'],\n",
      " 'rnn_att': [False],\n",
      " 'rnn_layers': [2],\n",
      " 'dec_mlp_layers': [2],\n",
      " 'num_output': [1],\n",
      " 'kernel_size': [3],\n",
      " 'hidden_as_factor': [False],\n",
      " 'ordered_param_group': [False],\n",
      " 'tra_num_states': [5]}\n",
      "try using /home/mengkjin/Workspace/learndl/data/DataSet/day.20240605.pt , success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:36:34|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Data], Cost 2.0 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:36:34|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Fit] at Mon Jun 24 01:36:34 2024!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mFirstBite Ep#  0 : loss  1.01252, train-0.01521, valid-0.03742, best-0.0374, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.91026, train 0.08135, valid 0.03185, best 0.0599, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.89291, train 0.09436, valid 0.04522, best 0.0599, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.87755, train 0.10648, valid 0.03959, best 0.0599, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.87426, train 0.10831, valid 0.03543, best 0.0599, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 21 : loss  0.87082, train 0.11088, valid 0.03980, best 0.0599, lr6.3e-04, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  0 : loss  0.99637, train 0.00462, valid 0.03106, best 0.0311, lr1.3e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  5 : loss  0.94196, train 0.05698, valid 0.03578, best 0.0560, lr2.5e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 10 : loss  0.92553, train 0.06750, valid 0.03080, best 0.0560, lr1.9e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 15 : loss  0.91653, train 0.07499, valid 0.03537, best 0.0560, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 20 : loss  0.91509, train 0.07561, valid 0.04185, best 0.0560, lr9.4e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 21 : loss  0.91403, train 0.07721, valid 0.04641, best 0.0560, lr6.3e-05, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  0 : loss  1.00549, train-0.00683, valid-0.02416, best-0.0242, lr1.3e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  5 : loss  0.94359, train 0.04867, valid 0.02798, best 0.0514, lr2.5e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 10 : loss  0.93368, train 0.05600, valid 0.02787, best 0.0514, lr1.9e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 15 : loss  0.92716, train 0.06278, valid 0.03398, best 0.0514, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 20 : loss  0.92646, train 0.06384, valid 0.04303, best 0.0514, lr9.4e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 21 : loss  0.92585, train 0.06408, valid 0.03492, best 0.0514, lr6.3e-03, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  0 : loss  1.01260, train-0.01493, valid-0.03974, best-0.0397, lr1.3e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  5 : loss  0.96876, train 0.02948, valid 0.04494, best 0.0477, lr2.5e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 10 : loss  0.96445, train 0.03233, valid 0.04554, best 0.0477, lr1.9e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 15 : loss  0.96098, train 0.03497, valid 0.04404, best 0.0477, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 20 : loss  0.96022, train 0.03574, valid 0.04354, best 0.0477, lr9.5e-06\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 21 : loss  0.95992, train 0.03628, valid 0.04487, best 0.0477, lr6.3e-06, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  0 : loss  0.99746, train 0.00225, valid-0.00306, best-0.0031, lr1.3e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  5 : loss  0.97511, train 0.02839, valid 0.01301, best 0.0537, lr2.5e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 10 : loss  0.96875, train 0.03070, valid 0.01638, best 0.0537, lr1.9e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 15 : loss  0.96160, train 0.03236, valid 0.03529, best 0.0537, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 20 : loss  0.96655, train 0.02830, valid 0.04237, best 0.0537, lr9.4e-02\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:43:22|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #1 @20231201|Retrain#4 Ep#110 EarlyStop|Train 0.0293 Valid 0.0368 BestVal 0.0599|Cost  6.7Min,  3.6Sec/Ep\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  0.99752, train 0.00292, valid 0.02356, best 0.0236, lr1.3e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.92738, train 0.06719, valid 0.04180, best 0.0533, lr2.5e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 10 : loss  0.90904, train 0.07964, valid 0.02435, best 0.0533, lr1.9e-03\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 15 : loss  0.89805, train 0.08672, valid 0.03391, best 0.0533, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 20 : loss  0.89733, train 0.08698, valid 0.03455, best 0.0533, lr9.4e-04\u001b[0m\n",
      "\u001b[32mFirstBite Ep# 21 : loss  0.89541, train 0.08806, valid 0.04139, best 0.0533, lr6.3e-04, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  0 : loss  1.00733, train-0.00926, valid-0.03680, best-0.0368, lr1.3e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep#  5 : loss  0.95510, train 0.04285, valid 0.05432, best 0.0549, lr2.5e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 10 : loss  0.94688, train 0.04934, valid 0.04339, best 0.0549, lr1.9e-04\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 15 : loss  0.93907, train 0.05436, valid 0.03791, best 0.0549, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 20 : loss  0.93641, train 0.05594, valid 0.03422, best 0.0549, lr9.4e-05\u001b[0m\n",
      "\u001b[32mRetrain#1 Ep# 22 : loss  0.93590, train 0.05702, valid 0.04172, best 0.0549, lr3.1e-05, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  0 : loss  0.99719, train 0.00375, valid 0.01564, best 0.0156, lr1.3e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep#  5 : loss  0.94177, train 0.04812, valid 0.03631, best 0.0465, lr2.5e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 10 : loss  0.93093, train 0.05774, valid 0.03864, best 0.0465, lr1.9e-02\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 15 : loss  0.92785, train 0.05940, valid 0.02965, best 0.0465, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 20 : loss  0.92459, train 0.06304, valid 0.03018, best 0.0465, lr9.4e-03\u001b[0m\n",
      "\u001b[32mRetrain#2 Ep# 22 : loss  0.92154, train 0.06476, valid 0.03223, best 0.0465, lr3.1e-03, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  0 : loss  0.99855, train 0.00161, valid 0.01776, best 0.0178, lr1.3e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep#  5 : loss  0.97299, train 0.02829, valid 0.05143, best 0.0514, lr2.5e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 10 : loss  0.96918, train 0.02884, valid 0.05178, best 0.0518, lr1.9e-05\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 15 : loss  0.96640, train 0.02942, valid 0.05233, best 0.0523, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 20 : loss  0.96524, train 0.02948, valid 0.05201, best 0.0523, lr9.5e-06\u001b[0m\n",
      "\u001b[32mRetrain#3 Ep# 22 : loss  0.96442, train 0.03008, valid 0.05196, best 0.0523, lr3.2e-06, Next attempt goes!\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  0 : loss  1.00164, train-0.00223, valid-0.02355, best-0.0236, lr1.3e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep#  5 : loss  0.97856, train 0.02428, valid 0.02481, best 0.0474, lr2.5e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 10 : loss  0.97707, train 0.02539, valid 0.04388, best 0.0474, lr1.9e-01\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 15 : loss  0.97920, train 0.02491, valid 0.01606, best 0.0474, lr1.0e-07\u001b[0m\n",
      "\u001b[32mRetrain#4 Ep# 20 : loss  0.97573, train 0.02385, valid 0.01877, best 0.0474, lr9.4e-02\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:48:54|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #0 @20240604|Retrain#4 Ep#114 EarlyStop|Train 0.0265 Valid 0.0202 BestVal 0.0549|Cost  5.5Min,  2.9Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:56:07|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mgruRES_day #1 @20240604|Retrain#4 Ep#115 EarlyStop|Train 0.0274 Valid 0.0224 BestVal 0.0568|Cost  7.2Min,  3.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:56:07|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Fit], Cost 0.3 Hours, 6.5 Min/model, 3.5 Sec/Epoch\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:56:07|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mStart Process [Test] at Mon Jun 24 01:56:07 2024!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-06-24 01:56:07|MOD:display     |\u001b[0m: \u001b[1m\u001b[34mEach Model Date Testing Mean Score(spearman):\u001b[0m\n",
      "\u001b[32mModels            0       0       0       1       1       1\u001b[0m\n",
      "\u001b[32mOutput         best swalast swabest    best swalast swabest\u001b[0m\n",
      "\u001b[32m20170103     0.1107  0.1119  0.1114  0.1060  0.1068  0.1081\u001b[0m\n",
      "\u001b[32m20170704     0.0937  0.0946  0.0972  0.0970  0.0989  0.0993\u001b[0m\n",
      "\u001b[32m20171226     0.0889  0.0892  0.0893  0.0901  0.0905  0.0905\u001b[0m\n",
      "\u001b[32m20180627     0.0713  0.0712  0.0713  0.0738  0.0715  0.0736\u001b[0m\n",
      "\u001b[32m20181220     0.0733  0.0742  0.0746  0.0740  0.0741  0.0728\u001b[0m\n",
      "\u001b[32m20190624     0.0702  0.0701  0.0703  0.0663  0.0672  0.0681\u001b[0m\n",
      "\u001b[32m20191217     0.0774  0.0745  0.0763  0.0789  0.0759  0.0750\u001b[0m\n",
      "\u001b[32m20200617     0.0555  0.0563  0.0570  0.0597  0.0579  0.0592\u001b[0m\n",
      "\u001b[32m20201214     0.0610  0.0605  0.0609  0.0619  0.0606  0.0642\u001b[0m\n",
      "\u001b[32m20210615     0.0406  0.0406  0.0438  0.0459  0.0475  0.0477\u001b[0m\n",
      "\u001b[32m20211209     0.0568  0.0572  0.0567  0.0561  0.0556  0.0573\u001b[0m\n",
      "\u001b[32m20220613     0.0457  0.0450  0.0465  0.0447  0.0461  0.0462\u001b[0m\n",
      "\u001b[32m20221206     0.0241  0.0235  0.0277  0.0287  0.0243  0.0296\u001b[0m\n",
      "\u001b[32m20230606     0.0589  0.0451  0.0425  0.0361  0.0324  0.0353\u001b[0m\n",
      "\u001b[32m20231201     0.0291  0.0323  0.0298  0.0140  0.0078  0.0160\u001b[0m\n",
      "\u001b[32m20240604    -0.0728 -0.0680 -0.0678 -0.0540  0.0016 -0.0326\u001b[0m\n",
      "\u001b[32mAllTimeAvg   0.0629  0.0622  0.0628  0.0614  0.0608  0.0622\u001b[0m\n",
      "\u001b[32mAllTimeSum   113.98  112.74  113.82  111.32  110.09  112.77\u001b[0m\n",
      "\u001b[32mStd          0.0537  0.0507  0.0500  0.0504  0.0493  0.0490\u001b[0m\n",
      "\u001b[32mTValue        49.87   52.28   53.42   51.92   52.51   54.01\u001b[0m\n",
      "\u001b[32mAnnIR        5.7394  6.0170  6.1485  5.9749  6.0436  6.2161\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:57:02|MOD:display     |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Test], Cost 55.3 Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-06-24 01:57:02|MOD:time        |\u001b[0m: \u001b[1m\u001b[31mMain Process Finished! Cost 20 Minutes 30.0 Seconds\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from src.api import Trainer\n",
    "Trainer.update_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
