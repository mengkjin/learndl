{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8421827",
   "metadata": {},
   "source": [
    "my goal is to avoid scenarios with highest probability of bad outcomes. my input is historical data’s of x(say 10 time series) and y(1 time series). my idea is to use a set of rolling windows to calculate features of 10x + 1y, and another fix window to calculate future y label, and find feature patterns that almost certainly lead to worst labels. Any methodology that fits my purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9be3192",
   "metadata": {},
   "source": [
    "Your goal is to identify **interpretable feature patterns** that precede the worst future outcomes with very high confidence (i.e., almost certainly). This is a classic **early warning** or **anomaly precursor** mining problem. Below is a systematic methodology that fits your idea, combining time series feature engineering with rule/pattern discovery.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Define “Worst Outcome” (Label Creation)\n",
    "For each time point \\( t \\), you need a **future label** based on a **fixed forecast horizon** \\( H \\) (e.g., next 1 hour, next 10 days).  \n",
    "- Compute an **aggregate** of \\( y \\) over \\([t+1, t+H]\\): mean, sum, min, max, or even a binary event (e.g., threshold exceedance).  \n",
    "- Then define the “worst” class: e.g., **lowest 10%** of the aggregated values, or values below a critical physical threshold.  \n",
    "- You now have a **binary target** \\( Y_t^{bad} \\) for each \\( t \\) where the forecast horizon fits inside the data.\n",
    "\n",
    "> **Crucial**: Never let any future information leak into the features. Ensure your rolling window ends **exactly at time \\( t \\)**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Feature Engineering from Rolling Windows\n",
    "Use **one or multiple window sizes** (as you suggested) to capture both short‑term and long‑term dynamics. For each time \\( t \\) and each window length \\( L \\), compute features from:\n",
    "- The 10 exogenous series \\( X_1 \\dots X_{10} \\)\n",
    "- The target series \\( y \\) itself (past values only!)\n",
    "\n",
    "### Typical features (per time series per window):\n",
    "- **Statistical**: mean, median, standard deviation, min, max, range, skew, kurtosis.\n",
    "- **Trend**: slope from linear regression, difference between first and last value.\n",
    "- **Frequency**: FFT coefficients, entropy.\n",
    "- **Change points**: number of local maxima/minima.\n",
    "- **Autocorrelation**: lag‑1 autocorrelation.\n",
    "- **Quantiles**: 25%, 75%, 90%, etc.\n",
    "\n",
    "**If you have many window sizes**, concatenate all features into a single row for each \\( t \\).  \n",
    "**Dimensionality can explode** – later we will use pattern mining that is robust to many features, or apply a feature selection step.\n",
    "\n",
    "> **Tip**: Use a library like `tsfresh` (Python) to automatically generate a rich feature set, then filter irrelevant features with a univariate test or a tree‑based importance measure.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Build the Supervised Dataset\n",
    "For each valid \\( t \\):\n",
    "- **Features**: vector \\( F_t \\) = all rolling‑window features computed up to time \\( t \\).\n",
    "- **Label**: \\( Y_t^{bad} \\) (0/1).\n",
    "\n",
    "The result is a **temporally ordered** classification dataset. Because of time‑dependence, **always split data chronologically** (e.g., train: first 60%, validation: next 20%, test: last 20%). Never shuffle randomly across time.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Mining Patterns That “Almost Certainly” Lead to the Worst Label\n",
    "You want **regions in feature space** where the probability of \\( Y^{bad}=1 \\) is extremely high (e.g., ≥ 95%). Several methods are well‑suited:\n",
    "\n",
    "### ✅ **Option A: Decision Tree with High‑Precision Leaf Extraction**\n",
    "Train a single decision tree (or a Random Forest, then distill) with:\n",
    "- **Class weight** = balance or overweight the “bad” class.\n",
    "- **min_samples_leaf** = large enough (e.g., 30–100) to ensure stable estimates.\n",
    "- **max_depth** = limited for interpretability.\n",
    "\n",
    "After fitting, **traverse the tree** and collect all leaves where the proportion of “bad” instances is above a threshold (e.g., 0.95). The **path conditions** from root to leaf become your patterns.  \n",
    "*Advantage*: Very simple, built into scikit‑learn. *Caveat*: May miss non‑rectangular patterns.\n",
    "\n",
    "### ✅ **Option B: Subgroup Discovery / PRIM (Patient Rule Induction Method)**\n",
    "PRIM is explicitly designed to find **boxes in feature space** with a high average response.  \n",
    "- It starts with the full data, then “peels” off the smallest or largest values of one feature to incrementally increase the proportion of the target class.  \n",
    "- The result is a set of axis‑aligned rectangles, each described by simple conditions (e.g., `feature3 > 0.7` and `feature7 < -0.2`).  \n",
    "- You can then filter those boxes with **precision > 0.95**.  \n",
    "\n",
    "Python implementations exist in `pysubgroup`, `sklearn-gbmi`, or you can write a custom PRIM wrapper.\n",
    "\n",
    "### ✅ **Option C: Association Rule Mining (after Discretization)**\n",
    "1. **Discretize** every continuous feature into a few bins (e.g., low/medium/high using quantiles).  \n",
    "2. Transform each time point into a **transaction**: a set of items `(feature_name, bin)` plus the class label as another item.  \n",
    "3. Run **Apriori** or **FP‑growth** to find rules of the form `{conditions} → {Y_bad}`.  \n",
    "4. Filter rules by **confidence ≥ 0.95** and **lift > 1**.  \n",
    "\n",
    "*Advantage*: Discovers non‑rectangular combinations. *Caveat*: Many redundant rules; need pruning.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Validate and Select Patterns\n",
    "- **Temporal validation**: Use the **chronologically later validation set** to estimate the **true precision** of each pattern.  \n",
    "- Patterns that look perfect in‑sample may degrade out‑of‑sample – always test on unseen time periods.  \n",
    "- **Combine multiple patterns**: The union of several high‑precision patterns can increase coverage while maintaining acceptable overall precision.  \n",
    "\n",
    "**Metric to optimize**: **Precision@K** – e.g., among all times flagged by the pattern(s), what fraction were actually bad?  \n",
    "**Secondary metric**: **Recall** – how many of all bad events are caught? (But your focus is “almost certainly”, so precision is primary.)\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Practical Implementation Tips\n",
    "\n",
    "| Challenge | Solution |\n",
    "|-----------|----------|\n",
    "| **Data leakage** | Strictly keep future information out of features. Use `pandas` shift and expanding windows. |\n",
    "| **Many features** | Apply feature selection (e.g., Boruta, permutation importance) before pattern mining. |\n",
    "| **Overfitting** | Use large `min_samples_leaf` or prune PRIM boxes with a minimum support constraint. |\n",
    "| **Interpretability** | Prefer methods that produce few, short conditions. Decision tree paths and PRIM boxes are ideal. |\n",
    "| **Real‑time deployment** | Once patterns are fixed, they can be checked every new time step by evaluating the conditions on the latest rolling window. |\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "The combination of **rolling‑window feature extraction** + **high‑precision rule mining** (via decision tree leaves, PRIM, or association rules) directly addresses your goal. This approach is widely used in predictive maintenance, financial risk, and healthcare early‑warning systems. Start simple with a shallow decision tree – it often yields surprisingly interpretable and robust patterns.\n",
    "\n",
    "Would you like more details on any specific step (e.g., feature engineering, PRIM implementation, handling imbalanced labels)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e845ea8",
   "metadata": {},
   "source": [
    "For your specific goal—**discovering interpretable feature patterns that “almost certainly” lead to worst outcomes**—the choice between a classic decision tree and gradient boosted trees hinges on a **trade‑off between interpretability and predictive power**. Below I compare both, address your question about how decision trees treat features, and give clear recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Should you use a classic decision tree or gradient boosting?\n",
    "\n",
    "**✅ Recommendation: Start with a classic decision tree (CART).**  \n",
    "- Your primary objective is **pattern discovery**, not maximising raw predictive accuracy.  \n",
    "- A single decision tree provides **explicit, human‑readable rules** (e.g., `if x1_mean_win5 > 0.7 and y_std_win20 < 0.2 then bad = 1`).  \n",
    "- You can directly extract leaves with **≥95% bad cases** and inspect the path conditions.  \n",
    "- It is **fast, requires minimal tuning**, and is robust to the time‑series structure when you use chronological splits.\n",
    "\n",
    "**⚠️ Consider gradient boosting only if:**  \n",
    "- You have already tried a single tree and its precision/recall is too low for your application.  \n",
    "- You are willing to invest time in **post‑hoc rule extraction** (e.g., using `sklearn.tree.export_text` on each boosting iteration, or surrogate decision tree distillation).  \n",
    "- You need to handle **very high‑dimensional feature spaces** where a single tree may overfit or miss subtle interactions.\n",
    "\n",
    "**Bottom line:** For “almost certain” rules, a shallow, well‑regularised decision tree is often sufficient and far easier to interpret. Start there.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Does a decision tree consider relative values / ranks / cross‑sectional relationships?\n",
    "\n",
    "**❌ No – by default, a decision tree splits on the **absolute values** of individual features, not on their rank or cross‑sample comparisons.**  \n",
    "\n",
    "- Each split is of the form: `feature ≤ threshold`.  \n",
    "- The threshold is chosen to maximise purity of the child nodes.  \n",
    "- It does **not** inherently know the rank of a sample among its neighbours, nor does it compare one sample’s value to another’s across the dataset *at the same time*.\n",
    "\n",
    "**✅ But you can easily engineer such relational features yourself** and feed them to the tree:  \n",
    "- **Rank features**: For each rolling window, compute the **percentile rank** of the current value within the window (or within the entire training set).  \n",
    "- **Cross‑sectional features**: If you have multiple time series (e.g., 10 sensors), you can compute the **rank of each sensor’s value** among all 10 at the same time point.  \n",
    "- **Relative change**: `(current - mean_of_window)/std_of_window` (z‑score).  \n",
    "\n",
    "Once these derived features are added to your dataset, the decision tree will happily split on them—it treats them as ordinary numeric columns.\n",
    "\n",
    "**What about cross‑sample interactions?**  \n",
    "A decision tree **does** capture cross‑sectional interactions *through its structure*: e.g., a rule like `x1_mean_win5 > 0.5 AND x2_min_win20 < 0.1` describes a **subgroup** of time points where both conditions hold. This is a form of cross‑sectional grouping, but it is built step‑by‑step from univariate splits.\n",
    "\n",
    "**Summary:** The tree itself does not “see” the relative ordering across samples unless you explicitly provide that information as features.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Pros and Cons: Classic Decision Tree vs. Gradient Boosted Trees\n",
    "\n",
    "| Aspect | Classic Decision Tree (CART) | Gradient Boosted Trees (GBDT) |\n",
    "|--------|------------------------------|-------------------------------|\n",
    "| **Interpretability** | ✅ **Excellent** – entire model is a single flowchart. Paths = explicit rules. | ❌ **Poor** – hundreds of trees. Black‑box. Feature importance only global. |\n",
    "| **Rule extraction** | ✅ **Trivial** – directly from tree leaves. | ⚠️ **Cumbersome** – need to aggregate rules from all trees (e.g., `xgboost.to_graphviz()` each tree) or use surrogate models. |\n",
    "| **Predictive accuracy** | ⚠️ **Moderate** – often underfits or overfits without careful pruning. | ✅ **High** – state‑of‑the‑art for tabular data. |\n",
    "| **Handling of interactions** | ✅ Captures interactions via nested splits. | ✅ Automatically learns complex interactions. |\n",
    "| **Training speed** | ✅ Very fast. | ⚠️ Slower (but still feasible for moderate data). |\n",
    "| **Hyperparameter tuning** | ✅ Minimal (max_depth, min_samples_leaf). | ❌ Many parameters (learning rate, n_estimators, subsample, etc.). |\n",
    "| **Stability** | ❌ Unstable – small data changes → very different tree. | ✅ More stable due to averaging. |\n",
    "| **Overfitting risk** | ⚠️ High without pruning/limiting depth. | ✅ Controlled via shrinkage and regularisation. |\n",
    "| **Feature scaling** | ✅ Not required. | ✅ Not required (tree‑based). |\n",
    "| **Missing values** | ✅ Can handle internally. | ✅ Most implementations handle natively. |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Concrete Advice for Your Pipeline\n",
    "\n",
    "1. **Start simple**  \n",
    "   - Build a **single decision tree** with `max_depth=3–4` and `min_samples_leaf=50` (or larger, depending on your dataset size).  \n",
    "   - Inspect the leaves – print the proportion of `bad=1` in each leaf.  \n",
    "   - If some leaves have ≥95% bad, you already have your patterns.\n",
    "\n",
    "2. **If you need more coverage/recall**  \n",
    "   - Try a **Random Forest** – each tree is interpretable individually. You can:  \n",
    "     - Extract all leaves from all trees, compute precision per leaf, and keep the high‑precision rules.  \n",
    "     - Or train a **single shallow decision tree** on the **output** of the Random Forest (a surrogate model).  \n",
    "\n",
    "3. **If you absolutely must use gradient boosting** (e.g., for a production early‑warning system where accuracy is paramount)  \n",
    "   - Train an XGBoost/LightGBM model.  \n",
    "   - Use its `.get_booster().get_dump()` to export all trees and parse the splitting conditions.  \n",
    "   - For each leaf in each tree, calculate precision on the training set and filter those with ≥95% bad.  \n",
    "   - **Be cautious**: rules from early trees may be weaker; you may want to **prune** rules by removing redundant conditions.\n",
    "\n",
    "---\n",
    "## 5. Final Verdict\n",
    "\n",
    "**Use a classic decision tree for your stated goal.**  \n",
    "It directly answers your question: *“which feature patterns almost certainly lead to worst labels?”*  \n",
    "You will obtain a small, inspectable set of rules that you can validate on a chronologically later test set.\n",
    "\n",
    "Once you have those rules, you can deploy them as an **interpretable alarm system** – whenever the current rolling‑window features satisfy the rule, you predict a high risk of a bad outcome. This is exactly the “avoid scenarios with highest probability of bad outcomes” you are aiming for.\n",
    "\n",
    "Would you like a complete code example showing how to extract high‑precision leaves from a decision tree and convert them into human‑readable rules?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "823aaa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.proj import DB\n",
    "market_risk = DB.load_df('data/DataBase/DB_exposure/daily_risk/2024/daily_risk.20241101.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20343c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20100104</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <td>-0.006016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.005298</td>\n",
       "      <td>0.011650</td>\n",
       "      <td>0.009542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.009316</td>\n",
       "      <td>0.003526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20100105</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.009077</td>\n",
       "      <td>0.011524</td>\n",
       "      <td>-0.000928</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>0.012587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20100106</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <td>-0.004712</td>\n",
       "      <td>-0.004803</td>\n",
       "      <td>-0.001548</td>\n",
       "      <td>0.003133</td>\n",
       "      <td>0.002613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.007071</td>\n",
       "      <td>-0.002099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20100107</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>-0.019865</td>\n",
       "      <td>-0.019669</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>-0.000730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>-0.020595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20100108</th>\n",
       "      <th>1</th>\n",
       "      <th>4</th>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.008382</td>\n",
       "      <td>-0.003383</td>\n",
       "      <td>0.006519</td>\n",
       "      <td>0.007098</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.004905</td>\n",
       "      <td>0.012980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250414</th>\n",
       "      <th>1</th>\n",
       "      <th>3708</th>\n",
       "      <td>0.008699</td>\n",
       "      <td>0.023370</td>\n",
       "      <td>-0.006399</td>\n",
       "      <td>-0.000490</td>\n",
       "      <td>0.004323</td>\n",
       "      <td>-0.005303</td>\n",
       "      <td>-0.006154</td>\n",
       "      <td>-0.005405</td>\n",
       "      <td>0.023872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250415</th>\n",
       "      <th>1</th>\n",
       "      <th>3709</th>\n",
       "      <td>-0.001305</td>\n",
       "      <td>0.006703</td>\n",
       "      <td>0.001862</td>\n",
       "      <td>-0.003067</td>\n",
       "      <td>-0.002820</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.006527</td>\n",
       "      <td>0.003735</td>\n",
       "      <td>0.000115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250416</th>\n",
       "      <th>1</th>\n",
       "      <th>3710</th>\n",
       "      <td>-0.005240</td>\n",
       "      <td>-0.016134</td>\n",
       "      <td>0.008320</td>\n",
       "      <td>-0.002912</td>\n",
       "      <td>-0.008064</td>\n",
       "      <td>-0.006825</td>\n",
       "      <td>0.013358</td>\n",
       "      <td>0.014313</td>\n",
       "      <td>-0.022818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250417</th>\n",
       "      <th>1</th>\n",
       "      <th>3711</th>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.014376</td>\n",
       "      <td>-0.001279</td>\n",
       "      <td>-0.001094</td>\n",
       "      <td>-0.000398</td>\n",
       "      <td>-0.000246</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>-0.000593</td>\n",
       "      <td>0.002346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250418</th>\n",
       "      <th>1</th>\n",
       "      <th>3712</th>\n",
       "      <td>-0.000226</td>\n",
       "      <td>-0.003171</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>-0.001123</td>\n",
       "      <td>0.002947</td>\n",
       "      <td>-0.007853</td>\n",
       "      <td>-0.000623</td>\n",
       "      <td>0.003034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3713 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        x0        x1        x2        x3        x4        x5  \\\n",
       "date     id time                                                               \n",
       "20100104 1  0    -0.006016  0.000000 -0.005298  0.011650  0.009542       NaN   \n",
       "20100105 1  1     0.009077  0.011524 -0.000928  0.001332  0.003510       NaN   \n",
       "20100106 1  2    -0.004712 -0.004803 -0.001548  0.003133  0.002613       NaN   \n",
       "20100107 1  3    -0.019865 -0.019669  0.000024 -0.000161 -0.000730       NaN   \n",
       "20100108 1  4     0.005882  0.008382 -0.003383  0.006519  0.007098       NaN   \n",
       "...                    ...       ...       ...       ...       ...       ...   \n",
       "20250414 1  3708  0.008699  0.023370 -0.006399 -0.000490  0.004323 -0.005303   \n",
       "20250415 1  3709 -0.001305  0.006703  0.001862 -0.003067 -0.002820  0.000005   \n",
       "20250416 1  3710 -0.005240 -0.016134  0.008320 -0.002912 -0.008064 -0.006825   \n",
       "20250417 1  3711  0.001120  0.014376 -0.001279 -0.001094 -0.000398 -0.000246   \n",
       "20250418 1  3712 -0.000226 -0.003171  0.000306  0.000894 -0.001123  0.002947   \n",
       "\n",
       "                        x6        x7         y  \n",
       "date     id time                                \n",
       "20100104 1  0          NaN -0.009316  0.003526  \n",
       "20100105 1  1          NaN  0.002593  0.012587  \n",
       "20100106 1  2          NaN -0.007071 -0.002099  \n",
       "20100107 1  3          NaN -0.000125 -0.020595  \n",
       "20100108 1  4          NaN -0.004905  0.012980  \n",
       "...                    ...       ...       ...  \n",
       "20250414 1  3708 -0.006154 -0.005405  0.023872  \n",
       "20250415 1  3709 -0.006527  0.003735  0.000115  \n",
       "20250416 1  3710  0.013358  0.014313 -0.022818  \n",
       "20250417 1  3711  0.001150 -0.000593  0.002346  \n",
       "20250418 1  3712 -0.007853 -0.000623  0.003034  \n",
       "\n",
       "[3713 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from anomaly_precursor import get_input_data , ClassicDecisionTree\n",
    "\n",
    "df = get_input_data()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8319dd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad event ratio: 0.088 , compare to bad_percentage: 0.10 , bad_threshold: -0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling: 100%|██████████| 20/20 [00:02<00:00,  8.88it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:04<00:00,  4.76it/s]\n",
      "Rolling: 100%|██████████| 20/20 [00:02<00:00,  9.44it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:04<00:00,  4.69it/s]\n",
      "Rolling: 100%|██████████| 20/20 [00:02<00:00,  9.23it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:04<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall precision: 0.15\n",
      "\n",
      "Found 14 leaves with ≥20% precision in training.\n",
      "Leaf 3: precision=0.529, support=17, rule: x0__root_mean_square_20 ≤ 3.854 AND x3__minimum_20 ≤ 0.378 AND x5__minimum_20 ≤ -0.007\n",
      "Leaf 8: precision=0.370, support=27, rule: x0__root_mean_square_20 ≤ 3.854 AND x3__minimum_20 ≤ 0.378 AND x5__minimum_20 > -0.007 AND x6_missing__mean_20 ≤ 0.439 AND x0__root_mean_square_10 > 0.855 AND x0__root_mean_square_10 ≤ 0.888\n",
      "Leaf 10: precision=0.600, support=10, rule: x0__root_mean_square_20 ≤ 3.854 AND x3__minimum_20 ≤ 0.378 AND x5__minimum_20 > -0.007 AND x6_missing__mean_20 > 0.439\n",
      "Leaf 14: precision=0.300, support=10, rule: x0__root_mean_square_20 ≤ 3.854 AND x3__minimum_20 > 0.378 AND x5__standard_deviation_20 ≤ 0.984 AND x1__minimum_10 ≤ -0.313 AND x7__sum_values_20 ≤ -0.252\n",
      "Leaf 16: precision=1.000, support=13, rule: x0__root_mean_square_20 ≤ 3.854 AND x3__minimum_20 > 0.378 AND x5__standard_deviation_20 ≤ 0.984 AND x1__minimum_10 ≤ -0.313 AND x7__sum_values_20 > -0.252 AND x4__length_10 ≤ 1.608\n",
      "Leaf 3 has no validation samples\n",
      "Leaf 8 validation precision: 0.294 (n=17)\n",
      "Leaf 10 has no validation samples\n",
      "Leaf 14 validation precision: 0.000 (n=5)\n",
      "Leaf 16 validation precision: 0.000 (n=5)\n",
      "Leaf 17 has no validation samples\n",
      "Leaf 22 has no validation samples\n",
      "Leaf 26 has no validation samples\n",
      "Leaf 35 has no validation samples\n",
      "Leaf 36 has no validation samples\n",
      "Leaf 37 has no validation samples\n",
      "Leaf 38 has no validation samples\n",
      "Leaf 41 has no validation samples\n",
      "Leaf 43 validation precision: 0.000 (n=6)\n",
      "\n",
      "Average validation precision of high precision leaves: 0.021\n"
     ]
    }
   ],
   "source": [
    "from anomaly_precursor import get_input_data , ClassicDecisionTree\n",
    "\n",
    "df = get_input_data()\n",
    "model = ClassicDecisionTree(df , 'severe')\n",
    "model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4baeb5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad event ratio: 0.187 , compare to bad_percentage: 0.20 , bad_threshold: -0.01\n",
      "Total windows: 3673, Features: 11\n",
      "Train: 2930, Val: 743\n",
      "Epoch 10/100 | Train Loss: 0.0315 | Val Loss: 0.0330 | Real Bad: 151/743 | Pred Bad: 743/743 | Precise Bad: 151/743\n",
      "Early stopping at epoch 20\n",
      "Epoch 20/100 | Train Loss: 0.0319 | Val Loss: 0.0338 | Real Bad: 151/743 | Pred Bad: 743/743 | Precise Bad: 151/743\n",
      "Found 11 leaves with ≥30% precision (train).\n",
      "Leaf 4: prec=0.600, sup=10, rule: embed_0 ≤ -0.234 AND embed_48 ≤ -1.003 AND embed_9 > -0.050\n",
      "Leaf 6: prec=1.000, sup=23, rule: embed_0 ≤ -0.234 AND embed_48 > -1.003 AND embed_19 ≤ -0.049\n",
      "Leaf 7: prec=0.500, sup=10, rule: embed_0 ≤ -0.234 AND embed_48 > -1.003 AND embed_19 > -0.049\n",
      "Leaf 19: prec=0.425, sup=106, rule: embed_0 > -0.234 AND embed_53 ≤ -0.938 AND embed_10 > -0.333 AND embed_53 ≤ -0.944 AND embed_54 > -0.001 AND embed_7 ≤ 0.319\n",
      "Leaf 23: prec=0.870, sup=23, rule: embed_0 > -0.234 AND embed_53 ≤ -0.938 AND embed_10 > -0.333 AND embed_53 > -0.944 AND embed_9 ≤ 0.119 AND embed_58 ≤ -0.017\n",
      "Leaf 4 has no validation samples\n",
      "Leaf 6 has no validation samples\n",
      "Leaf 7 has no validation samples\n",
      "Leaf 19 val prec: 0.243 (n=37)\n",
      "Leaf 23 val prec: 0.167 (n=6)\n",
      "Leaf 24 val prec: 0.000 (n=5)\n",
      "Leaf 31 has no validation samples\n",
      "Leaf 33 val prec: 0.333 (n=3)\n",
      "Leaf 46 has no validation samples\n",
      "Leaf 48 val prec: 0.125 (n=136)\n",
      "Leaf 53 val prec: 0.000 (n=1)\n",
      "ratio of bad events in validation set: 20.32% (151/743)\n",
      "Average val precision of high-precision leaves: 14.89% (28/188 samples)\n"
     ]
    }
   ],
   "source": [
    "from anomaly_precursor import get_input_data , NNDecisionTree\n",
    "\n",
    "df = get_input_data()\n",
    "model = NNDecisionTree(df , 'moderate' , 'transformer')\n",
    "model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3a7971",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d634690",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bddc99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.dataset['train_idx']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
