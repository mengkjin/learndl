{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try using d:\\Coding\\learndl\\learndl\\data\\DataSet/day.20240607.pt , success!\n",
      "Load  2 DataBlocks...... finished! Cost 0.04 secs\n",
      "Align 2 DataBlocks...... finished! Cost 0.16 secs\n",
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n"
     ]
    }
   ],
   "source": [
    "import os , torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from typing import Any , Literal , Optional\n",
    "\n",
    "from src.nn_model.classes import BatchOutput\n",
    "from src.nn_model.trainer.models import ModelEnsembler\n",
    "from src.nn_model.trainer import NetDataModule\n",
    "from src.nn_model.util import Deposition , Device , TrainConfig\n",
    "from src.basic import RegModel , PATH , REG_MODELS\n",
    "\n",
    "class HiddenExtractor:\n",
    "    '''for a model to predict recent/history data'''\n",
    "    def __init__(self , reg_model : RegModel):\n",
    "        self.reg_model = reg_model \n",
    "\n",
    "        self.contents : dict[str,pd.DataFrame] = {}\n",
    "\n",
    "        self.config     = TrainConfig.load(f'{PATH.model}/{self.reg_model.name}' , override={'short_test':False})\n",
    "        self.deposition = Deposition(self.config)\n",
    "        self.device     = Device()\n",
    "\n",
    "        self.data = NetDataModule(self.config , False).load_data()\n",
    "        self.target_path = os.path.join(PATH.hidden , self.reg_model.name)\n",
    "\n",
    "        if not np.isin(self.reg_model.model_dates , self.data.model_date_list).all():\n",
    "            print('Caution! Not all model dates are in data.model_date_list, possibly due to short_test!')\n",
    "            self.model_dates = self.data.model_date_list\n",
    "        else:\n",
    "            self.model_dates = self.reg_model.model_dates\n",
    "\n",
    "    def hidden_key(self , model_num , model_type , model_date) : \n",
    "        return f'hidden.{model_num}.{model_type}.{model_date}.feather'\n",
    "\n",
    "    def deploy(self):\n",
    "        '''deploy df in contents to target path'''\n",
    "        os.makedirs(self.target_path , exist_ok=True)\n",
    "        for hidden_key , hidden_df in self.contents.items():\n",
    "            hidden_df.to_feather(os.path.join(self.target_path , hidden_key))\n",
    "        return self\n",
    "\n",
    "    def update_model_iter(self):\n",
    "        model_iter = [(d , n , t) for (d , n , t) \n",
    "                      in product(self.model_dates[:-1] , self.reg_model.model_nums , self.reg_model.model_types)\n",
    "                      if not os.path.exists(os.path.join(self.target_path , self.hidden_key(n , t , d)))]\n",
    "        model_iter += list(product(self.model_dates[-1:] , self.reg_model.model_nums , self.reg_model.model_types))\n",
    "        return model_iter\n",
    "    \n",
    "    def given_model_iter(self , model_dates : Optional[list | np.ndarray | int] = None):\n",
    "        if model_dates is None:\n",
    "            print('Input model_dates to extract.')\n",
    "            return []\n",
    "        \n",
    "        if isinstance(model_dates , int): model_dates = [model_dates]\n",
    "        model_dates = np.intersect1d(self.model_dates , model_dates)\n",
    "        model_iter = list(product(model_dates , self.reg_model.model_nums , self.reg_model.model_types))\n",
    "        return model_iter\n",
    "\n",
    "    def extract_hidden(self , what : Literal['given' , 'update'] , model_dates : Optional[list | np.ndarray | int] = None ,\n",
    "                       verbose = True , deploy = False):\n",
    "        if what == 'given':\n",
    "            model_iter = self.given_model_iter(model_dates)\n",
    "        elif what == 'update':\n",
    "            model_iter = self.update_model_iter()\n",
    "        else:\n",
    "            raise KeyError(what)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for model_date , model_num , model_type in model_iter:\n",
    "                hidden_key = self.hidden_key(model_num , model_type , model_date)\n",
    "                hidden_df  = self.model_hidden(model_num , model_type , model_date , verbose = verbose)\n",
    "                if deploy:\n",
    "                    hidden_df.to_feather(os.path.join(self.target_path , hidden_key))\n",
    "                else:\n",
    "                    self.contents[hidden_key] = hidden_df\n",
    "        return self\n",
    "    \n",
    "    def model_hidden(self , model_num , model_type , model_date , verbose = True) -> pd.DataFrame:\n",
    "        model_param = self.config.model_param[model_num]\n",
    "        \n",
    "        model = self.deposition.load_model(model_date , model_num , model_type)\n",
    "        self.net = ModelEnsembler.get_net(self.config.model_module , model_param , model['state_dict'] , self.device)\n",
    "        self.net.eval()\n",
    "\n",
    "        df_list : list[pd.DataFrame] = []\n",
    "        desc = f'Extract {model_num}/{model_type}/{model_date}' if verbose else ''\n",
    "\n",
    "        self.data.setup('fit' ,  model_param , model_date)\n",
    "        df_list += self.loader_hidden('train' , desc)\n",
    "        df_list += self.loader_hidden('valid' , desc)\n",
    "\n",
    "        self.data.setup('test' ,  model_param , model_date)\n",
    "        df_list += self.loader_hidden('test' , desc)\n",
    "\n",
    "        df = pd.concat(df_list , axis=0)\n",
    "        return df\n",
    "    \n",
    "    def loader_hidden(self, dataset : Literal['train' , 'valid' , 'test'] , desc = ''):\n",
    "        if dataset == 'train': loader = self.data.train_dataloader()\n",
    "        elif dataset == 'valid': loader = self.data.val_dataloader()\n",
    "        elif dataset == 'test': loader = self.data.test_dataloader()\n",
    "\n",
    "        df_list : list[pd.DataFrame] = []\n",
    "        if desc: \n",
    "            loader = tqdm(loader , total=len(loader))\n",
    "            desc = f'{desc}/{dataset}'\n",
    "\n",
    "        for batch_data in loader:\n",
    "            batch_output = BatchOutput(self.net(batch_data.x))\n",
    "            df = batch_output.hidden_df(batch_data , self.data.y_secid , self.data.y_date).assign(dataset = dataset)\n",
    "            df_list.append(df)\n",
    "            if isinstance(loader , tqdm): loader.set_description(desc)\n",
    "        return df_list\n",
    "    \n",
    "aa = HiddenExtractor(REG_MODELS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extract 0/best/20170103/train: 100%|██████████| 380/380 [00:39<00:00,  9.70it/s]\n",
      "Extract 0/best/20170103/valid: 100%|██████████| 96/96 [00:12<00:00,  7.84it/s]\n",
      "Extract 0/best/20170103/test: 100%|██████████| 120/120 [00:18<00:00,  6.37it/s]\n",
      "Extract 0/swabest/20170103/train: 100%|██████████| 380/380 [00:43<00:00,  8.78it/s]\n",
      "Extract 0/swabest/20170103/valid: 100%|██████████| 96/96 [00:12<00:00,  7.82it/s]\n",
      "Extract 0/swabest/20170103/test: 100%|██████████| 120/120 [00:16<00:00,  7.14it/s]\n",
      "Extract 0/swalast/20170103/train: 100%|██████████| 380/380 [00:42<00:00,  9.00it/s]\n",
      "Extract 0/swalast/20170103/valid: 100%|██████████| 96/96 [00:14<00:00,  6.59it/s]\n",
      "Extract 0/swalast/20170103/test: 100%|██████████| 120/120 [00:16<00:00,  7.19it/s]\n",
      "Extract 1/best/20170103/train: 100%|██████████| 380/380 [01:24<00:00,  4.47it/s]\n",
      "Extract 1/best/20170103/valid: 100%|██████████| 96/96 [00:28<00:00,  3.34it/s]\n",
      "Extract 1/best/20170103/test: 100%|██████████| 120/120 [00:43<00:00,  2.78it/s]\n",
      "Extract 1/swabest/20170103/train: 100%|██████████| 380/380 [01:25<00:00,  4.43it/s]\n",
      "Extract 1/swabest/20170103/valid: 100%|██████████| 96/96 [00:30<00:00,  3.17it/s]\n",
      "Extract 1/swabest/20170103/test: 100%|██████████| 120/120 [00:38<00:00,  3.13it/s]\n",
      "Extract 1/swalast/20170103/train: 100%|██████████| 380/380 [01:28<00:00,  4.29it/s]\n",
      "Extract 1/swalast/20170103/valid: 100%|██████████| 96/96 [00:28<00:00,  3.43it/s]\n",
      "Extract 1/swalast/20170103/test: 100%|██████████| 120/120 [00:39<00:00,  3.03it/s]\n",
      "Extract 0/best/20170704/train: 100%|██████████| 384/384 [00:49<00:00,  7.70it/s]\n",
      "Extract 0/best/20170704/valid: 100%|██████████| 96/96 [00:14<00:00,  6.62it/s]\n",
      "Extract 0/best/20170704/test: 100%|██████████| 120/120 [00:23<00:00,  5.16it/s]\n",
      "Extract 0/swabest/20170704/train: 100%|██████████| 384/384 [00:45<00:00,  8.42it/s]\n",
      "Extract 0/swabest/20170704/valid: 100%|██████████| 96/96 [00:14<00:00,  6.68it/s]\n",
      "Extract 0/swabest/20170704/test: 100%|██████████| 120/120 [00:19<00:00,  6.21it/s]\n",
      "Extract 0/swalast/20170704/train: 100%|██████████| 384/384 [00:46<00:00,  8.31it/s]\n",
      "Extract 0/swalast/20170704/valid: 100%|██████████| 96/96 [00:13<00:00,  7.11it/s]\n",
      "Extract 0/swalast/20170704/test: 100%|██████████| 120/120 [00:19<00:00,  6.23it/s]\n",
      "Extract 1/best/20170704/train: 100%|██████████| 384/384 [01:23<00:00,  4.59it/s]\n",
      "Extract 1/best/20170704/valid: 100%|██████████| 96/96 [00:29<00:00,  3.24it/s]\n",
      "Extract 1/best/20170704/test: 100%|██████████| 120/120 [00:39<00:00,  3.03it/s]\n",
      "Extract 1/swabest/20170704/train: 100%|██████████| 384/384 [01:21<00:00,  4.70it/s]\n",
      "Extract 1/swabest/20170704/valid: 100%|██████████| 96/96 [00:26<00:00,  3.60it/s]\n",
      "Extract 1/swabest/20170704/test: 100%|██████████| 120/120 [00:39<00:00,  3.08it/s]\n",
      "Extract 1/swalast/20170704/train: 100%|██████████| 384/384 [01:23<00:00,  4.58it/s]\n",
      "Extract 1/swalast/20170704/valid: 100%|██████████| 96/96 [00:27<00:00,  3.50it/s]\n",
      "Extract 1/swalast/20170704/test: 100%|██████████| 120/120 [00:39<00:00,  3.04it/s]\n",
      "Extract 0/best/20171226/train: 100%|██████████| 384/384 [00:47<00:00,  8.17it/s]\n",
      "Extract 0/best/20171226/valid: 100%|██████████| 96/96 [00:14<00:00,  6.65it/s]\n",
      "Extract 0/best/20171226/test: 100%|██████████| 120/120 [00:21<00:00,  5.65it/s]\n",
      "Extract 0/swabest/20171226/train: 100%|██████████| 384/384 [00:56<00:00,  6.80it/s]\n",
      "Extract 0/swabest/20171226/valid: 100%|██████████| 96/96 [00:18<00:00,  5.08it/s]\n",
      "Extract 0/swabest/20171226/test: 100%|██████████| 120/120 [00:25<00:00,  4.66it/s]\n",
      "Extract 0/swalast/20171226/train: 100%|██████████| 384/384 [01:03<00:00,  6.00it/s]\n",
      "Extract 0/swalast/20171226/valid: 100%|██████████| 96/96 [00:16<00:00,  5.70it/s]\n",
      "Extract 0/swalast/20171226/test: 100%|██████████| 120/120 [00:21<00:00,  5.67it/s]\n",
      "Extract 1/best/20171226/train: 100%|██████████| 384/384 [01:38<00:00,  3.90it/s]\n",
      "Extract 1/best/20171226/valid: 100%|██████████| 96/96 [00:32<00:00,  2.93it/s]\n",
      "Extract 1/best/20171226/test: 100%|██████████| 120/120 [00:52<00:00,  2.28it/s]\n",
      "Extract 1/swabest/20171226/train: 100%|██████████| 384/384 [01:40<00:00,  3.84it/s]\n",
      "Extract 1/swabest/20171226/valid: 100%|██████████| 96/96 [00:30<00:00,  3.17it/s]\n",
      "Extract 1/swabest/20171226/test: 100%|██████████| 120/120 [00:45<00:00,  2.61it/s]\n",
      "Extract 1/swalast/20171226/train: 100%|██████████| 384/384 [01:35<00:00,  4.04it/s]\n",
      "Extract 1/swalast/20171226/valid: 100%|██████████| 96/96 [00:33<00:00,  2.83it/s]\n",
      "Extract 1/swalast/20171226/test: 100%|██████████| 120/120 [00:47<00:00,  2.54it/s]\n",
      "Extract 0/best/20180627/train: 100%|██████████| 384/384 [00:55<00:00,  6.88it/s]\n",
      "Extract 0/best/20180627/valid: 100%|██████████| 96/96 [00:17<00:00,  5.44it/s]\n",
      "Extract 0/best/20180627/test: 100%|██████████| 120/120 [00:23<00:00,  5.14it/s]\n",
      "Extract 0/swabest/20180627/train: 100%|██████████| 384/384 [00:56<00:00,  6.86it/s]\n",
      "Extract 0/swabest/20180627/valid: 100%|██████████| 96/96 [00:17<00:00,  5.65it/s]\n",
      "Extract 0/swabest/20180627/test: 100%|██████████| 120/120 [00:23<00:00,  5.13it/s]\n",
      "Extract 0/swalast/20180627/train: 100%|██████████| 384/384 [00:58<00:00,  6.60it/s]\n",
      "Extract 0/swalast/20180627/valid: 100%|██████████| 96/96 [00:17<00:00,  5.60it/s]\n",
      "Extract 0/swalast/20180627/test: 100%|██████████| 120/120 [00:23<00:00,  5.07it/s]\n",
      "Extract 1/best/20180627/train: 100%|██████████| 384/384 [01:39<00:00,  3.85it/s]\n",
      "Extract 1/best/20180627/valid: 100%|██████████| 96/96 [00:33<00:00,  2.84it/s]\n",
      "Extract 1/best/20180627/test: 100%|██████████| 120/120 [00:50<00:00,  2.36it/s]\n",
      "Extract 1/swabest/20180627/train: 100%|██████████| 384/384 [01:41<00:00,  3.80it/s]\n",
      "Extract 1/swabest/20180627/valid: 100%|██████████| 96/96 [00:32<00:00,  2.94it/s]\n",
      "Extract 1/swabest/20180627/test: 100%|██████████| 120/120 [00:51<00:00,  2.32it/s]\n",
      "Extract 1/swalast/20180627/train: 100%|██████████| 384/384 [01:38<00:00,  3.90it/s]\n",
      "Extract 1/swalast/20180627/valid: 100%|██████████| 96/96 [00:37<00:00,  2.59it/s]\n",
      "Extract 1/swalast/20180627/test: 100%|██████████| 120/120 [00:51<00:00,  2.35it/s]\n",
      "Extract 0/best/20181220/train: 100%|██████████| 384/384 [00:55<00:00,  6.91it/s]\n",
      "Extract 0/best/20181220/valid: 100%|██████████| 96/96 [00:16<00:00,  5.67it/s]\n",
      "Extract 0/best/20181220/test: 100%|██████████| 120/120 [00:24<00:00,  4.90it/s]\n",
      "Extract 0/swabest/20181220/train: 100%|██████████| 384/384 [00:55<00:00,  6.86it/s]\n",
      "Extract 0/swabest/20181220/valid: 100%|██████████| 96/96 [00:17<00:00,  5.45it/s]\n",
      "Extract 0/swabest/20181220/test: 100%|██████████| 120/120 [00:23<00:00,  5.02it/s]\n",
      "Extract 0/swalast/20181220/train: 100%|██████████| 384/384 [00:58<00:00,  6.54it/s]\n",
      "Extract 0/swalast/20181220/valid: 100%|██████████| 96/96 [00:17<00:00,  5.42it/s]\n",
      "Extract 0/swalast/20181220/test: 100%|██████████| 120/120 [00:23<00:00,  5.02it/s]\n",
      "Extract 1/best/20181220/train: 100%|██████████| 384/384 [01:41<00:00,  3.80it/s]\n",
      "Extract 1/best/20181220/valid: 100%|██████████| 96/96 [00:36<00:00,  2.62it/s]\n",
      "Extract 1/best/20181220/test: 100%|██████████| 120/120 [00:53<00:00,  2.25it/s]\n",
      "Extract 1/swabest/20181220/train: 100%|██████████| 384/384 [01:39<00:00,  3.84it/s]\n",
      "Extract 1/swabest/20181220/valid: 100%|██████████| 96/96 [00:35<00:00,  2.67it/s]\n",
      "Extract 1/swabest/20181220/test: 100%|██████████| 120/120 [00:52<00:00,  2.28it/s]\n",
      "Extract 1/swalast/20181220/train: 100%|██████████| 384/384 [01:43<00:00,  3.70it/s]\n",
      "Extract 1/swalast/20181220/valid: 100%|██████████| 96/96 [00:36<00:00,  2.61it/s]\n",
      "Extract 1/swalast/20181220/test: 100%|██████████| 120/120 [00:53<00:00,  2.26it/s]\n",
      "Extract 0/best/20190624/train: 100%|██████████| 384/384 [00:57<00:00,  6.62it/s]\n",
      "Extract 0/best/20190624/valid: 100%|██████████| 96/96 [00:18<00:00,  5.10it/s]\n",
      "Extract 0/best/20190624/test: 100%|██████████| 120/120 [00:24<00:00,  4.88it/s]\n",
      "Extract 0/swabest/20190624/train: 100%|██████████| 384/384 [00:58<00:00,  6.61it/s]\n",
      "Extract 0/swabest/20190624/valid: 100%|██████████| 96/96 [00:20<00:00,  4.64it/s]\n",
      "Extract 0/swabest/20190624/test: 100%|██████████| 120/120 [00:25<00:00,  4.77it/s]\n",
      "Extract 0/swalast/20190624/train: 100%|██████████| 384/384 [00:56<00:00,  6.84it/s]\n",
      "Extract 0/swalast/20190624/valid: 100%|██████████| 96/96 [00:18<00:00,  5.08it/s]\n",
      "Extract 0/swalast/20190624/test: 100%|██████████| 120/120 [00:24<00:00,  4.83it/s]\n",
      "Extract 1/best/20190624/train: 100%|██████████| 384/384 [01:43<00:00,  3.73it/s]\n",
      "Extract 1/best/20190624/valid: 100%|██████████| 96/96 [00:39<00:00,  2.45it/s]\n",
      "Extract 1/best/20190624/test: 100%|██████████| 120/120 [00:56<00:00,  2.14it/s]\n",
      "Extract 1/swabest/20190624/train: 100%|██████████| 384/384 [01:44<00:00,  3.69it/s]\n",
      "Extract 1/swabest/20190624/valid: 100%|██████████| 96/96 [00:38<00:00,  2.52it/s]\n",
      "Extract 1/swabest/20190624/test: 100%|██████████| 120/120 [00:54<00:00,  2.22it/s]\n",
      "Extract 1/swalast/20190624/train: 100%|██████████| 384/384 [01:46<00:00,  3.59it/s]\n",
      "Extract 1/swalast/20190624/valid: 100%|██████████| 96/96 [00:37<00:00,  2.54it/s]\n",
      "Extract 1/swalast/20190624/test: 100%|██████████| 120/120 [00:53<00:00,  2.25it/s]\n",
      "Extract 0/best/20191217/train: 100%|██████████| 384/384 [01:01<00:00,  6.24it/s]\n",
      "Extract 0/best/20191217/valid: 100%|██████████| 96/96 [00:19<00:00,  4.94it/s]\n",
      "Extract 0/best/20191217/test: 100%|██████████| 120/120 [00:27<00:00,  4.40it/s]\n",
      "Extract 0/swabest/20191217/train: 100%|██████████| 384/384 [01:03<00:00,  6.09it/s]\n",
      "Extract 0/swabest/20191217/valid: 100%|██████████| 96/96 [00:19<00:00,  5.02it/s]\n",
      "Extract 0/swabest/20191217/test: 100%|██████████| 120/120 [00:25<00:00,  4.70it/s]\n",
      "Extract 0/swalast/20191217/train: 100%|██████████| 384/384 [00:57<00:00,  6.64it/s]\n",
      "Extract 0/swalast/20191217/valid: 100%|██████████| 96/96 [00:18<00:00,  5.15it/s]\n",
      "Extract 0/swalast/20191217/test: 100%|██████████| 120/120 [00:24<00:00,  4.89it/s]\n",
      "Extract 1/best/20191217/train: 100%|██████████| 384/384 [01:53<00:00,  3.38it/s]\n",
      "Extract 1/best/20191217/valid: 100%|██████████| 96/96 [00:40<00:00,  2.36it/s]\n",
      "Extract 1/best/20191217/test: 100%|██████████| 120/120 [00:56<00:00,  2.14it/s]\n",
      "Extract 1/swabest/20191217/train: 100%|██████████| 384/384 [01:47<00:00,  3.59it/s]\n",
      "Extract 1/swabest/20191217/valid: 100%|██████████| 96/96 [00:42<00:00,  2.28it/s]\n",
      "Extract 1/swabest/20191217/test: 100%|██████████| 120/120 [00:56<00:00,  2.14it/s]\n",
      "Extract 1/swalast/20191217/train: 100%|██████████| 384/384 [01:45<00:00,  3.63it/s]\n",
      "Extract 1/swalast/20191217/valid: 100%|██████████| 96/96 [00:42<00:00,  2.24it/s]\n",
      "Extract 1/swalast/20191217/test: 100%|██████████| 120/120 [00:56<00:00,  2.13it/s]\n",
      "Extract 0/best/20200617/train: 100%|██████████| 384/384 [01:08<00:00,  5.60it/s]\n",
      "Extract 0/best/20200617/valid: 100%|██████████| 96/96 [00:20<00:00,  4.78it/s]\n",
      "Extract 0/best/20200617/test: 100%|██████████| 120/120 [00:30<00:00,  3.99it/s]\n",
      "Extract 0/swabest/20200617/train: 100%|██████████| 384/384 [01:03<00:00,  6.07it/s]\n",
      "Extract 0/swabest/20200617/valid: 100%|██████████| 96/96 [00:19<00:00,  4.86it/s]\n",
      "Extract 0/swabest/20200617/test: 100%|██████████| 120/120 [00:27<00:00,  4.34it/s]\n",
      "Extract 0/swalast/20200617/train: 100%|██████████| 384/384 [01:03<00:00,  6.01it/s]\n",
      "Extract 0/swalast/20200617/valid: 100%|██████████| 96/96 [00:23<00:00,  4.04it/s]\n",
      "Extract 0/swalast/20200617/test: 100%|██████████| 120/120 [00:28<00:00,  4.24it/s]\n",
      "Extract 1/best/20200617/train: 100%|██████████| 384/384 [02:03<00:00,  3.12it/s]\n",
      "Extract 1/best/20200617/valid: 100%|██████████| 96/96 [00:44<00:00,  2.16it/s]\n",
      "Extract 1/best/20200617/test: 100%|██████████| 120/120 [01:03<00:00,  1.90it/s]\n",
      "Extract 1/swabest/20200617/train: 100%|██████████| 384/384 [02:02<00:00,  3.14it/s]\n",
      "Extract 1/swabest/20200617/valid: 100%|██████████| 96/96 [00:44<00:00,  2.15it/s]\n",
      "Extract 1/swabest/20200617/test: 100%|██████████| 120/120 [01:02<00:00,  1.93it/s]\n",
      "Extract 1/swalast/20200617/train: 100%|██████████| 384/384 [02:05<00:00,  3.06it/s]\n",
      "Extract 1/swalast/20200617/valid: 100%|██████████| 96/96 [00:45<00:00,  2.09it/s]\n",
      "Extract 1/swalast/20200617/test: 100%|██████████| 120/120 [01:00<00:00,  2.00it/s]\n",
      "Extract 0/best/20201214/train: 100%|██████████| 384/384 [01:05<00:00,  5.82it/s]\n",
      "Extract 0/best/20201214/valid: 100%|██████████| 96/96 [00:22<00:00,  4.33it/s]\n",
      "Extract 0/best/20201214/test: 100%|██████████| 120/120 [00:32<00:00,  3.72it/s]\n",
      "Extract 0/swabest/20201214/train: 100%|██████████| 384/384 [01:07<00:00,  5.70it/s]\n",
      "Extract 0/swabest/20201214/valid: 100%|██████████| 96/96 [00:22<00:00,  4.24it/s]\n",
      "Extract 0/swabest/20201214/test: 100%|██████████| 120/120 [00:29<00:00,  4.10it/s]\n",
      "Extract 0/swalast/20201214/train: 100%|██████████| 384/384 [01:06<00:00,  5.78it/s]\n",
      "Extract 0/swalast/20201214/valid: 100%|██████████| 96/96 [00:21<00:00,  4.43it/s]\n",
      "Extract 0/swalast/20201214/test: 100%|██████████| 120/120 [00:27<00:00,  4.34it/s]\n",
      "Extract 1/best/20201214/train: 100%|██████████| 384/384 [02:13<00:00,  2.87it/s]\n",
      "Extract 1/best/20201214/valid: 100%|██████████| 96/96 [00:48<00:00,  2.00it/s]\n",
      "Extract 1/best/20201214/test: 100%|██████████| 120/120 [01:06<00:00,  1.80it/s]\n",
      "Extract 1/swabest/20201214/train: 100%|██████████| 384/384 [02:12<00:00,  2.90it/s]\n",
      "Extract 1/swabest/20201214/valid: 100%|██████████| 96/96 [00:46<00:00,  2.05it/s]\n",
      "Extract 1/swabest/20201214/test: 100%|██████████| 120/120 [01:04<00:00,  1.86it/s]\n",
      "Extract 1/swalast/20201214/train: 100%|██████████| 384/384 [02:10<00:00,  2.95it/s]\n",
      "Extract 1/swalast/20201214/valid: 100%|██████████| 96/96 [00:48<00:00,  1.98it/s]\n",
      "Extract 1/swalast/20201214/test: 100%|██████████| 120/120 [01:04<00:00,  1.85it/s]\n",
      "Extract 0/best/20210615/train: 100%|██████████| 384/384 [01:06<00:00,  5.78it/s]\n",
      "Extract 0/best/20210615/valid: 100%|██████████| 96/96 [00:23<00:00,  4.07it/s]\n",
      "Extract 0/best/20210615/test: 100%|██████████| 120/120 [00:30<00:00,  3.96it/s]\n",
      "Extract 0/swabest/20210615/train: 100%|██████████| 384/384 [01:13<00:00,  5.22it/s]\n",
      "Extract 0/swabest/20210615/valid: 100%|██████████| 96/96 [00:21<00:00,  4.39it/s]\n",
      "Extract 0/swabest/20210615/test: 100%|██████████| 120/120 [00:31<00:00,  3.82it/s]\n",
      "Extract 0/swalast/20210615/train: 100%|██████████| 384/384 [01:07<00:00,  5.73it/s]\n",
      "Extract 0/swalast/20210615/valid: 100%|██████████| 96/96 [00:21<00:00,  4.51it/s]\n",
      "Extract 0/swalast/20210615/test: 100%|██████████| 120/120 [00:33<00:00,  3.61it/s]\n",
      "Extract 1/best/20210615/train: 100%|██████████| 384/384 [02:13<00:00,  2.87it/s]\n",
      "Extract 1/best/20210615/valid: 100%|██████████| 96/96 [00:47<00:00,  2.03it/s]\n",
      "Extract 1/best/20210615/test: 100%|██████████| 120/120 [01:06<00:00,  1.81it/s]\n",
      "Extract 1/swabest/20210615/train: 100%|██████████| 384/384 [01:48<00:00,  3.55it/s]\n",
      "Extract 1/swabest/20210615/valid: 100%|██████████| 96/96 [00:35<00:00,  2.73it/s]\n",
      "Extract 1/swabest/20210615/test: 100%|██████████| 120/120 [00:49<00:00,  2.44it/s]\n",
      "Extract 1/swalast/20210615/train: 100%|██████████| 384/384 [01:33<00:00,  4.10it/s]\n",
      "Extract 1/swalast/20210615/valid: 100%|██████████| 96/96 [00:41<00:00,  2.32it/s]\n",
      "Extract 1/swalast/20210615/test: 100%|██████████| 120/120 [00:57<00:00,  2.08it/s]\n",
      "Extract 0/best/20211209/train: 100%|██████████| 384/384 [00:59<00:00,  6.43it/s]\n",
      "Extract 0/best/20211209/valid: 100%|██████████| 96/96 [00:19<00:00,  4.99it/s]\n",
      "Extract 0/best/20211209/test: 100%|██████████| 120/120 [00:28<00:00,  4.26it/s]\n",
      "Extract 0/swabest/20211209/train: 100%|██████████| 384/384 [00:54<00:00,  6.99it/s]\n",
      "Extract 0/swabest/20211209/valid: 100%|██████████| 96/96 [00:17<00:00,  5.49it/s]\n",
      "Extract 0/swabest/20211209/test: 100%|██████████| 120/120 [00:24<00:00,  4.83it/s]\n",
      "Extract 0/swalast/20211209/train: 100%|██████████| 384/384 [00:52<00:00,  7.26it/s]\n",
      "Extract 0/swalast/20211209/valid: 100%|██████████| 96/96 [00:18<00:00,  5.07it/s]\n",
      "Extract 0/swalast/20211209/test: 100%|██████████| 120/120 [00:23<00:00,  5.03it/s]\n",
      "Extract 1/best/20211209/train: 100%|██████████| 384/384 [01:43<00:00,  3.70it/s]\n",
      "Extract 1/best/20211209/valid: 100%|██████████| 96/96 [00:39<00:00,  2.41it/s]\n",
      "Extract 1/best/20211209/test: 100%|██████████| 120/120 [00:59<00:00,  2.02it/s]\n",
      "Extract 1/swabest/20211209/train: 100%|██████████| 384/384 [01:51<00:00,  3.43it/s]\n",
      "Extract 1/swabest/20211209/valid: 100%|██████████| 96/96 [00:40<00:00,  2.35it/s]\n",
      "Extract 1/swabest/20211209/test: 100%|██████████| 120/120 [00:55<00:00,  2.15it/s]\n",
      "Extract 1/swalast/20211209/train: 100%|██████████| 384/384 [01:50<00:00,  3.48it/s]\n",
      "Extract 1/swalast/20211209/valid: 100%|██████████| 96/96 [00:48<00:00,  1.99it/s]\n",
      "Extract 1/swalast/20211209/test: 100%|██████████| 120/120 [00:53<00:00,  2.24it/s]\n",
      "Extract 0/best/20220613/train: 100%|██████████| 384/384 [00:54<00:00,  7.04it/s]\n",
      "Extract 0/best/20220613/valid: 100%|██████████| 96/96 [00:21<00:00,  4.57it/s]\n",
      "Extract 0/best/20220613/test: 100%|██████████| 120/120 [00:27<00:00,  4.40it/s]\n",
      "Extract 0/swabest/20220613/train: 100%|██████████| 384/384 [00:54<00:00,  7.06it/s]\n",
      "Extract 0/swabest/20220613/valid: 100%|██████████| 96/96 [00:17<00:00,  5.48it/s]\n",
      "Extract 0/swabest/20220613/test: 100%|██████████| 120/120 [00:28<00:00,  4.21it/s]\n",
      "Extract 0/swalast/20220613/train: 100%|██████████| 384/384 [00:56<00:00,  6.82it/s]\n",
      "Extract 0/swalast/20220613/valid: 100%|██████████| 96/96 [00:17<00:00,  5.37it/s]\n",
      "Extract 0/swalast/20220613/test: 100%|██████████| 120/120 [00:25<00:00,  4.64it/s]\n",
      "Extract 1/best/20220613/train: 100%|██████████| 384/384 [01:34<00:00,  4.05it/s]\n",
      "Extract 1/best/20220613/valid: 100%|██████████| 96/96 [00:36<00:00,  2.60it/s]\n",
      "Extract 1/best/20220613/test: 100%|██████████| 120/120 [00:53<00:00,  2.24it/s]\n",
      "Extract 1/swabest/20220613/train: 100%|██████████| 384/384 [01:34<00:00,  4.05it/s]\n",
      "Extract 1/swabest/20220613/valid: 100%|██████████| 96/96 [00:35<00:00,  2.71it/s]\n",
      "Extract 1/swabest/20220613/test: 100%|██████████| 120/120 [00:50<00:00,  2.38it/s]\n",
      "Extract 1/swalast/20220613/train: 100%|██████████| 384/384 [01:39<00:00,  3.87it/s]\n",
      "Extract 1/swalast/20220613/valid: 100%|██████████| 96/96 [00:35<00:00,  2.68it/s]\n",
      "Extract 1/swalast/20220613/test: 100%|██████████| 120/120 [01:02<00:00,  1.92it/s]\n",
      "Extract 0/best/20221206/train: 100%|██████████| 384/384 [00:55<00:00,  6.87it/s]\n",
      "Extract 0/best/20221206/valid: 100%|██████████| 96/96 [00:16<00:00,  5.66it/s]\n",
      "Extract 0/best/20221206/test: 100%|██████████| 120/120 [00:24<00:00,  4.97it/s]\n",
      "Extract 0/swabest/20221206/train: 100%|██████████| 384/384 [00:49<00:00,  7.68it/s]\n",
      "Extract 0/swabest/20221206/valid: 100%|██████████| 96/96 [00:18<00:00,  5.28it/s]\n",
      "Extract 0/swabest/20221206/test: 100%|██████████| 120/120 [00:24<00:00,  4.88it/s]\n",
      "Extract 0/swalast/20221206/train: 100%|██████████| 384/384 [00:48<00:00,  7.88it/s]\n",
      "Extract 0/swalast/20221206/valid: 100%|██████████| 96/96 [00:17<00:00,  5.43it/s]\n",
      "Extract 0/swalast/20221206/test: 100%|██████████| 120/120 [00:23<00:00,  5.13it/s]\n",
      "Extract 1/best/20221206/train: 100%|██████████| 384/384 [01:41<00:00,  3.79it/s]\n",
      "Extract 1/best/20221206/valid: 100%|██████████| 96/96 [00:37<00:00,  2.54it/s]\n",
      "Extract 1/best/20221206/test: 100%|██████████| 120/120 [00:59<00:00,  2.02it/s]\n",
      "Extract 1/swabest/20221206/train: 100%|██████████| 384/384 [01:56<00:00,  3.31it/s]\n",
      "Extract 1/swabest/20221206/valid: 100%|██████████| 96/96 [00:38<00:00,  2.52it/s]\n",
      "Extract 1/swabest/20221206/test: 100%|██████████| 120/120 [00:55<00:00,  2.16it/s]\n",
      "Extract 1/swalast/20221206/train: 100%|██████████| 384/384 [01:39<00:00,  3.87it/s]\n",
      "Extract 1/swalast/20221206/valid: 100%|██████████| 96/96 [00:38<00:00,  2.50it/s]\n",
      "Extract 1/swalast/20221206/test: 100%|██████████| 120/120 [00:53<00:00,  2.24it/s]\n",
      "Extract 0/best/20230606/train: 100%|██████████| 384/384 [00:51<00:00,  7.39it/s]\n",
      "Extract 0/best/20230606/valid: 100%|██████████| 96/96 [00:20<00:00,  4.72it/s]\n",
      "Extract 0/best/20230606/test: 100%|██████████| 120/120 [00:24<00:00,  4.93it/s]\n",
      "Extract 0/swabest/20230606/train: 100%|██████████| 384/384 [00:53<00:00,  7.14it/s]\n",
      "Extract 0/swabest/20230606/valid: 100%|██████████| 96/96 [00:17<00:00,  5.53it/s]\n",
      "Extract 0/swabest/20230606/test: 100%|██████████| 120/120 [00:24<00:00,  4.82it/s]\n",
      "Extract 0/swalast/20230606/train: 100%|██████████| 384/384 [00:53<00:00,  7.22it/s]\n",
      "Extract 0/swalast/20230606/valid: 100%|██████████| 96/96 [00:18<00:00,  5.15it/s]\n",
      "Extract 0/swalast/20230606/test: 100%|██████████| 120/120 [00:24<00:00,  4.81it/s]\n",
      "Extract 1/best/20230606/train: 100%|██████████| 384/384 [01:59<00:00,  3.21it/s]\n",
      "Extract 1/best/20230606/valid: 100%|██████████| 96/96 [00:51<00:00,  1.88it/s]\n",
      "Extract 1/best/20230606/test: 100%|██████████| 120/120 [01:03<00:00,  1.88it/s]\n",
      "Extract 1/swabest/20230606/train: 100%|██████████| 384/384 [02:10<00:00,  2.95it/s]\n",
      "Extract 1/swabest/20230606/valid: 100%|██████████| 96/96 [00:51<00:00,  1.85it/s]\n",
      "Extract 1/swabest/20230606/test: 100%|██████████| 120/120 [01:01<00:00,  1.95it/s]\n",
      "Extract 1/swalast/20230606/train: 100%|██████████| 384/384 [01:43<00:00,  3.71it/s]\n",
      "Extract 1/swalast/20230606/valid: 100%|██████████| 96/96 [00:46<00:00,  2.06it/s]\n",
      "Extract 1/swalast/20230606/test: 100%|██████████| 120/120 [00:57<00:00,  2.09it/s]\n",
      "Extract 0/best/20231201/train: 100%|██████████| 384/384 [00:58<00:00,  6.57it/s]\n",
      "Extract 0/best/20231201/valid: 100%|██████████| 96/96 [00:19<00:00,  5.03it/s]\n"
     ]
    }
   ],
   "source": [
    "aa.extract_hidden('update' , deploy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20170704, 0, 'best'),\n",
       " (20170704, 0, 'swabest'),\n",
       " (20170704, 0, 'swalast'),\n",
       " (20170704, 1, 'best'),\n",
       " (20170704, 1, 'swabest'),\n",
       " (20170704, 1, 'swalast'),\n",
       " (20171226, 0, 'best'),\n",
       " (20171226, 0, 'swabest'),\n",
       " (20171226, 0, 'swalast'),\n",
       " (20171226, 1, 'best'),\n",
       " (20171226, 1, 'swabest'),\n",
       " (20171226, 1, 'swalast'),\n",
       " (20180627, 0, 'best'),\n",
       " (20180627, 0, 'swabest'),\n",
       " (20180627, 0, 'swalast'),\n",
       " (20180627, 1, 'best'),\n",
       " (20180627, 1, 'swabest'),\n",
       " (20180627, 1, 'swalast'),\n",
       " (20181220, 0, 'best'),\n",
       " (20181220, 0, 'swabest'),\n",
       " (20181220, 0, 'swalast'),\n",
       " (20181220, 1, 'best'),\n",
       " (20181220, 1, 'swabest'),\n",
       " (20181220, 1, 'swalast'),\n",
       " (20190624, 0, 'best'),\n",
       " (20190624, 0, 'swabest'),\n",
       " (20190624, 0, 'swalast'),\n",
       " (20190624, 1, 'best'),\n",
       " (20190624, 1, 'swabest'),\n",
       " (20190624, 1, 'swalast'),\n",
       " (20191217, 0, 'best'),\n",
       " (20191217, 0, 'swabest'),\n",
       " (20191217, 0, 'swalast'),\n",
       " (20191217, 1, 'best'),\n",
       " (20191217, 1, 'swabest'),\n",
       " (20191217, 1, 'swalast'),\n",
       " (20200617, 0, 'best'),\n",
       " (20200617, 0, 'swabest'),\n",
       " (20200617, 0, 'swalast'),\n",
       " (20200617, 1, 'best'),\n",
       " (20200617, 1, 'swabest'),\n",
       " (20200617, 1, 'swalast'),\n",
       " (20201214, 0, 'best'),\n",
       " (20201214, 0, 'swabest'),\n",
       " (20201214, 0, 'swalast'),\n",
       " (20201214, 1, 'best'),\n",
       " (20201214, 1, 'swabest'),\n",
       " (20201214, 1, 'swalast'),\n",
       " (20210615, 0, 'best'),\n",
       " (20210615, 0, 'swabest'),\n",
       " (20210615, 0, 'swalast'),\n",
       " (20210615, 1, 'best'),\n",
       " (20210615, 1, 'swabest'),\n",
       " (20210615, 1, 'swalast'),\n",
       " (20211209, 0, 'best'),\n",
       " (20211209, 0, 'swabest'),\n",
       " (20211209, 0, 'swalast'),\n",
       " (20211209, 1, 'best'),\n",
       " (20211209, 1, 'swabest'),\n",
       " (20211209, 1, 'swalast'),\n",
       " (20220613, 0, 'best'),\n",
       " (20220613, 0, 'swabest'),\n",
       " (20220613, 0, 'swalast'),\n",
       " (20220613, 1, 'best'),\n",
       " (20220613, 1, 'swabest'),\n",
       " (20220613, 1, 'swalast'),\n",
       " (20221206, 0, 'best'),\n",
       " (20221206, 0, 'swabest'),\n",
       " (20221206, 0, 'swalast'),\n",
       " (20221206, 1, 'best'),\n",
       " (20221206, 1, 'swabest'),\n",
       " (20221206, 1, 'swalast'),\n",
       " (20230606, 0, 'best'),\n",
       " (20230606, 0, 'swabest'),\n",
       " (20230606, 0, 'swalast'),\n",
       " (20230606, 1, 'best'),\n",
       " (20230606, 1, 'swabest'),\n",
       " (20230606, 1, 'swalast'),\n",
       " (20231201, 0, 'best'),\n",
       " (20231201, 0, 'swabest'),\n",
       " (20231201, 0, 'swalast'),\n",
       " (20231201, 1, 'best'),\n",
       " (20231201, 1, 'swabest'),\n",
       " (20231201, 1, 'swalast'),\n",
       " (20240604, 0, 'best'),\n",
       " (20240604, 0, 'swabest'),\n",
       " (20240604, 0, 'swalast'),\n",
       " (20240604, 1, 'best'),\n",
       " (20240604, 1, 'swabest'),\n",
       " (20240604, 1, 'swalast')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.update_model_iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20170103, 20170704, 20171226, 20180627, 20181220, 20190624,\n",
       "       20191217, 20200617, 20201214, 20210615, 20211209, 20220613,\n",
       "       20221206, 20230606, 20231201, 20240604])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.reg_model.model_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc , torch\n",
    "import numpy as np\n",
    "\n",
    "from abc import abstractmethod\n",
    "from numpy.random import permutation\n",
    "from torch import Tensor\n",
    "from torch.utils.data import BatchSampler\n",
    "from typing import Any , Iterator , Literal , Optional\n",
    "\n",
    "from src.nn_model.classes import BaseDataModule , BatchData\n",
    "from src.nn_model.util import BufferSpace , DataloaderStored , Device , LoaderWrapper , Storage , TrainConfig\n",
    "from src.data import DataBlockNorm , DataProcessor , ModuleData , BoosterData\n",
    "from src.basic import PATH , CONF\n",
    "from src.func import tensor_standardize_and_weight , match_values\n",
    "from src.nn_model.trainer.data_module import DataModule\n",
    "\n",
    "from src.func import index_intersect\n",
    "from src.data import DataBlock\n",
    "\n",
    "class AggregatorDataModule(DataModule):\n",
    "    '''for boosting such as algo.boost.lgbm, create booster'''\n",
    "    def train_dataloader(self) -> Iterator[BoosterData]: return self.loader_dict['train']\n",
    "    def val_dataloader(self) -> Iterator[BoosterData]:   return self.loader_dict['valid']\n",
    "    def test_dataloader(self) -> Iterator[BoosterData]:  return self.loader_dict['test']\n",
    "    def predict_dataloader(self) -> Iterator[BoosterData]: return self.loader_dict['test']\n",
    "        \n",
    "    def load_data(self):\n",
    "        with CONF.Silence():\n",
    "            self.datas = ModuleData.load([] , self.config['data.labels'], self.predict, self.config.precision)\n",
    "        self.labels_n = min(self.datas.y.shape[-1] , self.config.Model.max_num_output)\n",
    "        if self.predict:\n",
    "            self.model_date_list = self.datas.date[0]\n",
    "            self.test_full_dates = self.datas.date[1:]\n",
    "        else:\n",
    "            self.model_date_list = self.datas.date_within(self.config['beg_date'] , self.config['end_date'] , self.config['interval'])\n",
    "            self.test_full_dates = self.datas.date_within(self.config['beg_date'] , self.config['end_date'])[1:]\n",
    "\n",
    "        self.static_prenorm_method = {}\n",
    "        self.reset_dataloaders()\n",
    "        return self\n",
    "\n",
    "    def setup(self, stage : Literal['fit' , 'test' , 'predict'] , \n",
    "              param = {'seqlens' : {'day': 30 , '30m': 30 , 'style': 30}} , \n",
    "              model_date = -1) -> None:\n",
    "        if self.predict: stage = 'predict'\n",
    "\n",
    "        if self.loader_param == (stage , model_date): return\n",
    "        self.loader_param = stage , model_date\n",
    "\n",
    "        assert stage in ['fit' , 'test' , 'predict'] and model_date > 0 , (stage , model_date)\n",
    "\n",
    "        self.stage = stage\n",
    "        self.seqs = {'hidden':1}\n",
    "        self.seq0 = self.seqx = self.seqy = 1\n",
    "\n",
    "        hidden_dates : list[np.ndarray] = []\n",
    "        hidden_df : pd.DataFrame | Any = None\n",
    "        ds_list = ['train' , 'valid'] if stage == 'fit' else ['test' , 'predict']\n",
    "        for hidden_key in self.config['data.hidden']:\n",
    "            model_name , model_num , model_type = hidden_key.split('.')\n",
    "            hidden_path = os.path.join(PATH.hidden , model_name , f'hidden.{model_num}.{model_type}.{model_date}.feather')\n",
    "            df = pd.read_feather(hidden_path)\n",
    "            df = df[df['dataset'].isin(ds_list)].drop(columns='dataset').set_index(['secid','date'])\n",
    "            hidden_dates.append(df.index.get_level_values('date').unique().to_numpy())\n",
    "            df.columns = [f'{hidden_key}.{col}' for col in df.columns]\n",
    "            hidden_df = df if hidden_df is None else hidden_df.join(df , how='outer')\n",
    "\n",
    "        stage_date = index_intersect(hidden_dates)[0]\n",
    "        if self.stage != 'fit':\n",
    "            stage_date = index_intersect([stage_date , self.test_full_dates])[0]\n",
    "        self.day_len = len(stage_date)\n",
    "        self.step_len = len(stage_date)\n",
    "        self.date_idx , self.step_idx = torch.arange(self.day_len) , torch.arange(self.day_len)\n",
    "\n",
    "        y_aligned = self.datas.y.align_date(stage_date , inplace=False)\n",
    "        self.y_secid , self.y_date = y_aligned.secid , y_aligned.date\n",
    "\n",
    "        if stage == 'fit':\n",
    "            ...\n",
    "        elif stage in ['predict' , 'test']:\n",
    "            self.model_test_dates = stage_date\n",
    "            self.early_test_dates = stage_date[:0]\n",
    "        else:\n",
    "            raise KeyError(stage)\n",
    "\n",
    "        x = {'hidden':DataBlock.from_dataframe(hidden_df).align_secid_date(self.y_secid , self.y_date).as_tensor().values}\n",
    "        y = Tensor(y_aligned.values).squeeze(2)[...,:self.labels_n]\n",
    "        self.hidden_cols = hidden_df.columns\n",
    "        self.y , _ = self.standardize_y(y , None , None , no_weight = True)\n",
    "\n",
    "        if stage != 'fit':\n",
    "            w , valid = None , None\n",
    "            y , _ = self.standardize_y(self.y , None , self.step_idx)\n",
    "        else:\n",
    "            valid = self.full_valid_sample(x , self.y , self.step_idx)\n",
    "            y , w = self.standardize_y(self.y , valid , self.step_idx)\n",
    "\n",
    "        self.y[:,self.step_idx] = y[:]\n",
    "        self.static_dataloader(x , y , w , valid)\n",
    "\n",
    "        gc.collect() \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def static_dataloader(self , x : dict[str,Tensor] , y : Tensor , w = None , valid = None) -> None:\n",
    "        '''update loader_dict , save batch_data to f'{PATH.model}/{model_name}/{set_name}_batch_data' and later load them'''\n",
    "        index0, index1 = torch.arange(len(y)) , self.step_idx\n",
    "        sample_index = self.split_sample(self.stage , index0 , index1 , self.config.train_ratio)\n",
    "        self.storage.del_group(self.stage)\n",
    "        assert len(x) == 1 , len(x)\n",
    "        x0 = x['hidden']\n",
    "        for set_key , set_samples in sample_index.items():\n",
    "            if set_key in ['train' , 'valid']:\n",
    "                bb_x , bb_y , bb_d = [] , [] , []\n",
    "                for bnum , b_i in enumerate(set_samples):\n",
    "                    i0 , i1 , yindex1 = b_i[:,0] , b_i[:,1] , match_values(b_i[:,1] , index1)\n",
    "\n",
    "                    bb_x.append(x0[i0 , i1].reshape(len(self.y_secid),1,-1))\n",
    "                    bb_y.append(y[i0 , yindex1])\n",
    "                    bb_d.append(self.y_date[index1[yindex1[0]]])\n",
    "                bb_x = torch.concat(bb_x , dim = 1)\n",
    "                bb_y = torch.concat(bb_y , dim = 1)\n",
    "                bb_d = np.array(bb_d)\n",
    "                bnum = 0\n",
    "                batch_files = [f'{PATH.batch}/{set_key}.{bnum}.pt']\n",
    "                self.storage.save(BoosterData(bb_x , bb_y , self.y_secid , bb_d , self.hidden_cols) , batch_files[bnum] , group = self.stage)\n",
    "            elif set_key == 'test':\n",
    "                batch_files = [f'{PATH.batch}/{set_key}.{bnum}.pt' for bnum in range(len(set_samples))]\n",
    "                for bnum , b_i in enumerate(set_samples):\n",
    "                    i0 , i1 , yindex1 = b_i[:,0] , b_i[:,1] , match_values(b_i[:,1] , index1)\n",
    "\n",
    "                    b_x = x0[i0,i1].reshape(len(self.y_secid),1,-1)\n",
    "                    b_y = y[i0 , yindex1] # [n_stock x num_output]\n",
    "                    dates = np.array([self.y_date[index1[yindex1[0]]]])\n",
    "                    self.storage.save(BoosterData(b_x , b_y , self.y_secid , dates , self.hidden_cols) , batch_files[bnum] , group = self.stage)\n",
    "            else:\n",
    "                raise KeyError(set_key)\n",
    "            self.loader_dict[set_key] = DataloaderStored(self.storage , batch_files)\n",
    "\n",
    "    @staticmethod\n",
    "    def split_sample(stage , index0 : Tensor , index1 : Tensor , train_ratio   : float = 0.8) -> dict[str,list]:\n",
    "        l0 , l1 = len(index0) , len(index1)\n",
    "        pos = torch.stack([index0.repeat_interleave(l1) , index1.repeat(l0)] , -1).reshape(l0,l1,2)\n",
    "\n",
    "        def sequential_sampling(beg , end , posit = pos): return [posit[:,j] for j in range(beg , end)]\n",
    "        \n",
    "        sample_index = {}\n",
    "        if stage == 'fit':\n",
    "            # must be sequential\n",
    "            sep = int(l1 * train_ratio)\n",
    "            sample_index['train'] = sequential_sampling(0 , sep)\n",
    "            sample_index['valid'] = sequential_sampling(sep , l1)\n",
    "        else:\n",
    "            # test dataloader should have the same length as dates, so no filtering of val[:,j].sum() > 0\n",
    "            sample_index['test'] = sequential_sampling(0 , l1)\n",
    "        return sample_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.AggregatorDataModule at 0x1abc3770c10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self = AggregatorDataModule(predict=False)\n",
    "self.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.setup('fit',model_date=20170103)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from src.func import index_intersect\n",
    "from src.data import DataBlock\n",
    "\n",
    "stage : Literal['fit' , 'test' , 'predict'] = 'fit'  \n",
    "param = {'seqlens' : {'day': 30 , '30m': 30 , 'style': 30}}\n",
    "model_date = 20170103\n",
    "\n",
    "if self.predict: stage = 'predict'\n",
    "\n",
    "# if self.loader_param == (stage , model_date): return\n",
    "self.loader_param = stage , model_date\n",
    "\n",
    "assert stage in ['fit' , 'test' , 'predict'] and model_date > 0 , (stage , model_date)\n",
    "\n",
    "self.stage = stage\n",
    "x_keys , y_keys = ['hidden'] , []\n",
    "self.seqs = {'hidden':1}\n",
    "self.seq0 = self.seqx = self.seqy = 1\n",
    "\n",
    "hidden_dates : list[np.ndarray] = []\n",
    "hidden_df : pd.DataFrame | Any = None\n",
    "ds_list = ['train' , 'valid'] if stage == 'fit' else ['test' , 'predict']\n",
    "for hidden_key in self.config['data.hidden']:\n",
    "    model_name , model_num , model_type = hidden_key.split('.')\n",
    "    hidden_path = os.path.join(PATH.hidden , model_name , f'hidden.{model_num}.{model_type}.{model_date}.feather')\n",
    "    df = pd.read_feather(hidden_path)\n",
    "    df = df[df['dataset'].isin(ds_list)].drop(columns='dataset').set_index(['secid','date'])\n",
    "    hidden_dates.append(df.index.get_level_values('date').unique().to_numpy())\n",
    "    df.columns = [f'{hidden_key}.{col}' for col in df.columns]\n",
    "    hidden_df = df if hidden_df is None else hidden_df.join(df , how='outer')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stage_date = index_intersect(hidden_dates)[0]\n",
    "\n",
    "self.day_len = len(stage_date)\n",
    "self.step_len = len(stage_date)\n",
    "self.date_idx , self.step_idx = torch.arange(self.day_len) , torch.arange(self.day_len)\n",
    "\n",
    "y_aligned = self.datas.y.align_date(stage_date , inplace=False)\n",
    "self.y_secid , self.y_date = y_aligned.secid , y_aligned.date\n",
    "\n",
    "if stage == 'fit':\n",
    "    ...\n",
    "elif stage in ['predict' , 'test']:\n",
    "    self.model_test_dates = stage_date\n",
    "    self.early_test_dates = stage_date[:0]\n",
    "else:\n",
    "    raise KeyError(stage)\n",
    "\n",
    "x = {'hidden':DataBlock.from_dataframe(hidden_df).align_secid_date(self.y_secid , self.y_date).as_tensor().values}\n",
    "y = Tensor(y_aligned.values).squeeze(2)[...,:self.labels_n]\n",
    "\n",
    "self.y , _ = self.standardize_y(y , None , None , no_weight = True)\n",
    "\n",
    "if stage != 'fit':\n",
    "    w , valid = None , None\n",
    "    y , _ = self.standardize_y(self.y , None , self.step_idx)\n",
    "else:\n",
    "    valid = self.full_valid_sample(x , self.y , self.step_idx)\n",
    "    y , w = self.standardize_y(self.y , valid , self.step_idx)\n",
    "\n",
    "self.y[:,self.step_idx] = y[:]\n",
    "self.static_dataloader(x , y , w , valid)\n",
    "\n",
    "gc.collect() \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from src.func import index_intersect\n",
    "from src.data import DataBlock\n",
    "\n",
    "stage : Literal['fit' , 'test' , 'predict'] = 'fit'  \n",
    "param = {'seqlens' : {'day': 30 , '30m': 30 , 'style': 30}}\n",
    "model_date = 20170103\n",
    "\n",
    "if self.predict: stage = 'predict'\n",
    "\n",
    "# if self.loader_param == (stage , model_date): return\n",
    "self.loader_param = stage , model_date\n",
    "\n",
    "assert stage in ['fit' , 'test' , 'predict'] and model_date > 0 , (stage , model_date)\n",
    "\n",
    "self.stage = stage\n",
    "x_keys = ['hidden']\n",
    "y_keys = []\n",
    "self.seqs = {k:1 for k in y_keys + x_keys}\n",
    "self.seq0 = self.seqx = self.seqy = 1\n",
    "\n",
    "hidden_dates : list[np.ndarray] = []\n",
    "hidden_blocks : list[DataBlock] = []\n",
    "ds_list = ['train' , 'valid'] if stage == 'fit' else ['test' , 'predict']\n",
    "for hidden_key in self.config['data.hidden']:\n",
    "    model_name , model_num , model_type = hidden_key.split('.')\n",
    "    hidden_path = os.path.join(PATH.hidden , model_name , f'hidden.{model_num}.{model_type}.{model_date}.feather')\n",
    "    df = pd.read_feather(hidden_path)\n",
    "    df = df[df['dataset'].isin(ds_list)].drop(columns='dataset').set_index(['secid','date'])\n",
    "    hidden_dates.append(df.index.get_level_values('date').unique().to_numpy())\n",
    "    df.columns = [f'{hidden_key}.{col}' for col in df.columns]\n",
    "    hidden_blocks.append(DataBlock.from_dataframe(df).align_secid_date(self.datas.y.secid , self.datas.y.date))\n",
    "\n",
    "db = DataBlock.concat_feature(hidden_blocks)\n",
    "self.datas.x['hidden'] = db.as_tensor()\n",
    "\n",
    "stage_date = index_intersect([*hidden_dates , self.datas.y.date])[0]\n",
    "d0 = np.where(self.datas.y.date == stage_date.min())[0][0]\n",
    "d1 = np.where(self.datas.y.date == stage_date.max())[0][0] + 1\n",
    "\n",
    "self.day_len = d1 - d0\n",
    "self.step_len = len(stage_date)\n",
    "self.date_idx = torch.tensor(match_values(stage_date , self.datas.y.date))\n",
    "self.step_idx = self.date_idx - d0\n",
    "\n",
    "self.y_secid , self.y_date = self.datas.y.secid , self.datas.y.date[d0:d1]\n",
    "\n",
    "if stage == 'fit':\n",
    "    ...\n",
    "elif stage in ['predict' , 'test']:\n",
    "    self.model_test_dates = stage_date\n",
    "    self.early_test_dates = stage_date[:0]\n",
    "else:\n",
    "    raise KeyError(stage)\n",
    "\n",
    "x = {k:Tensor(v.values)[:,d0:d1] for k,v in self.datas.x.items()}\n",
    "y = Tensor(self.datas.y.values)[:,d0:d1].squeeze(2)[...,:self.labels_n]\n",
    "\n",
    "\n",
    "self.y , _ = self.standardize_y(y , None , None , no_weight = True)\n",
    "\n",
    "if stage != 'fit':\n",
    "    w , valid = None , None\n",
    "    y , _ = self.standardize_y(self.y , None , self.step_idx)\n",
    "else:\n",
    "    valid = self.full_valid_sample(x , self.y , self.step_idx)\n",
    "    y , w = self.standardize_y(self.y , valid , self.step_idx)\n",
    "\n",
    "self.y[:,self.step_idx] = y[:]\n",
    "self.static_dataloader(x , y , w , valid)\n",
    "\n",
    "gc.collect() \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
