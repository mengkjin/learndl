{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch , h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, shutil , gc , time , argparse\n",
    "from scripts.data_utils.ModelData import save_block_data\n",
    "from scripts.functional.func import *\n",
    "from scripts.util.environ import get_logger , get_config , DIR_data\n",
    "\n",
    "NBARS      = {'day' : 1 , '15m' : 16 ,}\n",
    "DATATYPE   = get_config('data_type')['DATATYPE']\n",
    "\n",
    "logger = get_logger()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='manual to this script')\n",
    "    parser.add_argument(\"--confirm\", type=str, default='')\n",
    "    if parser.parse_args().confirm == 'no':\n",
    "        pass\n",
    "    else:\n",
    "        a = input('You want to update data? print \"yes\" to confirm!')\n",
    "        if a == 'yes':\n",
    "            t1 = time.time()\n",
    "            logger.critical('Data loading start!')\n",
    "\n",
    "            #update_trading_data()\n",
    "            #prepare_model_data()\n",
    "            #cal_norm_param()\n",
    "            \n",
    "            t2 = time.time()\n",
    "            logger.critical('Data loading Finished! Cost {:.2f} Seconds'.format(t2-t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "secid = date = ndim = None\n",
    "secid , date , ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch , h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, shutil , gc , time , argparse\n",
    "from scripts.data_utils.ModelData import (\n",
    "     DataBlock)\n",
    "from scripts.data_utils.DataTank import DataTank\n",
    "from scripts.functional.func import *\n",
    "from scripts.util.environ import get_logger , get_config , DIR_data\n",
    "from scripts.util.basic import timer\n",
    "\n",
    "start_dt , end_dt = None , None # 20150101 , 20150331 # None , None\n",
    "process_param = {\n",
    "    'y' : {\n",
    "        'dtank_h5' : 'DB_labels.h5' ,\n",
    "        'dtank_file' : ['10days/lag1' , '20days/lag1'] ,\n",
    "        'start_dt' : start_dt , 'end_dt' : end_dt\n",
    "    },\n",
    "    'trade_day' : {\n",
    "        'dtank_h5' : 'DB_trade_day.h5' ,\n",
    "        'dtank_file' : 'day/trade' ,\n",
    "        'feature' : ['adjfactor', 'close', 'high', 'low', 'open', 'volume', 'vwap'] ,\n",
    "        'process_method' : 'adj_order' ,\n",
    "        'start_dt' : start_dt , 'end_dt' : end_dt\n",
    "    },\n",
    "    'trade_15m' : {\n",
    "        'dtank_h5' : 'DB_trade_Xmin.h5' ,\n",
    "        'dtank_file' : '15min/trade' ,\n",
    "        'feature' : ['minute' , 'close', 'high', 'low', 'open', 'volume', 'vwap'] ,\n",
    "        'process_method' : 'order' ,\n",
    "        'start_dt' : start_dt , 'end_dt' : end_dt\n",
    "    },\n",
    "    # 'gp' : {}\n",
    "}\n",
    "x_trade_norm_dict = dict()\n",
    "DIR_block  = f'{DIR_data}/block_data'\n",
    "path_xnorm = f'{DIR_block}/X_normdict.pt'\n",
    "logger = get_logger()\n",
    "\n",
    "def block_default_path(key):\n",
    "    if key.lower() == 'y':\n",
    "        return f'{DIR_block}/Y.npz'\n",
    "    else:\n",
    "        return f'{DIR_block}/X_{key}.npz'\n",
    "    \n",
    "def block_process(data_block , process_method = 'default' , feature = [] , **kwargs):\n",
    "    np.seterr(invalid='ignore')\n",
    "    assert isinstance(data_block , DataBlock) , type(data_block)\n",
    "    if process_method == 'default':\n",
    "        process_method = 'order'\n",
    "    if 'adj' in process_method and 'adjfactor' in data_block.feature:\n",
    "        price_feat = np.intersect1d(['close', 'high', 'low', 'open', 'vwap'] , data_block.feature)\n",
    "        ifeat = np.where(np.isin(data_block.feature,price_feat))[0]\n",
    "        iadj  = np.where(data_block.feature == 'adjfactor')[0]\n",
    "        data_block.values[...,ifeat] = np.multiply(data_block.values[...,ifeat],data_block.values[...,iadj])\n",
    "        ifeat  = np.where(data_block.feature != 'adjfactor')[0]\n",
    "        data_block.update(values = data_block.values[...,ifeat] , feature = data_block.feature[ifeat])\n",
    "    if 'order' in process_method:\n",
    "        raw_order = feature\n",
    "        raw_order = [o for o in raw_order if o in data_block.feature]\n",
    "        raw_order += [o for o in data_block.feature if o not in raw_order]\n",
    "        ifeat = np.array([raw_order.index(f) for f in data_block.feature])\n",
    "        data_block.update(values = data_block.values[...,ifeat] , feature = data_block.feature[ifeat])\n",
    "    np.seterr(invalid='warn')\n",
    "    return data_block\n",
    "\n",
    "def block_hist_norm(data_block , key , norm_dict = None , \n",
    "                    start_dt = None , end_dt = 20161231 , \n",
    "                    step_day = 5 , eps = 1e-4 , **kwargs):\n",
    "    if not key.startswith(('trade','day','15m','min','30m','60m')): return norm_dict\n",
    "    maxday = {\n",
    "        'trade_day' : 120 ,\n",
    "        'trade_min' : 20 ,\n",
    "        'others'    : 60 ,\n",
    "    }\n",
    "    maxday = maxday[key] if key in maxday.keys() else maxday['others']\n",
    "\n",
    "    date_slice = np.repeat(True , len(data_block.date))\n",
    "    if start_dt is not None: date_slice[data_block.date < start_dt] = False\n",
    "    if end_dt   is not None: date_slice[data_block.date > end_dt]   = False\n",
    "\n",
    "    x = data_block.values[:,date_slice]\n",
    "    secid = data_block.secid\n",
    "    date  = data_block.date[date_slice]\n",
    "    feat  = data_block.feature\n",
    "    inday = x.shape[2]\n",
    "\n",
    "    len_step = len(date) // step_day\n",
    "    len_bars = maxday * inday\n",
    "    x = np.concatenate([np.full((len(secid),maxday,*x.shape[2:]),np.nan),x],axis=1)\n",
    "\n",
    "    x = torch.tensor(x)\n",
    "    x_endpoint = x.shape[1]-1 + step_day * np.arange(-len_step + 1 , 1)\n",
    "    avg_x = torch.zeros(len_bars , len(feat))\n",
    "    std_x = torch.zeros(len_bars , len(feat))\n",
    "\n",
    "    x_div = torch.ones(len(secid) , len_step , 1 , len(feat))\n",
    "    x_div.copy_(x[:,x_endpoint,-1:])\n",
    "\n",
    "    nan_sample = (x_div < 0).sum(dim = (-2,-1)) > 0\n",
    "    nan_set = [x[:,x_endpoint-i].reshape(len(secid),len_step ,-1).isnan().any(dim = -1) for i in range(maxday)]\n",
    "    for i_set in nan_set: nan_sample += i_set\n",
    "\n",
    "    for i in range(maxday):\n",
    "        vijs = ((x[:,x_endpoint - maxday+1 + i] + eps) / (x_div + eps))[nan_sample == 0]\n",
    "        avg_x[i*inday:(i+1)*inday] = vijs.mean(dim = 0)\n",
    "        std_x[i*inday:(i+1)*inday] = vijs.std(dim = 0)\n",
    "\n",
    "    # assert avg_x.isnan().sum() + std_x.isnan().sum() == 0 , ((nan_sample == 0).sum())\n",
    "\n",
    "    if norm_dict is None:\n",
    "        return {key : {'avg' : avg_x , 'std' : std_x}}\n",
    "    else:\n",
    "        norm_dict.update({key : {'avg' : avg_x , 'std' : std_x}})\n",
    "        return norm_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[45m23-12-21 23:07:19|MOD:1789959765  |\u001b[0m: \u001b[1m\u001b[35mPreparing y data...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y reading 10days/lag1 Data1D's ...... cost 0.18 secs\n",
      "y reading 20days/lag1 Data1D's ...... cost 0.20 secs\n",
      "y merging blocks ...... cost 0.02 secs\n",
      "y process blocks ...... cost 0.00 secs\n",
      "y savenpz blocks ..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[45m23-12-21 23:07:19|MOD:1789959765  |\u001b[0m: \u001b[1m\u001b[35mPreparing trade_day data...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... cost 0.21 secs\n",
      "norming y blocks ...... cost 0.00 secs\n",
      "trade_day reading day/trade Data1D's ...... cost 0.35 secs\n",
      "trade_day merging blocks ...... cost 0.00 secs\n",
      "trade_day process blocks ...... cost 0.02 secs\n",
      "trade_day savenpz blocks ..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[45m23-12-21 23:07:20|MOD:1789959765  |\u001b[0m: \u001b[1m\u001b[35mPreparing trade_15m data...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... cost 0.30 secs\n",
      "norming trade_day blocks ...... cost 0.14 secs\n",
      "trade_15m reading 15min/trade Data1D's ...... cost 10.52 secs\n",
      "trade_15m merging blocks ...... cost 0.00 secs\n",
      "trade_15m process blocks ...... cost 0.08 secs\n",
      "trade_15m savenpz blocks ...... cost 1.47 secs\n",
      "norming trade_15m blocks ...... cost 1.20 secs\n"
     ]
    }
   ],
   "source": [
    "for key ,param in process_param.items():\n",
    "    logger.error(f'Preparing {key} data...')\n",
    "\n",
    "    if isinstance(param['dtank_file'] , str): param['dtank_file'] = [param['dtank_file']]\n",
    "    blocks = []\n",
    "    dtank  = DataTank('/'.join([DIR_data,param['dtank_h5']]) , open = True , mode = 'r')\n",
    "    for f in param['dtank_file']:\n",
    "        with timer(f'{key} reading {f} Data1D\\'s') as t:\n",
    "            blocks.append(DataBlock().from_dtank(dtank,f,**param))\n",
    "    dtank.close()\n",
    "    with timer(f'{key} merging blocks') as t:\n",
    "        new_block = DataBlock().merge_others(blocks)\n",
    "    with timer(f'{key} process blocks') as t:\n",
    "        new_block = block_process(new_block , **param)\n",
    "    with timer(f'{key} savenpz blocks') as t:\n",
    "        new_block.save_npz(block_default_path(key))\n",
    "    with timer(f'norming {key} blocks') as t:\n",
    "        block_hist_norm(new_block , key , x_trade_norm_dict , **param)\n",
    "torch.save(x_trade_norm_dict , path_xnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[45m23-12-21 23:06:03|MOD:3381047532  |\u001b[0m: \u001b[1m\u001b[35mPreparing trade_day data...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade_day reading day/trade Data1D's ... , cost 0.61 secs\n",
      "trade_day merging blocks ... , cost 0.00 secs\n",
      "trade_day process blocks ... , cost 0.01 secs\n",
      "trade_day savenpz blocks ... , cost 0.29 secs\n",
      "norming trade_day blocks ... , cost 0.13 secs\n"
     ]
    }
   ],
   "source": [
    "key = 'trade_day'\n",
    "param = process_param[key]\n",
    "logger.error(f'Preparing {key} data...')\n",
    "\n",
    "if isinstance(param['dtank_file'] , str): param['dtank_file'] = [param['dtank_file']]\n",
    "blocks = []\n",
    "dtank  = DataTank('/'.join([DIR_data,param['dtank_h5']]) , open = True , mode = 'r')\n",
    "for f in param['dtank_file']:\n",
    "    with timer(f'{key} reading {f} Data1D\\'s') as t:\n",
    "        blocks.append(DataBlock().from_dtank(dtank,f,**param))\n",
    "dtank.close()\n",
    "with timer(f'{key} merging blocks') as t:\n",
    "    new_block = DataBlock().merge_others(blocks)\n",
    "with timer(f'{key} process blocks') as t:\n",
    "    new_block = block_process(new_block , **param)\n",
    "with timer(f'{key} savenpz blocks') as t:\n",
    "    new_block.save_npz(block_default_path(key))\n",
    "with timer(f'norming {key} blocks') as t:\n",
    "    block_hist_norm(new_block , key , x_trade_norm_dict , **param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[45m23-12-21 23:06:13|MOD:4182274638  |\u001b[0m: \u001b[1m\u001b[35mPreparing trade_15m data...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade_15m reading 15min/trade Data1D's ... , cost 10.52 secs\n",
      "trade_15m merging blocks ... , cost 0.00 secs\n",
      "trade_15m process blocks ... , cost 0.10 secs\n",
      "trade_15m savenpz blocks ... , cost 1.47 secs\n",
      "norming trade_15m blocks ... , cost 1.22 secs\n"
     ]
    }
   ],
   "source": [
    "key = 'trade_15m'\n",
    "param = process_param[key]\n",
    "logger.error(f'Preparing {key} data...')\n",
    "\n",
    "if isinstance(param['dtank_file'] , str): param['dtank_file'] = [param['dtank_file']]\n",
    "blocks = []\n",
    "dtank  = DataTank('/'.join([DIR_data,param['dtank_h5']]) , open = True , mode = 'r')\n",
    "for f in param['dtank_file']:\n",
    "    with timer(f'{key} reading {f} Data1D\\'s') as t:\n",
    "        blocks.append(DataBlock().from_dtank(dtank,f,**param))\n",
    "dtank.close()\n",
    "with timer(f'{key} merging blocks') as t:\n",
    "    new_block = DataBlock().merge_others(blocks)\n",
    "with timer(f'{key} process blocks') as t:\n",
    "    new_block = block_process(new_block , **param)\n",
    "with timer(f'{key} savenpz blocks') as t:\n",
    "    new_block.save_npz(block_default_path(key))\n",
    "with timer(f'norming {key} blocks') as t:\n",
    "    block_hist_norm(new_block , key , x_trade_norm_dict , **param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n",
      "torch.Size([2679, 11])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "tensor(0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/mengkjin/Workspace/learndl/test_gen_data.ipynb 单元格 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mengkjin/Workspace/learndl/test_gen_data.ipynb#Y145sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m         norm_dict\u001b[39m.\u001b[39mupdate({key : {\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m : avg_x , \u001b[39m'\u001b[39m\u001b[39mstd\u001b[39m\u001b[39m'\u001b[39m : std_x}})\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mengkjin/Workspace/learndl/test_gen_data.ipynb#Y145sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m norm_dict\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mengkjin/Workspace/learndl/test_gen_data.ipynb#Y145sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m block_hist_norm(new_block , key , xnorm_dict , \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparam)\n",
      "\u001b[1;32m/home/mengkjin/Workspace/learndl/test_gen_data.ipynb 单元格 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mengkjin/Workspace/learndl/test_gen_data.ipynb#Y145sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     avg_x[i\u001b[39m*\u001b[39minday:(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39minday] \u001b[39m=\u001b[39m vijs\u001b[39m.\u001b[39mmean(dim \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mengkjin/Workspace/learndl/test_gen_data.ipynb#Y145sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     std_x[i\u001b[39m*\u001b[39minday:(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39minday] \u001b[39m=\u001b[39m vijs\u001b[39m.\u001b[39mstd(dim \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mengkjin/Workspace/learndl/test_gen_data.ipynb#Y145sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39massert\u001b[39;00m avg_x\u001b[39m.\u001b[39misnan()\u001b[39m.\u001b[39msum() \u001b[39m+\u001b[39m std_x\u001b[39m.\u001b[39misnan()\u001b[39m.\u001b[39msum() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m , ((nan_sample \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39msum())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mengkjin/Workspace/learndl/test_gen_data.ipynb#Y145sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mif\u001b[39;00m norm_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mengkjin/Workspace/learndl/test_gen_data.ipynb#Y145sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {key : {\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m : avg_x , \u001b[39m'\u001b[39m\u001b[39mstd\u001b[39m\u001b[39m'\u001b[39m : std_x}}\n",
      "\u001b[0;31mAssertionError\u001b[0m: tensor(0)"
     ]
    }
   ],
   "source": [
    "def block_hist_norm(data_block , key , norm_dict = None , \n",
    "                    start_dt = None , end_dt = 20161231 , \n",
    "                    step_day = 5 , eps = 1e-4 , **kwargs):\n",
    "    if not key.startswith(('trade','day','15m','min','30m','60m')): return norm_dict\n",
    "    maxday = {\n",
    "        'trade_day' : 120 ,\n",
    "        'trade_min' : 20 ,\n",
    "        'others'    : 60 ,\n",
    "    }\n",
    "    maxday = maxday[key] if key in maxday.keys() else maxday['others']\n",
    "\n",
    "    date_slice = np.repeat(True , len(data_block.date))\n",
    "    if start_dt is not None: date_slice[data_block.date < start_dt] = False\n",
    "    if end_dt   is not None: date_slice[data_block.date > end_dt]   = False\n",
    "\n",
    "    x = data_block.values[:,date_slice]\n",
    "    secid = data_block.secid\n",
    "    date  = data_block.date[date_slice]\n",
    "    feat  = data_block.feature\n",
    "    inday = x.shape[2]\n",
    "\n",
    "    len_step = len(date) // step_day\n",
    "    len_bars = maxday * inday\n",
    "    x = np.concatenate([np.full((len(secid),maxday,*x.shape[2:]),np.nan),x],axis=1)\n",
    "\n",
    "    x = torch.tensor(x)\n",
    "    x_endpoint = x.shape[1]-1 + step_day * np.arange(-len_step + 1 , 1)\n",
    "    avg_x = torch.zeros(len_bars , len(feat))\n",
    "    std_x = torch.zeros(len_bars , len(feat))\n",
    "\n",
    "    x_div = torch.ones(len(secid) , len_step , 1 , len(feat))\n",
    "    x_div.copy_(x[:,x_endpoint,-1:])\n",
    "\n",
    "    nan_sample = (x_div < 0).sum(dim = (-2,-1)) > 0\n",
    "    nan_set = [x[:,x_endpoint-i].reshape(len(secid),len_step ,-1).isnan().any(dim = -1) for i in range(maxday)]\n",
    "    for i_set in nan_set: nan_sample += i_set\n",
    "\n",
    "    for i in range(maxday):\n",
    "        vijs = ((x[:,x_endpoint - maxday+1 + i] + eps) / (x_div + eps))[nan_sample == 0]\n",
    "        avg_x[i*inday:(i+1)*inday] = vijs.mean(dim = 0)\n",
    "        std_x[i*inday:(i+1)*inday] = vijs.std(dim = 0)\n",
    "\n",
    "    assert avg_x.isnan().sum() + std_x.isnan().sum() == 0 , ((nan_sample == 0).sum())\n",
    "\n",
    "    if norm_dict is None:\n",
    "        return {key : {'avg' : avg_x , 'std' : std_x}}\n",
    "    else:\n",
    "        norm_dict.update({key : {'avg' : avg_x , 'std' : std_x}})\n",
    "        return norm_dict\n",
    "    \n",
    "block_hist_norm(new_block , key , xnorm_dict , **param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DataUpdater1.h5'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtank = DataTank('/'.join([DIR_data,param['dtank_h5']]) , open = True , mode = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtank = DataTank('/'.join([DIR_data,param['dtank_h5']]) , open = True , mode = 'r')\n",
    "file = ['15min' , 'trade']\n",
    "g = dtank.get_object(file)\n",
    "g.keys()\n",
    "k = dtank.read_data1D(['15min' , 'trade' , '20100104'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'scripts.data_utils.DataTank.Data1D'>\n",
       "secid len (27120): array([     1,      1,      1, ..., 601999, 601999, 601999], dtype=int32)\n",
       "feature len (8): array(['minute', 'open', 'high', 'low', 'close', 'amount', 'volume',\n",
       "       'vwap'], dtype='<U6')\n",
       "values shape (27120, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'isnan'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/mengkjin/Workspace/learndl/test_gen_data.ipynb 单元格 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/mengkjin/Workspace/learndl/test_gen_data.ipynb#Y136sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m k\u001b[39m.\u001b[39;49mto_kline()\n",
      "File \u001b[0;32m~/Workspace/learndl/scripts/data_utils/DataTank.py:132\u001b[0m, in \u001b[0;36mData1D.to_kline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 new_values[i_sec,mm] \u001b[39m=\u001b[39m values[mpos , \u001b[39m1\u001b[39m:]\n\u001b[1;32m    131\u001b[0m         values \u001b[39m=\u001b[39m new_values\n\u001b[0;32m--> 132\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_kline_fillna(values , feature)\n\u001b[1;32m    133\u001b[0m     \u001b[39mreturn\u001b[39;00m Data1DKline(secid , feature, values)\n\u001b[1;32m    134\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Workspace/learndl/scripts/data_utils/DataTank.py:138\u001b[0m, in \u001b[0;36mData1D._kline_fillna\u001b[0;34m(self, kline, feature)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_kline_fillna\u001b[39m(\u001b[39mself\u001b[39m , kline , feature):\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mif\u001b[39;00m kline\u001b[39m.\u001b[39;49misnan()\u001b[39m.\u001b[39msum() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39mreturn\u001b[39;00m kline\n\u001b[1;32m    139\u001b[0m     \u001b[39mfor\u001b[39;00m i , f \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(feature):\n\u001b[1;32m    140\u001b[0m         \u001b[39mif\u001b[39;00m f \u001b[39m==\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mclose\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mopen\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mhigh\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlow\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mvwap\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'isnan'"
     ]
    }
   ],
   "source": [
    "k.to_kline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtank = DataTank('/'.join([DIR_data,param['dtank_h5']]) , open = True , mode = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtank.get_object(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtank.file['15min/trade'].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
