{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import torchmetrics\n",
    "import random\n",
    "import glob\n",
    "import math\n",
    "from einops import rearrange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IC LOSS\n",
    "class ICLoss(nn.Module):\n",
    "    def __init__(self, gamma=0):\n",
    "        super(ICLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred_ = torch.mean(y_pred, dim=1).unsqueeze(1)\n",
    "        y_pred_demean = y_pred_ - y_pred_.mean(dim=0, keepdim=True)\n",
    "        y_true_demean = y_true - y_true.mean(dim=0, keepdim=True)\n",
    "        cos_sim = F.cosine_similarity(y_pred_demean, y_true_demean, dim=0)\n",
    "        loss1 = 1 - cos_sim.mean()\n",
    "        \n",
    "        if self.gamma > 0:\n",
    "            F_inv = torch.linalg.inv(torch.matmul(y_pred_demean.T, y_pred_demean))\n",
    "            penalty = torch.trace(F_inv)\n",
    "            loss1 = loss1 + self.gamma * penalty\n",
    "        return loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= 位置编码=================\n",
    "class PositionalEncoding(nn.Module):\n",
    "    '''\n",
    "    PatchTST位置编码\n",
    "    num_patches: 序列长度\n",
    "    num_input_channels: 输入通道数\n",
    "    d_model: 嵌入维度\n",
    "    use_cls_token: bool 是否在开头添加一个cls token\n",
    "    positional_encoding_type: 位置编码类型\n",
    "    \n",
    "    input : [B  M  N  d_model]\n",
    "    output: [B  M  N  d_model] / [B  M  N+1 d_model]\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                num_patches,\n",
    "                num_input_channels,\n",
    "                d_model,\n",
    "                positional_dropout = 0,\n",
    "                use_cls_token = False,\n",
    "                positional_encoding_type = 'sincos' # 可选random（随机可学习）/ sincos（固定）\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.use_cls_token = use_cls_token\n",
    "        self.num_input_channels = num_input_channels\n",
    "        \n",
    "        if use_cls_token:\n",
    "            # cls_token: [1 x num_input_channels x 1 x d_model]\n",
    "            self.cls_token = nn.Parameter(torch.zeros(1, 1, 1, d_model))\n",
    "            # num_patches += 1\n",
    "            \n",
    "        # postional encoding: [num_patches x d_model]\n",
    "        \n",
    "        self.position_enc = self._init_pe(positional_encoding_type,num_patches,d_model)\n",
    "        # Positional dropout\n",
    "        self.positional_dropout = (\n",
    "            nn.Dropout(positional_dropout) if positional_dropout > 0 else nn.Identity()\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_pe(positional_encoding_type, num_patches,d_model):\n",
    "        # Positional encoding\n",
    "        if positional_encoding_type == \"random\":\n",
    "            position_enc = nn.Parameter(torch.randn(num_patches, d_model), requires_grad=True)\n",
    "        elif positional_encoding_type == \"sincos\":\n",
    "            position_enc = torch.zeros(num_patches,d_model)\n",
    "            position = torch.arange(0, num_patches).unsqueeze(1)\n",
    "            div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "            position_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "            position_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "            position_enc = position_enc - position_enc.mean()\n",
    "            position_enc = position_enc / (position_enc.std() * 10)\n",
    "            position_enc = nn.Parameter(position_enc, requires_grad=False)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"{positional_encoding_type} is not a valid positional encoder. Available types are 'random' and 'sincos'.\"\n",
    "            )\n",
    "        return position_enc\n",
    "\n",
    "    def forward(self, patch_input):\n",
    "        if self.use_cls_token:\n",
    "            # patch_input: [bs x num_channels x num_patches x d_model]\n",
    "            patch_input = self.positional_dropout(patch_input + self.position_enc[1:, :])\n",
    "            # append cls token where cls_token: [1 x num_channels x 1 x d_model]\n",
    "            cls_token = self.cls_token + self.position_enc[:1, :]\n",
    "            # get the same copy of cls_token for all the samples in batch: [bs x num_channels x 1 x d_model]\n",
    "            cls_tokens = cls_token.expand(patch_input.shape[0], self.num_input_channels, -1, -1)\n",
    "            # hidden_state: [bs x num_channels x (num_patches+1) x d_model]\n",
    "            hidden_state = torch.cat((cls_tokens, patch_input), dim=2)\n",
    "        else:\n",
    "            # hidden_state: [bs x num_channels x num_patches x d_model]\n",
    "            hidden_state = self.positional_dropout(patch_input + self.position_enc)\n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================PATCH EMBEDDING====================\n",
    "class TSTPatchEmbed(torch.nn.Module):\n",
    "    '''\n",
    "    输入: [B L M]\n",
    "    (padding)unfold patch + linear embedding + 位置编码(可选)\n",
    "    输出: B M N d_model ->  [B*M N d_model]\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                seq_len,\n",
    "                d_model,\n",
    "                do_padding = 'end', # padding方法，如果为None则不进行padding\n",
    "                P = None,\n",
    "                S = None,\n",
    "                M = 6, # 变量数\n",
    "                emb_dropout = 0,\n",
    "                do_pe = True,       # 是否加入位置编码\n",
    "                pe_type = 'sincos',\n",
    "                ):\n",
    "        super().__init__()\n",
    "        assert do_padding in ['end',None]\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.P = P\n",
    "        self.S = S\n",
    "        \n",
    "        # 计算patch_number\n",
    "        patch_num = int((seq_len - P) / S + 1)\n",
    "        # padding\n",
    "        self.do_padding = do_padding\n",
    "        \n",
    "        if do_padding == \"end\":\n",
    "            self.padding_patch_layer = torch.nn.ReplicationPad1d((0, S))\n",
    "            patch_num += 1\n",
    "        else:\n",
    "            # 不进行padding时，需要保证时序排在【最后的一个窗口】正好能形成一组\n",
    "            new_sequence_length = self.P + self.S * (patch_num - 1)\n",
    "            self.sequence_start = seq_len - new_sequence_length\n",
    "        \n",
    "        # linear embedding layer\n",
    "        self.emb = torch.nn.Linear(P, d_model)\n",
    "        \n",
    "        # 位置编码\n",
    "        self.do_pe = do_pe\n",
    "        if self.do_pe:\n",
    "            self.pe_enc = positional_encoding(pe = pe_type\n",
    "                                            , q_len = patch_num\n",
    "                                            , d_model = d_model\n",
    "                                            )\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        z = rearrange(inputs, 'B L M -> B M L')  # [B L M] -> [B M L]\n",
    "        \n",
    "        if self.do_padding=='end': \n",
    "            z = self.padding_patch_layer(z)\n",
    "        else:\n",
    "            z = z[:, :, self.sequence_start :]  \n",
    "        \n",
    "        z = z.unfold(dimension=-1, size=self.P, step=self.S) # z: [bs x M x N x P]\n",
    "        x_emb = self.emb(z)\n",
    "        x_emb = rearrange(x_emb,\"B M N d_model -> (B M) N d_model\")  # 合并B和M的维度 -> 把每个变量视为了一个样本\n",
    "        \n",
    "        if self.do_pe:\n",
    "            x_emb = self.emb_dropout(x_emb + self.pe_enc)   # 加入位置编码,输出维度: (BM , N ,d_model)\n",
    "        else:\n",
    "            x_emb = self.emb_dropout(x_emb)\n",
    "        return x_emb  # [BM, N, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->PatchTST\n",
    "class PatchTSTAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        num_heads,\n",
    "        dropout = 0.0,\n",
    "        is_decoder= False,\n",
    "        is_causal = False,\n",
    "        bias = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        if (self.head_dim * num_heads) != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n",
    "                f\" and `num_heads`: {num_heads}).\"\n",
    "            )\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.is_decoder = is_decoder\n",
    "        self.is_causal = is_causal\n",
    "\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        key_value_states=None,\n",
    "        past_key_value=None,\n",
    "        attention_mask=None,\n",
    "        layer_head_mask=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "        \n",
    "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
    "        # for the decoder\n",
    "        is_cross_attention = key_value_states is not None\n",
    "\n",
    "        bsz, tgt_len, _ = hidden_states.size()\n",
    "\n",
    "        # get query proj\n",
    "        query_states = self.q_proj(hidden_states) * self.scaling\n",
    "        # get key, value proj\n",
    "        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n",
    "        # is checking that the `sequence_length` of the `past_key_value` is the same as\n",
    "        # the provided `key_value_states` to support prefix tuning\n",
    "        if (\n",
    "            is_cross_attention\n",
    "            and past_key_value is not None\n",
    "            and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
    "        ):\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_states = past_key_value[0]\n",
    "            value_states = past_key_value[1]\n",
    "        elif is_cross_attention:\n",
    "            # cross_attentions\n",
    "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
    "        elif past_key_value is not None:\n",
    "            # reuse k, v, self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
    "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
    "        else:\n",
    "            # self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "            # key/value_states (first \"if\" case)\n",
    "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "            past_key_value = (key_states, value_states)\n",
    "\n",
    "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "        key_states = key_states.reshape(*proj_shape)\n",
    "        value_states = value_states.reshape(*proj_shape)\n",
    "\n",
    "        src_len = key_states.size(1)\n",
    "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
    "\n",
    "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        if layer_head_mask is not None:\n",
    "            if layer_head_mask.size() != (self.num_heads,):\n",
    "                raise ValueError(\n",
    "                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n",
    "                    f\" {layer_head_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        if output_attentions:\n",
    "            # this operation is a bit awkward, but it's required to\n",
    "            # make sure that attn_weights keeps its gradient.\n",
    "            # In order to do so, attn_weights have to be reshaped\n",
    "            # twice and have to be reused in the following\n",
    "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "        else:\n",
    "            attn_weights_reshaped = None\n",
    "\n",
    "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "\n",
    "        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n",
    "        # partitioned across GPUs when using tensor-parallelism.\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights_reshaped, past_key_value\n",
    "    \n",
    "    \n",
    "class _ScaledDotProductAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads, attn_dropout=0., res_attention=False, lsa=False):\n",
    "        super().__init__()\n",
    "        self.attn_dropout = torch.nn.Dropout(attn_dropout)\n",
    "        self.res_attention = res_attention\n",
    "        head_dim = d_model // n_heads\n",
    "        self.scale = torch.nn.Parameter(torch.tensor(head_dim ** -0.5), requires_grad=lsa)\n",
    "        self.lsa = lsa\n",
    "\n",
    "    def forward(self, q, k, v, prev=None, key_padding_mask=None, attn_mask=None):\n",
    "        # Scaled MatMul (q, k) - similarity scores for all pairs of positions in an input sequence\n",
    "        attn_scores = torch.matmul(q, k) * self.scale      # attn_scores : [bs x n_heads x max_q_len x q_len]\n",
    "\n",
    "        # Add pre-softmax attention scores from the previous layer (optional)\n",
    "        if prev is not None: attn_scores = attn_scores + prev\n",
    "\n",
    "        # Attention mask (optional)\n",
    "        if attn_mask is not None:                                     # attn_mask with shape [q_len x seq_len] - only used when q_len == seq_len\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                attn_scores.masked_fill_(attn_mask, -np.inf)\n",
    "            else:\n",
    "                attn_scores += attn_mask\n",
    "\n",
    "        # Key padding mask (optional)\n",
    "        if key_padding_mask is not None:                              # mask with shape [bs x q_len] (only when max_w_len == q_len)\n",
    "            attn_scores.masked_fill_(key_padding_mask.unsqueeze(1).unsqueeze(2), -np.inf)\n",
    "\n",
    "        # normalize the attention weights\n",
    "        attn_weights = torch.nn.functional.softmax(attn_scores, dim=-1)                 # attn_weights   : [bs x n_heads x max_q_len x q_len]\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        # compute the new values given the attention weights\n",
    "        output = torch.matmul(attn_weights, v)                        # output: [bs x n_heads x max_q_len x d_v]\n",
    "\n",
    "        if self.res_attention: return output, attn_weights, attn_scores\n",
    "        else: return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchTSTBatchNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute batch normalization over the sequence length (time) dimension.\n",
    "    inputs: B L D\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.batchnorm = nn.BatchNorm1d(d_model)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            inputs (`torch.Tensor` of shape `(batch_size, sequence_length, d_model)`):\n",
    "                input for Batch norm calculation\n",
    "        Returns:\n",
    "            `torch.Tensor` of shape `(batch_size, sequence_length, d_model)`\n",
    "        \"\"\"\n",
    "        output = inputs.transpose(1, 2)  # output: (batch_size, d_model, sequence_length)\n",
    "        output = self.batchnorm(output)\n",
    "        return output.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchTSTEncoderLayer(nn.Module):\n",
    "    '''\n",
    "    PatchTST Encoder\n",
    "    input : B, M, N, D\n",
    "    output: B, M, N, D\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 num_attention_heads,\n",
    "                 expand_factor = 2,\n",
    "                 pre_norm = True,\n",
    "                 attention_dropout=0,\n",
    "                 path_dropout=0,\n",
    "                 ff_dropout = 0,\n",
    "                 channel_attention=False,\n",
    "                 norm_type = 'batchnorm',\n",
    "                 output_attentions = False\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        ffn_dim = expand_factor * d_model\n",
    "        self.channel_attention = channel_attention\n",
    "        self.self_attn = PatchTSTAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=num_attention_heads,\n",
    "            dropout=attention_dropout,\n",
    "        )\n",
    "        # Add & Norm of the sublayer 1\n",
    "        self.dropout_path1 = nn.Dropout(path_dropout) if path_dropout > 0 else nn.Identity()\n",
    "        if norm_type == \"batchnorm\":\n",
    "            self.norm_sublayer1 = PatchTSTBatchNorm(d_model)\n",
    "        elif norm_type == \"layernorm\":\n",
    "            self.norm_sublayer1 = nn.LayerNorm(d_model)\n",
    "        else:\n",
    "            raise ValueError(f\"{norm_type} is not a supported norm layer type.\")\n",
    "\n",
    "        # Add & Norm of the sublayer 2\n",
    "        if self.channel_attention:\n",
    "            self.dropout_path2 = nn.Dropout(path_dropout) if path_dropout > 0 else nn.Identity()\n",
    "            if norm_type == \"batchnorm\":\n",
    "                self.norm_sublayer2 = PatchTSTBatchNorm(d_model)\n",
    "            elif norm_type == \"layernorm\":\n",
    "                self.norm_sublayer2 = nn.LayerNorm(d_model)\n",
    "            else:\n",
    "                raise ValueError(f\"{norm_type} is not a supported norm layer type.\")\n",
    "        \n",
    "        # Position-wise Feed-Forward\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, ffn_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(ff_dropout) if ff_dropout > 0 else nn.Identity(),\n",
    "            nn.Linear(ffn_dim, d_model),\n",
    "        )\n",
    "\n",
    "        # Add & Norm of sublayer 3\n",
    "        self.dropout_path3 = nn.Dropout(path_dropout) if path_dropout > 0 else nn.Identity()\n",
    "        if norm_type == \"batchnorm\":\n",
    "            self.norm_sublayer3 = PatchTSTBatchNorm(d_model)\n",
    "        elif norm_type == \"layernorm\":\n",
    "            self.norm_sublayer3 = nn.LayerNorm(d_model)\n",
    "        else:\n",
    "            raise ValueError(f\"{norm_type} is not a supported norm layer type.\")\n",
    "\n",
    "        self.pre_norm = pre_norm\n",
    "\n",
    "    def forward(self, hidden_state, output_attentions=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            hidden_state (`torch.Tensor` of shape `(batch_size, num_channels, sequence_length, d_model)`, *required*):\n",
    "                Past values of the time series\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the output attention of all layers\n",
    "        Return:\n",
    "            `torch.Tensor` of shape `(batch_size, num_channels, sequence_length, d_model)`\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size, num_input_channels, sequence_length, d_model = hidden_state.shape\n",
    "        # --------------------------1. attention across time--------------------------\n",
    "        hidden_state = rearrange(hidden_state, 'b m n d -> (b m) n d')\n",
    "        \n",
    "        if self.pre_norm:\n",
    "            attn_output, attn_weights, _ = self.self_attn(\n",
    "                hidden_states = self.norm_sublayer1(hidden_state),\n",
    "                output_attentions=output_attentions\n",
    "            )\n",
    "            hidden_state = hidden_state + self.dropout_path1(attn_output)\n",
    "        else:\n",
    "            attn_output, attn_weights, _ = self.self_attn(\n",
    "                hidden_states=hidden_state, output_attentions=output_attentions\n",
    "            )\n",
    "            hidden_state = self.norm_sublayer1(hidden_state + self.dropout_path1(attn_output))\n",
    "\n",
    "        hidden_state = rearrange(hidden_state, '(b m) n d -> b m n d', m = num_input_channels)\n",
    "        # --------------------------2. attention across variable at any given time--------------------------\n",
    "        if self.channel_attention:\n",
    "            hidden_state = rearrange(hidden_state, 'b m n d -> (b n) m d')\n",
    "            \n",
    "            if self.pre_norm:\n",
    "                ## Norm and Multi-Head attention and Add residual connection\n",
    "                attn_output, channel_attn_weights, _ = self.self_attn(\n",
    "                    hidden_states=self.norm_sublayer2(hidden_state), output_attentions=output_attentions\n",
    "                )\n",
    "                # Add: residual connection with residual dropout\n",
    "                hidden_state = hidden_state + self.dropout_path2(attn_output)\n",
    "            else:\n",
    "                ## Multi-Head attention and Add residual connection and Norm\n",
    "                attn_output, channel_attn_weights, _ = self.self_attn(\n",
    "                    hidden_states=hidden_state, output_attentions=output_attentions\n",
    "                )\n",
    "                # hidden_states: [(bs*sequence_length) x num_channels x d_model]\n",
    "                hidden_state = self.norm_sublayer2(hidden_state + self.dropout_path2(attn_output))\n",
    "\n",
    "            # Reshape hidden state\n",
    "            hidden_state = rearrange(hidden_state, '(b n) m d-> b m n d', n = sequence_length)\n",
    "            \n",
    "        # --------------------------3. mixing across hidden(FFN)--------------------------\n",
    "        hidden_state = rearrange(hidden_state, 'b m n d -> (b m) n d')\n",
    "        if self.pre_norm:\n",
    "            ## Norm and Position-wise Feed-Forward and Add residual connection\n",
    "            # Add: residual connection with residual dropout\n",
    "            hidden_state = hidden_state + self.dropout_path3(self.ff(self.norm_sublayer3(hidden_state)))\n",
    "        else:\n",
    "            ## Position-wise Feed-Forward and Add residual connection and Norm\n",
    "            # Add: residual connection with residual dropout\n",
    "            hidden_state = self.norm_sublayer3(hidden_state + self.dropout_path3(self.ff(hidden_state)))\n",
    "        # [bs x num_channels x sequence_length x d_model]\n",
    "        # hidden_state = hidden_state.reshape(batch_size, num_input_channels, sequence_length, d_model)\n",
    "        hidden_state = rearrange(hidden_state, '(b m) n d -> b m n d', m = num_input_channels)\n",
    "        \n",
    "        outputs = (hidden_state,)\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights, channel_attn_weights) if self.channel_attention else (attn_weights,)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchTSTFactorNetV4(L.LightningModule):\n",
    "    def __init__(self\n",
    "                ,input_dim = 6\n",
    "                ,d_model = 16\n",
    "                ,P = 4\n",
    "                ,S = 2\n",
    "                ,seq_len = 30\n",
    "                ,n_heads = 2\n",
    "                ,use_cls = False\n",
    "                ,channel_attention = False\n",
    "                ,padding_patch = 'end'\n",
    "                ,pe_type = 'sincos'\n",
    "                ,expand_factor = 1\n",
    "                ,norm = 'batch'\n",
    "                ,pre_norm = True\n",
    "                ,label_idx = 1\n",
    "                ,gamma = 0\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_ff = d_model * expand_factor\n",
    "        \n",
    "        self.N = int((seq_len - P)/S + 1)\n",
    "        \n",
    "        if padding_patch == \"end\":\n",
    "            self.N += 1\n",
    "        if use_cls:\n",
    "            self.N += 1\n",
    "            \n",
    "        self.patch_emb = TSTPatchEmbed(d_model = d_model,\n",
    "                                 seq_len = seq_len,\n",
    "                                 do_padding = padding_patch,\n",
    "                                 P = P,\n",
    "                                 S = S,\n",
    "                                 do_pe = False,\n",
    "                                 emb_dropout = 0\n",
    "                                 )\n",
    "\n",
    "        self.use_cls = use_cls\n",
    "        self.pe_enc = PositionalEncoding(num_patches = self.N,\n",
    "                            num_input_channels = input_dim,\n",
    "                            d_model = d_model,\n",
    "                            use_cls_token = use_cls,\n",
    "                            positional_encoding_type = pe_type\n",
    "                            )\n",
    "\n",
    "        self.layers  = PatchTSTEncoderLayer(d_model = d_model,\n",
    "                            expand_factor = expand_factor,\n",
    "                            num_attention_heads = n_heads,\n",
    "                            attention_dropout=0,\n",
    "                            channel_attention=channel_attention,\n",
    "                            norm_type = 'batchnorm',\n",
    "                            output_attentions = False,\n",
    "                            pre_norm=pre_norm\n",
    "                            )\n",
    "        self.label_idx = label_idx\n",
    "\n",
    "        self.predict_layer = nn.Linear(self.input_dim * d_model,50)\n",
    "        self.loss_fn = ICLoss(gamma)\n",
    "\n",
    "        self.ts_linear = nn.Linear(self.N * d_model,d_model)\n",
    "        \n",
    "    def forward(self, inputs): \n",
    "        x_emb = self.patch_emb(inputs)  # BM N D\n",
    "        x_emb = rearrange(x_emb, '(B M) N D -> B M N D',M = self.input_dim)\n",
    "        x_emb = self.pe_enc(x_emb)\n",
    "        x_out = self.layers(x_emb)[0]  # B M N D\n",
    "        \n",
    "        if self.use_cls:\n",
    "            x_out = x_out[:,:,0,:] # B M D\n",
    "        else:\n",
    "            x_out = rearrange(x_out,\"B M N d_model -> B M (N d_model)\")\n",
    "            x_out = self.ts_linear(x_out) \n",
    "        \n",
    "        output = rearrange(x_out,\"B M D -> B (M D)\")\n",
    "        return output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs = batch[0].squeeze(0)\n",
    "        x_out = self.forward(inputs)\n",
    "        y_pred = self.predict_layer(x_out)\n",
    "        \n",
    "        labels  = batch[1].squeeze(0)[:,self.label_idx].reshape(-1,1)\n",
    "        loss = self.loss_fn(y_pred,labels)\n",
    "\n",
    "        self.log(\"train_loss\",loss, on_epoch=True,prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        inputs = batch[0].squeeze(0)\n",
    "        ids = batch[1].squeeze(0)[:,0]\n",
    "        x_out = self.forward(inputs)\n",
    "        y_pred = self.predict_layer(x_out)\n",
    "        \n",
    "        return torch.cat((y_pred,ids.unsqueeze(1)),1).cpu().numpy()\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        inputs = batch[0].squeeze(0)\n",
    "        x_out = self.forward(inputs)\n",
    "        y_pred = self.predict_layer(x_out)\n",
    "        \n",
    "        labels  = batch[1].squeeze(0)[:,self.label_idx].reshape(-1,1)\n",
    "        loss = self.loss_fn(y_pred,labels)\n",
    "        \n",
    "        self.log(\"val_loss\",loss,prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_idx: 1\n",
    "d_model: 16\n",
    "P: 4\n",
    "S: 2\n",
    "n_heads: 2\n",
    "expand_factor: 1\n",
    "channel_attention: True\n",
    "use_cls: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"input_dim\": 6,\n",
    "    \"d_model\": 16,\n",
    "    \"P\": 4,\n",
    "    \"S\": 2,\n",
    "    \"seq_len\": 30,\n",
    "    \"n_heads\": 2,\n",
    "    \"d_model\": 16,\n",
    "    \"use_cls\": True,\n",
    "    \"channel_attention\": True,\n",
    "    \"expand_factor\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PatchTSTFactorNetV4(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape: B L M\n",
    "x_in  = torch.rand(1024, 30, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 96])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调用forward模块的结果\n",
    "# 用CLS时 输出维度为 B M*D\n",
    "x_out = model(x_in)\n",
    "x_out.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 6\n",
    "D = 16\n",
    "ts_linear = nn.Linear(M*D,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 50])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_final = ts_linear(x_out)\n",
    "x_final.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
