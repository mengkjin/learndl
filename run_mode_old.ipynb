{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nif __name__ == '__main__':\\n    t1 = time.time()\\n    logger.critical('Data loading start!')\\n        \\n    update_trading_data()\\n    prepare_model_data()\\n    cal_norm_param()\\n    \\n    t2 = time.time()\\n    logger.critical('Data loading Finished! Cost {:.2f} Seconds'.format(t2-t1))\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## environ\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import colorlog\n",
    "from logging import handlers\n",
    "import yaml\n",
    "\n",
    "'''\n",
    "LOG_CONFIG = {\n",
    "    'name' : 'default_log',\n",
    "    'handlers': ['console' , 'file'],\n",
    "    'level': 'DEBUG',\n",
    "    'datefmt' : '%y-%m-%d %H:%M:%S',\n",
    "    # 处理器集合\n",
    "    'console': {\n",
    "        'level': 'INFO',  # 输出信息的最低级别\n",
    "        'class': 'logging.StreamHandler',\n",
    "        'param' : {},\n",
    "        'formatter_class' : '_LevelColorFormatter', \n",
    "        # 'colorlog.ColoredFormatter' , '_LevelFormatter' , 'logging.Formatter'\n",
    "        'formatter': 'levelcolor',  # 'color' , 'level' , 'standard'\n",
    "    },\n",
    "    # 输出到文件\n",
    "    'file': {\n",
    "        'level': 'DEBUG',\n",
    "        'class': 'logging.handlers.TimedRotatingFileHandler',\n",
    "        'param' : {\n",
    "            'filename' : './logs/nn_fac_log.log',\n",
    "            'when' : 'D',\n",
    "            'backupCount': 5,  # 备份份数\n",
    "            'encoding': 'utf-8',  # 文件编码\n",
    "        },\n",
    "        'formatter_class' : '_LevelFormatter',\n",
    "        'formatter': 'level', \n",
    "    },\n",
    "    # 日志格式集合\n",
    "    'formatters': {\n",
    "        # 标准输出格式 , omit part : 'TRD:%(threadName)-10s|LVL:%(levelno)s|'\n",
    "        'standard': {\n",
    "            'fmt': '%(asctime)s|MOD:%(module)-12s|: %(message)s',\n",
    "        },\n",
    "        'level' : {\n",
    "            'fmt': '%(asctime)s|MOD:%(module)-12s|: %(message)s',\n",
    "            'level_fmts' : {\n",
    "                'DEBUG' : '%(message)s',\n",
    "                'INFO' : '%(message)s',\n",
    "            },\n",
    "        },\n",
    "        'color' : {\n",
    "            'fmt': '%(log_color)s%(asctime)s|MOD:%(module)-12s|%(reset_log_color)s: %(message_log_color)s%(message)s',\n",
    "            'log_colors' : {\n",
    "                'DEBUG':'bold,white,bg_cyan',\n",
    "                'INFO':'bold,white,bg_green',\n",
    "                'WARNING':'bold,white,bg_blue',\n",
    "                'ERROR':'bold,white,bg_purple',\n",
    "                'CRITICAL':'bold,white,bg_red',\n",
    "            },\n",
    "            'secondary_log_colors' : {\n",
    "                'reset': {\n",
    "                    'DEBUG':'reset',\n",
    "                    'INFO':'reset',\n",
    "                    'WARNING':'reset',\n",
    "                    'ERROR':'reset',\n",
    "                    'CRITICAL':'reset',\n",
    "                },\n",
    "                'message': {\n",
    "                    'DEBUG':'cyan',\n",
    "                    'INFO':'green',\n",
    "                    'WARNING':'bold,blue',\n",
    "                    'ERROR':'bold,purple',\n",
    "                    'CRITICAL':'bold,red',\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        'levelcolor' : {\n",
    "            #'fmt': '%(log_color)s%(asctime)s|MOD:%(module)-12s|TRD:%(threadName)-12s|LVL:%(levelno)s|%(reset_log_color)s: %(message_log_color)s%(message)s',\n",
    "            'fmt': '%(log_color)s%(asctime)s|MOD:%(module)-12s|%(reset_log_color)s: %(message_log_color)s%(message)s',\n",
    "            'level_fmts' : {\n",
    "                'DEBUG' : '%(message_log_color)s%(message)s',\n",
    "                'INFO' : '%(message_log_color)s%(message)s',\n",
    "            },\n",
    "            'log_colors' : {\n",
    "                'DEBUG':'bold,white,bg_cyan',\n",
    "                'INFO':'bold,white,bg_green',\n",
    "                'WARNING':'bold,white,bg_blue',\n",
    "                'ERROR':'bold,white,bg_purple',\n",
    "                'CRITICAL':'bold,white,bg_red',\n",
    "            },\n",
    "            'secondary_log_colors' : {\n",
    "                'reset': {\n",
    "                    'DEBUG':'reset',\n",
    "                    'INFO':'reset',\n",
    "                    'WARNING':'reset',\n",
    "                    'ERROR':'reset',\n",
    "                    'CRITICAL':'reset',\n",
    "                },\n",
    "                'message': {\n",
    "                    'DEBUG':'cyan',\n",
    "                    'INFO':'green',\n",
    "                    'WARNING':'bold,blue',\n",
    "                    'ERROR':'bold,purple',\n",
    "                    'CRITICAL':'bold,red',\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "'''\n",
    "\n",
    "class _LevelFormatter(logging.Formatter):\n",
    "    def __init__(self, fmt=None, datefmt=None, level_fmts={}):\n",
    "        self._level_formatters = {}\n",
    "        for level, format in level_fmts.items():\n",
    "            # Could optionally support level names too\n",
    "            self._level_formatters[getattr(logging , level)] = logging.Formatter(fmt=format, datefmt=datefmt)\n",
    "        # self._fmt will be the default format\n",
    "        super(_LevelFormatter, self).__init__(fmt=fmt, datefmt=datefmt)\n",
    "\n",
    "    def format(self, record):\n",
    "        if record.levelno in self._level_formatters:\n",
    "            return self._level_formatters[record.levelno].format(record)\n",
    "        return super(_LevelFormatter, self).format(record)\n",
    "    \n",
    "class _LevelColorFormatter(colorlog.ColoredFormatter):\n",
    "    def __init__(self, fmt=None, datefmt=None, log_colors={},level_fmts={},secondary_log_colors={}):\n",
    "        self._level_formatters = {}\n",
    "        for level, format in level_fmts.items():\n",
    "            # Could optionally support level names too\n",
    "            self._level_formatters[getattr(logging , level)] = colorlog.ColoredFormatter(fmt=format, datefmt=datefmt , log_colors=log_colors , secondary_log_colors=secondary_log_colors)\n",
    "        # self._fmt will be the default format\n",
    "        super(_LevelColorFormatter, self).__init__(fmt=fmt, datefmt=datefmt,log_colors=log_colors,secondary_log_colors=secondary_log_colors)\n",
    "\n",
    "    def format(self, record):\n",
    "        if record.levelno in self._level_formatters:\n",
    "            return self._level_formatters[record.levelno].format(record)\n",
    "        return super(_LevelColorFormatter, self).format(record)\n",
    "\n",
    "    \n",
    "def get_logger(test_output = False):\n",
    "    config_logger = get_config('logger')\n",
    "    os.makedirs(os.path.dirname(config_logger['file']['param']['filename']), exist_ok = True)\n",
    "    log = logging.getLogger(config_logger['name'])\n",
    "    exec(\"log.setLevel(logging.\"+config_logger['level']+\")\")\n",
    "\n",
    "    while log.handlers:\n",
    "        log.removeHandler(log.handlers[-1])\n",
    "\n",
    "    for hdname in config_logger['handlers']:\n",
    "        exec(hdname+\"_hdargs=config_logger[hdname]['param']\")\n",
    "        exec(hdname+\"_handler=\"+config_logger[hdname]['class']+\"(**\"+hdname+\"_hdargs)\")\n",
    "        exec(hdname+\"_fmtargs=config_logger['formatters'][config_logger[hdname]['formatter']]\")\n",
    "        exec(hdname+\"_formatter=\"+config_logger[hdname]['formatter_class']+\"(datefmt=config_logger['datefmt'],**\"+hdname+\"_fmtargs)\")\n",
    "        exec(hdname+\"_handler.setLevel(logging.\"+config_logger[hdname]['level']+\")\")\n",
    "        exec(hdname+\"_handler.setFormatter(\"+hdname+\"_formatter)\")\n",
    "        exec(\"log.addHandler(\"+hdname+\"_handler)\")\n",
    "    \n",
    "    if test_output:\n",
    "        log.debug('This is the DEBUG    message...')\n",
    "        log.info('This is the INFO     message...')\n",
    "        log.warning('This is the WARNING  message...')\n",
    "        log.error('This is the ERROR    message...')\n",
    "        log.critical('This is the CRITICAL message...')\n",
    "    return log\n",
    "\n",
    "def get_config(config_files = ['data_type' , 'train']):\n",
    "    config_dict = dict()\n",
    "    if isinstance(config_files , str): config_files = [config_files]\n",
    "    for cfg_name in config_files:\n",
    "        with open(f'./configs/config_{cfg_name}.yaml' ,'r') as f:\n",
    "            cfg = yaml.load(f , Loader = yaml.FullLoader)\n",
    "        if cfg_name == 'train':\n",
    "            if 'SPECIAL_CONFIG' in cfg.keys() and 'SHORTTEST' in cfg['SPECIAL_CONFIG'].keys(): \n",
    "                if cfg['SHORTTEST']: cfg.update(cfg['SPECIAL_CONFIG']['SHORTTEST'])\n",
    "                del cfg['SPECIAL_CONFIG']['SHORTTEST']\n",
    "            if 'SPECIAL_CONFIG' in cfg.keys() and 'TRANSFORMER' in cfg['SPECIAL_CONFIG'].keys():\n",
    "                if cfg['MODEL_MODULE'] == 'Transformer' or (cfg['MODEL_MODULE'] in ['GeneralRNN'] and 'transformer' in cfg['MODEL_PARAM']['type_rnn']):\n",
    "                    cfg['TRAIN_PARAM']['trainer'].update(cfg['SPECIAL_CONFIG']['TRANSFORMER']['trainer'])\n",
    "                del cfg['SPECIAL_CONFIG']['TRANSFORMER']\n",
    "        config_dict.update(cfg)\n",
    "    return config_dict\n",
    "## function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time , os , shutil , pprint , psutil\n",
    "from scipy import stats\n",
    "from pytimedinput import timedInput\n",
    "\n",
    "def emphasize_header(header=''):\n",
    "    print('{: ^100}'.format(''))\n",
    "    print('{:*^100}'.format(''))\n",
    "    print('{:*^100}'.format('    '+header+'    '))\n",
    "    print('{:*^100}'.format(''))\n",
    "    print('{: ^100}'.format(''))\n",
    "        \n",
    "def tensor_nancount(x, dim=None, keepdim=False):  \n",
    "    return (1-x.isnan().int()).sum(dim = dim , keepdim = keepdim)\n",
    "\n",
    "def tensor_nanmean(x, dim=None, keepdim=False):  \n",
    "    try:\n",
    "        return x.nanmean(dim = dim , keepdim = keepdim)\n",
    "    except:\n",
    "        return x.nansum(dim = dim , keepdim = keepdim) / tensor_nancount(x , dim = dim , keepdim = keepdim)\n",
    "\n",
    "def tensor_nanstd(x, dim=None, correction=1 , keepdim=False):\n",
    "    if dim is None:\n",
    "        return torch.tensor(np.nanstd(x.flatten()))\n",
    "    nancount = tensor_nancount(x , dim = dim , keepdim = True) - correction\n",
    "    return ((x - tensor_nanmean(x , dim = dim , keepdim = True)).square() / nancount).nansum(dim = dim , keepdim = keepdim).sqrt()\n",
    "\n",
    "def tensor_standardize_and_weight(x, dim=None):\n",
    "    if x.isnan().all().item():\n",
    "        return x , x       \n",
    "    x = (x - tensor_nanmean(x,dim=dim,keepdim=True)) / (tensor_nanstd(x,dim=dim,correction=0,keepdim=True) + 1e-4)\n",
    "    w = torch.ones_like(x)\n",
    "    try: \n",
    "        w[x >= x.nanmedian(dim = dim , keepdim = True)[0]] = 2\n",
    "    except:    \n",
    "        w[x >= x.nanmedian()] = 2\n",
    "    return x, w\n",
    "\n",
    "def standardize_x(x , dim=None):\n",
    "    if np.all(np.isnan(x)):\n",
    "        pass\n",
    "    elif dim is None or len(x.shape) == 1:\n",
    "        x = (x - np.nanmean(x)) / (np.nanstd(x) + 1e-4)\n",
    "    else:\n",
    "        tran_dim = np.arange(len(x.shape))\n",
    "        tran_dim[0],tran_dim[dim] = dim,0\n",
    "        y = x.transpose(*tran_dim).reshape(x.shape[dim],-1) * 1.\n",
    "        for i in range(y.shape[-1]):\n",
    "            y[:,i] = standardize_x(y[:,i])\n",
    "        x = y.reshape(*[x.shape[j] for j in tran_dim]).transpose(*tran_dim)\n",
    "    return x\n",
    "\n",
    "def standardize_and_weight(x , dim=None):\n",
    "    if np.all(np.isnan(x)):\n",
    "        pass\n",
    "    elif dim is None or len(x.shape) == 1:\n",
    "        x = (x - np.nanmean(x)) / (np.nanstd(x) + 1e-4)\n",
    "        w = np.ones_like(x)\n",
    "        w[x >= np.nanmedian(x)] = 2.\n",
    "    else:\n",
    "        tran_dim = np.arange(len(x.shape))\n",
    "        tran_dim[0],tran_dim[dim] = dim,0\n",
    "        y = x.transpose(*tran_dim).reshape(x.shape[dim],-1) * 1.\n",
    "        w = np.ones_like(y)\n",
    "        for i in range(y.shape[-1]):\n",
    "            _x , _w = standardize_and_weight(y[:,i])\n",
    "            y[:,i] , w[:,i] = _x , _w\n",
    "        x = y.reshape(*[x.shape[j] for j in tran_dim]).transpose(*tran_dim)\n",
    "        w = w.reshape(*[x.shape[j] for j in tran_dim]).transpose(*tran_dim)\n",
    "    return x , w\n",
    "\n",
    "def multi_bin_label(x , n = 10):\n",
    "    y , w = np.zeros_like(x) , np.zeros_like(x)\n",
    "    for i in range(n):\n",
    "        low , high = np.quantile(x, i/n) , np.quantile(x, (i+1)/n)\n",
    "        if i == n-1:\n",
    "            y[(x >= low)] = 2 * i - n + 1\n",
    "        elif i == 0:\n",
    "            y[(x < high)] = 2 * i - n + 1\n",
    "        else:\n",
    "            y[(x >= low) & (x < high)] = 2 * i - n + 1\n",
    "    w[:] = np.abs(y)\n",
    "    return y, w\n",
    "\n",
    "\n",
    "def bin_label(x):\n",
    "    y , w = np.zeros_like(x) , np.zeros_like(x)\n",
    "    y[x >= np.nanmedian(x)] = 1\n",
    "    w[:] = y + 1\n",
    "    return y, w\n",
    "\n",
    "def tensor_rank(x):    \n",
    "    assert x.dim() == 1 , x.dim()\n",
    "    return torch.zeros_like(x).index_copy_(0,x.argsort(),torch.arange(0.,len(x)))\n",
    "def rank_weight(x):    \n",
    "    r = tensor_rank(x)\n",
    "    w = torch.pow(0.5,((r.numel() - 1 - r) * 2 / (r.numel() - 1)))\n",
    "    return w / w.sum()\n",
    "def nd_rank(x , dim = None):\n",
    "    if dim is None:\n",
    "        w = tensor_rank(x.flatten()).reshape(x.shape)\n",
    "    else:\n",
    "        w = torch.zeros_like(x).copy_(x).transpose(-1 , dim)\n",
    "        new_shape = w.shape\n",
    "        w = w.reshape(-1 , new_shape[-1])\n",
    "        for i in range(len(w)):\n",
    "            w[i] = tensor_rank(w[i])\n",
    "        w = w.reshape(*new_shape).transpose(-1 , dim)   \n",
    "    return w\n",
    "def nd_rank_weight(x , dim = None):\n",
    "    if dim is None:\n",
    "        w = rank_weight(x.flatten()).reshape(x.shape)\n",
    "    else:\n",
    "        w = torch.zeros_like(x).copy_(x).transpose(-1 , dim)\n",
    "        new_shape = w.shape\n",
    "        w = w.reshape(-1 , new_shape[-1])\n",
    "        for i in range(len(w)):\n",
    "            w[i] = rank_weight(w[i])\n",
    "        w = w.reshape(*new_shape).transpose(-1 , dim)   \n",
    "    return w \n",
    "def nd_minus_mean(x , w , dim = None):\n",
    "    return x - (w * x).mean(dim=dim,keepdim=True)\n",
    "\n",
    "def pearson(x, y , w = None, dim = None , **kwargs):\n",
    "    w = 1. if w is None else w / w.sum(dim=dim,keepdim=True) * (w.numel() if dim is None else w.size(dim=dim))\n",
    "    x1 , y1 = nd_minus_mean(x , w , dim) , nd_minus_mean(y , w , dim)\n",
    "    return (w * x1 * y1).mean(dim = dim) / ((w * x1.square()).mean(dim=dim).sqrt() + 1e-4) / ((w * y1.square()).mean(dim=dim).sqrt() + 1e-4)\n",
    "    \n",
    "def ccc(x , y , w = None, dim = None , **kwargs):\n",
    "    w = 1. if w is None else w / w.sum(dim=dim,keepdim=True) * (w.numel() if dim is None else w.size(dim=dim))\n",
    "    x1 , y1 = nd_minus_mean(x , w , dim) , nd_minus_mean(y , w , dim)\n",
    "    cov_xy = (w * x1 * y1).mean(dim=dim)\n",
    "    mse_xy = (w * (x1 - y1).square()).mean(dim=dim)\n",
    "    return (2 * cov_xy) / (mse_xy + 2 * cov_xy + 1e-4)\n",
    "\n",
    "def mse(x , y , w = None, dim = None , reduction='mean' , **kwargs):\n",
    "    w = 1. if w is None else w / w.sum(dim=dim,keepdim=True) * (w.numel() if dim is None else w.size(dim=dim))\n",
    "    f = torch.mean if reduction == 'mean' else torch.sum\n",
    "    return f(w * (x - y).square() , dim=dim)\n",
    "\n",
    "def spearman(x , y , w = None , dim = None , **kwargs):\n",
    "    x , y = nd_rank(x , dim = dim) , nd_rank(y , dim = dim)\n",
    "    return pearson(x , y , w , dim , **kwargs)\n",
    "\n",
    "def wpearson(x, y , dim = None , **kwargs):\n",
    "    w = nd_rank_weight(y , dim = dim)\n",
    "    return pearson(x,y,w,dim)\n",
    "\n",
    "def wccc(x , y , dim = None , **kwargs):\n",
    "    w = nd_rank_weight(y , dim = dim)\n",
    "    return ccc(x,y,w,dim)\n",
    "\n",
    "def wmse(x , y , dim = None , reduction='mean' , **kwargs):\n",
    "    w = nd_rank_weight(y , dim = dim)\n",
    "    return mse(x,y,w,dim,reduction)\n",
    "\n",
    "def wspearman(x , y , dim = None , **kwargs):\n",
    "    w = nd_rank_weight(y , dim = dim)\n",
    "    return spearman(x,y,w,dim)\n",
    "\n",
    "def np_rankic(x , y , w = None , dim = None):\n",
    "    return stats.spearmanr(x,y)[0]\n",
    "\n",
    "def transpose_qkv(X,num_heads):\n",
    "    X = X.reshape(X.shape[0],X.shape[1],num_heads,-1)\n",
    "    X = X.permute(0,2,1,3)\n",
    "    return X.reshape(-1,X.shape[2],X.shape[3])\n",
    "\n",
    "def transpose_output(X,num_heads):\n",
    "    X = X.reshape(-1,num_heads,X.shape[1],X.shape[2])\n",
    "    X = X.permute(0,2,1,3)\n",
    "    return X.reshape(X.shape[0],X.shape[1],-1)\n",
    "\n",
    "def np_nanrankic(x , y):\n",
    "    assert len(x) == len(y)\n",
    "    pairwise_nonnan = (np.isnan(x)*1.0 + np.isnan(y) * 1.0 == 0)\n",
    "    try:\n",
    "        return np_rankic(x[pairwise_nonnan],y[pairwise_nonnan])\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def np_nanrankic_2d(x , y , dim = 0):\n",
    "    assert type(x) == type(y)\n",
    "    assert x.shape == y.shape\n",
    "    if dim == 0:\n",
    "        return [np_nanrankic(x[:,i],y[:,i]) for i in range(x.shape[1])]\n",
    "    else:\n",
    "        return [np_nanrankic(x[i,:],y[i,:]) for i in range(x.shape[0])]\n",
    "    \n",
    "def ask_for_confirmation(prompt ='' , timeout = 10 , recurrent = 1 , proceed_condition = lambda x:True , print_function = print):\n",
    "    assert isinstance(prompt , str)\n",
    "    userText_list , userText_cond = [] , []\n",
    "    for t in range(recurrent):\n",
    "        if t == 0:\n",
    "            _prompt = prompt \n",
    "        elif t == 1:\n",
    "            _prompt = 'Really?'\n",
    "        else:\n",
    "            _prompt = 'Really again?'\n",
    "            \n",
    "        userText, timedOut = None , None\n",
    "        if timeout > 0:\n",
    "            try:\n",
    "                userText, timedOut = timedInput(f'{_prompt} (in {timeout} seconds): ' , timeout = timeout)\n",
    "            except:\n",
    "                pass\n",
    "        if userText is None : \n",
    "            userText, timedOut = input(f'{_prompt} : ') , False\n",
    "        (_timeout , _sofar) = ('Time Out! ' , 'so far') if timedOut else ('' , '')\n",
    "        print_function(f'{_timeout}User-input {_sofar} is : [{userText}].')\n",
    "        userText_list.append(userText)\n",
    "        userText_cond.append(proceed_condition(userText))\n",
    "        if not userText_cond[-1]: \n",
    "            break\n",
    "    return userText_list , userText_cond\n",
    "\n",
    "def total_memory(unit = 1e9):\n",
    "    return psutil.Process(os.getpid()).memory_info().rss / unit\n",
    "\n",
    "def match_values(arr , values , ambiguous = 0):\n",
    "    sorter = np.argsort(arr)\n",
    "    index = np.tile(len(arr) , values.shape)\n",
    "    if ambiguous == 0:\n",
    "        index[np.isin(values , arr)] = sorter[np.searchsorted(arr, values[np.isin(values , arr)], sorter=sorter)]\n",
    "    else:\n",
    "        index[values <= max(arr)] = sorter[np.searchsorted(arr, values[values <= max(arr)], sorter=sorter)]\n",
    "    return index\n",
    "\n",
    "def merge_data_2d(data_tuple , row_tuple , col_tuple , row_all = None , col_all = None):\n",
    "    if all([not isinstance(inp,tuple) for inp in (data_tuple , row_tuple , col_tuple)]):\n",
    "        return data_tuple , row_tuple , col_tuple\n",
    "    elif not all([isinstance(inp,tuple) for inp in (data_tuple , row_tuple , col_tuple)]):\n",
    "        raise Exception(f'Not All of data_tuple , row_tuple , col_tuple are tuple instance!')\n",
    "    \n",
    "    assert len(data_tuple) == len(row_tuple) == len(col_tuple)\n",
    "    for i in range(len(data_tuple)):\n",
    "        #print(i , data_tuple[i].shape , (len(row_tuple[i]) , len(col_tuple[i])))\n",
    "        assert data_tuple[i].shape == (len(row_tuple[i]) , len(col_tuple[i]))\n",
    "    \n",
    "    row_all = sorted(list(set().union(*row_tuple))) if row_all is None else row_all\n",
    "    row_index = [[list(row_all).index(r) for r in row_i] for row_i in row_tuple]\n",
    "    \n",
    "    col_all = sorted(list(set().union(*col_tuple))) if col_all is None else col_all\n",
    "    col_index = [[list(col_all).index(c) for c in col_i] for col_i in col_tuple]\n",
    "    \n",
    "    data_all = np.full((len(row_all) , len(col_all)) , np.nan)\n",
    "    for i , data in enumerate(data_tuple):\n",
    "        data_all[np.repeat(row_index[i],len(col_index[i])),np.tile(col_index[i],len(row_index[i]))] = data[:].flatten()\n",
    "    return data_all , row_all , col_all\n",
    "\n",
    "def rmdir(d , remake_dir = False):\n",
    "    \"\"\"\n",
    "    Remove list/instance of dirs , and remake the dir if remake_dir = True\n",
    "    \"\"\"\n",
    "    if isinstance(d , (list,tuple)):\n",
    "        [shutil.rmtree(x) for x in d if os.path.exists(x)]\n",
    "        if remake_dir : [os.makedirs(x , exist_ok = True) for x in d]\n",
    "    elif isinstance(d , str):\n",
    "        if os.path.exists(d): shutil.rmtree(d)\n",
    "        if remake_dir : os.mkdir(d)\n",
    "    else:\n",
    "        raise Exception(f'KeyError : {str(d)}')\n",
    "        \n",
    "def list_converge(l , n = None , eps = None):\n",
    "    \"\"\"\n",
    "    Last n element of l has range smaller than eps\n",
    "    \"\"\"\n",
    "    n = len(l) if n is None else n\n",
    "    eps = 0 if eps is None else eps\n",
    "    return len(l) >= n and (max(l[-n:]) - min(l[-n:])) < eps   \n",
    "\n",
    "def pretty_print_dict(dictionary , width = 140 , sort_dicts = False):\n",
    "    pprint.pprint(dictionary, indent = 1, width = width , sort_dicts = sort_dicts)\n",
    "## my_utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import gc\n",
    "from torch.utils.data.dataset import IterableDataset , Dataset\n",
    "from mpl_toolkits import mplot3d\n",
    "from copy import deepcopy \n",
    "\n",
    "class lr_cosine_scheduler:\n",
    "    def __init__(self , optimizer , warmup_stage = 10 , anneal_stage = 40 , initial_lr_div = 10 , final_lr_div = 1e4):\n",
    "        self.warmup_stage= warmup_stage\n",
    "        self.anneal_stage= anneal_stage\n",
    "        self.base_lrs = [x['lr'] for x in optimizer.param_groups]\n",
    "        self.initial_lr= [x / initial_lr_div for x in self.base_lrs]\n",
    "        self.final_lr= [x / final_lr_div for x in self.base_lrs]\n",
    "        self.last_epoch = 0\n",
    "        self._step_count= 1\n",
    "        self._linear_phase = self._step_count / self.warmup_stage\n",
    "        self._cos_phase = math.pi / 2 * (self._step_count - self.warmup_stage) / self.anneal_stage\n",
    "        self._last_lr= self.initial_lr\n",
    "        \n",
    "    def get_last_lr(self):\n",
    "        #Return last computed learning rate by current scheduler.\n",
    "        return self._last_lr\n",
    "\n",
    "    def state_dict(self):\n",
    "        #Returns the state of the scheduler as a dict.\n",
    "        return self.__dict__\n",
    "    \n",
    "    def step(self):\n",
    "        self.last_epoch += 1\n",
    "        if self._step_count <= self.warmup_stage:\n",
    "            self._last_lr = [y+(x-y)*self._linear_phase for x,y in zip(self.base_lrs,self.initial_lr)]\n",
    "        elif self._step_count <= self.warmup_stage + self.anneal_stage:\n",
    "            self._last_lr = [y+(x-y)*math.cos(self._cos_phase) for x,y in zip(self.base_lrs,self.final_lr)]\n",
    "        else:\n",
    "            self._last_lr = self.final_lr\n",
    "        for x , param_group in zip(self._last_lr,self.optimizer.param_groups):\n",
    "            param_group['lr'] = x\n",
    "        self._step_count += 1\n",
    "        self._linear_phase = self._step_count / self.warmup_stage\n",
    "        self._cos_phase = math.pi / 2 * (self._step_count - self.warmup_stage) / self.anneal_stage\n",
    "        \n",
    "class Mydataset(Dataset):\n",
    "    def __init__(self, data1 , label) -> None:\n",
    "            super().__init__()\n",
    "            self.data1 = data1\n",
    "            self.label = label\n",
    "    def __len__(self):\n",
    "        return len(self.data1)\n",
    "    def __getitem__(self , ii):\n",
    "        return self.data1[ii], self.label[ii]\n",
    "\n",
    "class MyIterdataset(IterableDataset):\n",
    "    def __init__(self, data1 , label) -> None:\n",
    "            super().__init__()\n",
    "            self.data1 = data1\n",
    "            self.label = label\n",
    "    def __len__(self):\n",
    "        return len(self.data1)\n",
    "    def __iter__(self):\n",
    "        for ii in range(len(self.data1)):\n",
    "            yield self.data1[ii], self.label[ii]\n",
    "            \n",
    "class Mydataloader_basic:\n",
    "    def __init__(self, x_set , y_set , batch_size = 1, num_worker = 0, set_name = '', batch_num = None):\n",
    "        self.dataset = Mydataset(x_set, y_set)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_worker = num_worker\n",
    "        self.dataloader = torch.utils.data.DataLoader(self.dataset , batch_size = batch_size , num_workers = num_worker)\n",
    "        self.set_name = set_name\n",
    "        self.batch_num = math.ceil(len(y_set)/batch_size)\n",
    "    def __iter__(self):\n",
    "        for d in self.dataloader: \n",
    "            yield d\n",
    "\n",
    "class Mydataloader_saved:\n",
    "    def __init__(self, set_name , batch_num , batch_folder):\n",
    "        self.set_name = set_name\n",
    "        self.batch_num = batch_num\n",
    "        self.batch_folder = batch_folder\n",
    "        self.batch_path = [f'{self.batch_folder}/{self.set_name}.{ii}.pt' for ii in range(self.batch_num)]\n",
    "    def __iter__(self):\n",
    "        for ii in range(self.batch_num): \n",
    "            yield torch.load(self.batch_path[ii])\n",
    "                \n",
    "class DesireBatchSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, sampler , batch_size_list , drop_res = True):\n",
    "        self.sampler = sampler\n",
    "        self.batch_size_list = np.array(batch_size_list).astype(int)\n",
    "        assert (self.batch_size_list >= 0).all()\n",
    "        self.drop_res = drop_res\n",
    "        \n",
    "    def __iter__(self):\n",
    "        if (not self.drop_res) and (sum(self.batch_size_list) < len(self.sampler)):\n",
    "            new_list = np.append(self.batch_size_list , len(self.sampler) - sum(self.batch_size_list))\n",
    "        else:\n",
    "            new_list = self.batch_size_list\n",
    "        \n",
    "        batch_count , sample_idx = 0 , 0\n",
    "        while batch_count < len(new_list):\n",
    "            if new_list[batch_count] > 0:\n",
    "                batch = [0] * new_list[batch_count]\n",
    "                idx_in_batch = 0\n",
    "                while True:\n",
    "                    batch[idx_in_batch] = self.sampler[sample_idx]\n",
    "                    idx_in_batch += 1\n",
    "                    sample_idx +=1\n",
    "                    if idx_in_batch == new_list[batch_count]:\n",
    "                        yield batch\n",
    "                        break\n",
    "            batch_count += 1\n",
    "        if idx_in_batch > 0:\n",
    "            yield batch[:idx_in_batch]\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.batch_size_list.sum() < len(self.sampler):\n",
    "            return len(self.batch_size_list) + 1 - self.drop_res\n",
    "        else:\n",
    "            return np.where(self.batch_size_list.cumsum() >= len(self.sampler))[0][0] + 1\n",
    "        \n",
    "class multiloss_calculator:\n",
    "    def __init__(self , multi_type = None):\n",
    "        \"\"\"\n",
    "        example:\n",
    "            import torch\n",
    "            import numpy as np\n",
    "            import matplotlib.pyplot as plt\n",
    "            \n",
    "            ml = multiloss(2)\n",
    "            ml.view_plot(2 , 'dwa')\n",
    "            ml.view_plot(2 , 'ruw')\n",
    "            ml.view_plot(2 , 'gls')\n",
    "            ml.view_plot(2 , 'rws')\n",
    "        \"\"\"\n",
    "        self.multi_type = multi_type\n",
    "        \n",
    "    def reset_multi_type(self, num_task , **kwargs):\n",
    "        self.num_task   = num_task\n",
    "        self.multi_class = self.multi_class_dict()[self.multi_type](num_task , **kwargs)\n",
    "    \n",
    "    def calculate_multi_loss(self , losses , mt_param , **kwargs):\n",
    "        return self.multi_class.forward(losses , mt_param)\n",
    "    \n",
    "    \"\"\"\n",
    "    def reset_loss_function(self, loss_type):\n",
    "        self.loss_type = loss_type\n",
    "        if isinstance(self.loss_type , (list,tuple)):\n",
    "            # various loss function version, tasks can be of the same output but different loss function\n",
    "            self.loss_functions = [loss_function(k) for k in self.loss_type]\n",
    "        else:\n",
    "            self.loss_functions = [loss_function(self.loss_type) for _ in range(self.num_task)]\n",
    "            \n",
    "    def losses(self , y , x , **kwargs):\n",
    "        return torch.tensor([f(y[i] , x[i] , **kwargs) for i,f in enumerate(self.loss_functions)])\n",
    "    \n",
    "    def calculate_multi_loss(self , y , x , **kwargs):\n",
    "        sub_losses = self.losses(y , x , **kwargs)\n",
    "        multi_loss = self.multi_class.total_loss(sub_losses)\n",
    "        return multi_loss , sub_losses\n",
    "    \"\"\"\n",
    "    def multi_class_dict(self):\n",
    "        return {\n",
    "            'ewa':self.EWA,\n",
    "            'hybrid':self.Hybrid,\n",
    "            'dwa':self.DWA,\n",
    "            'ruw':self.RUW,\n",
    "            'gls':self.GLS,\n",
    "            'rws':self.RWS,\n",
    "        }\n",
    "    \n",
    "    class _base_class():\n",
    "        \"\"\"\n",
    "        base class of multi_class class\n",
    "        \"\"\"\n",
    "        def __init__(self , num_task , **kwargs):\n",
    "            self.num_task = num_task\n",
    "            self.record_num = 0 \n",
    "            self.record_losses = []\n",
    "            self.record_weight = []\n",
    "            self.record_penalty = []\n",
    "            self.kwargs = kwargs\n",
    "            self.reset(**kwargs)\n",
    "        def reset(self , **kwargs):\n",
    "            pass\n",
    "        def record(self , losses , weight , penalty):\n",
    "            self.record_num += 1\n",
    "            self.record_losses.append(losses.detach() if isinstance(losses,torch.Tensor) else losses)\n",
    "            self.record_weight.append(weight.detach() if isinstance(weight,torch.Tensor) else weight)\n",
    "            self.record_penalty.append(penalty.detach() if isinstance(penalty,torch.Tensor) else penalty)\n",
    "        def forward(self , losses , mt_param , **kwargs):\n",
    "            weight , penalty = self.weight(losses , mt_param) , self.penalty(losses , mt_param)\n",
    "            self.record(losses , weight , penalty)\n",
    "            return self.total_loss(losses , weight , penalty)\n",
    "        def weight(self , losses , mt_param):\n",
    "            return torch.ones_like(losses)\n",
    "        def penalty(self , losses , mt_param): \n",
    "            return 0.\n",
    "        def total_loss(self , losses , weight , penalty):\n",
    "            return (losses * weight).sum() + penalty\n",
    "    \n",
    "    class EWA(_base_class):\n",
    "        \"\"\"\n",
    "        Equal weight average\n",
    "        \"\"\"\n",
    "        def __init__(self , num_task , **kwargs):\n",
    "            super().__init__(num_task , **kwargs)\n",
    "    \n",
    "    class Hybrid(_base_class):\n",
    "        \"\"\"\n",
    "        Hybrid of DWA and RUW\n",
    "        \"\"\"\n",
    "        def __init__(self , num_task , **kwargs):\n",
    "            super().__init__(num_task , **kwargs)\n",
    "        def reset(self , **kwargs):\n",
    "            self.tau = kwargs['tau']\n",
    "            self.phi = kwargs['phi']\n",
    "        def weight(self , losses , mt_param):\n",
    "            if self.record_num < 2:\n",
    "                weight = torch.ones_like(losses)\n",
    "            else:\n",
    "                weight = (self.record_losses[-1] / self.record_losses[-2] / self.tau).exp()\n",
    "                weight = weight / weight.sum() * weight.numel()\n",
    "            return weight + 1 / mt_param['alpha'].square()\n",
    "        def penalty(self , losses , mt_param): \n",
    "            penalty = (mt_param['alpha'].log().square()+1).log().sum()\n",
    "            if self.phi is not None: \n",
    "                penalty = penalty + (self.phi - mt_param['alpha'].log().abs().sum()).abs()\n",
    "            return penalty\n",
    "    \n",
    "    class DWA(_base_class):\n",
    "        \"\"\"\n",
    "        dynamic weight average\n",
    "        https://arxiv.org/pdf/1803.10704.pdf\n",
    "        https://github.com/lorenmt/mtan/tree/master/im2im_pred\n",
    "        \"\"\"\n",
    "        def __init__(self , num_task , **kwargs):\n",
    "            super().__init__(num_task , **kwargs)\n",
    "        def reset(self , **kwargs):\n",
    "            self.tau = kwargs['tau']\n",
    "        def weight(self , losses , mt_param):\n",
    "            if self.record_num < 2:\n",
    "                weight = torch.ones_like(losses)\n",
    "            else:\n",
    "                weight = (self.record_losses[-1] / self.record_losses[-2] / self.tau).exp()\n",
    "                weight = weight / weight.sum() * weight.numel()\n",
    "            return weight\n",
    "        \n",
    "    class RUW(_base_class):\n",
    "        \"\"\"\n",
    "        Revised Uncertainty Weighting (RUW) Loss\n",
    "        https://arxiv.org/pdf/2206.11049v2.pdf (RUW + DWA)\n",
    "        \"\"\"\n",
    "        def __init__(self , num_task , **kwargs):\n",
    "            super().__init__(num_task , **kwargs)\n",
    "        def reset(self , **kwargs):\n",
    "            self.phi = kwargs['phi']\n",
    "        def weight(self , losses , mt_param):\n",
    "            return 1 / mt_param['alpha'].square()\n",
    "        def penalty(self , losses , mt_param): \n",
    "            penalty = (mt_param['alpha'].log().square()+1).log().sum()\n",
    "            if self.phi is not None: \n",
    "                penalty = penalty + (self.phi - mt_param['alpha'].log().abs().sum()).abs()\n",
    "            return penalty\n",
    "\n",
    "    class GLS(_base_class):\n",
    "        \"\"\"\n",
    "        geometric loss strategy , Chennupati etc.(2019)\n",
    "        \"\"\"\n",
    "        def __init__(self , num_task , **kwargs):\n",
    "            super().__init__(num_task , **kwargs)\n",
    "        def total_loss(self , losses , weight , penalty):\n",
    "            return losses.pow(weight).prod().pow(1/weight.sum()) + penalty\n",
    "    \n",
    "    class RWS(_base_class):\n",
    "        \"\"\"\n",
    "        random weight loss, RW , Lin etc.(2021)\n",
    "        https://arxiv.org/pdf/2111.10603.pdf\n",
    "        \"\"\"\n",
    "        def __init__(self , num_task , **kwargs):\n",
    "            super().__init__(num_task , **kwargs)\n",
    "        def weight(self , losses , mt_param): \n",
    "            return torch.nn.functional.softmax(torch.rand_like(losses),-1)\n",
    "\n",
    "    def view_plot(self , multi_type = 'ruw'):\n",
    "        num_task = 2\n",
    "        if multi_type == 'ruw':\n",
    "            if num_task > 2 : num_task = 2\n",
    "            x,y = torch.rand(100,num_task),torch.rand(100,1)\n",
    "            alpha = torch.tensor(np.repeat(np.linspace(0.2, 10, 40),num_task).reshape(-1,num_task))\n",
    "            fig,ax = plt.figure(),plt.axes(projection='3d')\n",
    "            s1, s2 = np.meshgrid(alpha[:,0].numpy(), alpha[:,1].numpy())\n",
    "            l = torch.stack([torch.stack([self.RUW(y,x,[s1[i,j],s2[i,j]])[0] for j in range(s1.shape[1])]) for i in range(s1.shape[0])]).numpy()\n",
    "            ax.plot_surface(s1, s2, l, cmap='viridis')\n",
    "            ax.set_xlabel('alpha-1')\n",
    "            ax.set_ylabel('alpha-2')\n",
    "            ax.set_zlabel('loss')\n",
    "            ax.set_title(f'RUW Loss vs alpha ({num_task}-D)')\n",
    "        elif multi_type == 'gls':\n",
    "            ls = torch.tensor(np.repeat(np.linspace(0.2, 10, 40),num_task).reshape(-1,num_task))\n",
    "            fig,ax = plt.figure(),plt.axes(projection='3d')\n",
    "            s1, s2 = np.meshgrid(ls[:,0].numpy(), ls[:,1].numpy())\n",
    "            l = torch.stack([torch.stack([torch.tensor([s1[i,j],s2[i,j]]).prod().sqrt() for j in range(s1.shape[1])]) for i in range(s1.shape[0])]).numpy()\n",
    "            ax.plot_surface(s1, s2, l, cmap='viridis')\n",
    "            ax.set_xlabel('loss-1')\n",
    "            ax.set_ylabel('loss-2')\n",
    "            ax.set_zlabel('gls_loss')\n",
    "            ax.set_title(f'GLS Loss vs sub-Loss ({num_task}-D)')\n",
    "        elif multi_type == 'rws':\n",
    "            ls = torch.tensor(np.repeat(np.linspace(0.2, 10, 40),num_task).reshape(-1,num_task))\n",
    "            fig,ax = plt.figure(),plt.axes(projection='3d')\n",
    "            s1, s2 = np.meshgrid(ls[:,0].numpy(), ls[:,1].numpy())\n",
    "            l = torch.stack([torch.stack([(torch.tensor([s1[i,j],s2[i,j]])*torch.nn.functional.softmax(torch.rand(ntask),-1)).sum() for j in range(s1.shape[1])]) \n",
    "                             for i in range(s1.shape[0])]).numpy()\n",
    "            ax.plot_surface(s1, s2, l, cmap='viridis')\n",
    "            ax.set_xlabel('loss-1')\n",
    "            ax.set_ylabel('loss-2')\n",
    "            ax.set_zlabel('rws_loss')\n",
    "            ax.set_title(f'RWS Loss vs sub-Loss ({num_task}-D)')\n",
    "        elif multi_type == 'dwa':\n",
    "            ntask = 2\n",
    "            nepoch = 100\n",
    "            s = np.arange(nepoch)\n",
    "            l1 = 1 / (4+s) + 0.1 + np.random.rand(nepoch) *0.05\n",
    "            l2 = 1 / (4+2*s) + 0.15 + np.random.rand(nepoch) *0.03\n",
    "            tau = 2\n",
    "            w1 = np.exp(np.concatenate((np.array([1,1]),l1[2:]/l1[1:-1]))/tau)\n",
    "            w2 = np.exp(np.concatenate((np.array([1,1]),l2[2:]/l1[1:-1]))/tau)\n",
    "            w1 , w2 = ntask * w1 / (w1+w2) , ntask * w2 / (w1+w2)\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            ax1.plot(s, l1, color='blue', label='task1')\n",
    "            ax1.plot(s, l2, color='red', label='task2')\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.set_title('Loss for Epoch')\n",
    "            ax1.legend()\n",
    "            ax2.plot(s, w1, color='blue', label='task1')\n",
    "            ax2.plot(s, w2, color='red', label='task2')\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('Weight')\n",
    "            ax2.set_title('Weight for Epoch')\n",
    "            ax2.legend()\n",
    "        else:\n",
    "            print(f'Unknow multi_type : {multi_type}')\n",
    "            \n",
    "        plt.show()\n",
    "        \n",
    "class versatile_storage():\n",
    "    def __init__(self , default = 'disk'):\n",
    "        assert default in ['disk' , 'mem']\n",
    "        self.default = default\n",
    "        self.mem_disk = dict()\n",
    "        self.file_record = list()\n",
    "        self.file_group = dict()\n",
    "    \n",
    "    def save(self , obj , paths , to_disk = False , group = 'default'):\n",
    "        for p in self._pathlist(paths): \n",
    "            self._saveone(obj , p , self.default == 'disk' or to_disk)\n",
    "            self._addrecord(p , group)\n",
    "            \n",
    "    def load(self , path , from_disk = False):\n",
    "        return torch.load(path) if self.default == 'disk' or from_disk else self.mem_disk[path]\n",
    "\n",
    "    def _pathlist(self , p):\n",
    "        if p is None: return []\n",
    "        return [p] if isinstance(p , str) else p\n",
    "    \n",
    "    def _saveone(self , obj , p , to_disk = False):\n",
    "        if to_disk:\n",
    "            torch.save(obj , p)\n",
    "        else:\n",
    "            self.mem_disk[p] = deepcopy(obj)\n",
    "    \n",
    "    def _addrecord(self , p , group):\n",
    "        self.file_record = np.union1d(self.file_record , p)\n",
    "        if group not in self.file_group.keys(): \n",
    "            self.file_group[group] = [p]\n",
    "        else:\n",
    "            self.file_group[group] = np.union1d(self.file_group[group] , [p])\n",
    "    \n",
    "    def save_model_state(self , model , paths , to_disk = False , group = 'default'):\n",
    "        sd = model.state_dict() if (self.default == 'disk' or to_disk) else deepcopy(model).cpu().state_dict()\n",
    "        self.save(sd , paths , to_disk , group)\n",
    "        \n",
    "    def load_model_state(self , model , path , from_disk = False):\n",
    "        sd = self.load(path , from_disk)\n",
    "        model.load_state_dict(sd)\n",
    "        return model\n",
    "            \n",
    "    def valid_paths(self , paths):\n",
    "        return np.intersect1d(self._pathlist(paths) ,  self.file_record).tolist()\n",
    "    \n",
    "    def del_path(self , *args):\n",
    "        for paths in args:\n",
    "            if self.default == 'disk':\n",
    "                [os.remove(p) for p in self._pathlist(paths) if os.path.exists(p)]\n",
    "            else:\n",
    "                [self.mem_disk.__delitem__(p) for p in np.intersect1d(self._pathlist(paths) , list(self.mem_disk.keys()))]\n",
    "            self.file_record = np.setdiff1d(self.file_record , paths)\n",
    "        gc.collect()\n",
    "        \n",
    "    def del_group(self , clear_groups = []):\n",
    "        for g in self._pathlist(clear_groups):\n",
    "            paths = self.file_group.get(g)\n",
    "            if paths is not None:\n",
    "                self.del_path(paths)\n",
    "                del self.file_group[g]\n",
    "## mymodel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "from copy import deepcopy\n",
    "\n",
    "class mod_tcn_block(nn.Module):\n",
    "    def __init__(self, input_dim , output_dim , dilation, dropout=0.0 , kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size-1) * dilation\n",
    "        self.conv1 = weight_norm(nn.Conv1d(input_dim , output_dim, kernel_size, padding=padding, dilation=dilation))\n",
    "        self.conv2 = weight_norm(nn.Conv1d(output_dim, output_dim, kernel_size, padding=padding, dilation=dilation))\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        \n",
    "        self.net = nn.Sequential(self.conv1, self._chomp(padding), nn.ReLU(), nn.Dropout(dropout), \n",
    "                                 self.conv2, self._chomp(padding), nn.ReLU(), nn.Dropout(dropout))\n",
    "        \n",
    "        if input_dim != output_dim:\n",
    "            self.residual = nn.Conv1d(input_dim , output_dim, 1)\n",
    "            self.residual.weight.data.normal_(0, 0.01)\n",
    "        else:\n",
    "            self.residual = nn.Sequential()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        output = self.net(inputs)\n",
    "        output = self.relu(output + self.residual(inputs))\n",
    "        return output\n",
    "    \n",
    "    class _chomp(nn.Module):\n",
    "        def __init__(self, padding):\n",
    "            super().__init__()\n",
    "            self.padding = padding\n",
    "        def forward(self, x):\n",
    "            return x[:, :, :-self.padding] # .contiguous()\n",
    "\n",
    "class mod_tcn(nn.Module):\n",
    "    def __init__(self, input_dim , output_dim , dropout=0.0 , num_layers = 2 , kernel_size = 3):\n",
    "        super().__init__()\n",
    "        num_layers = max(2 , num_layers)\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            dilation = 2 ** i\n",
    "            inp_d , out_dim = (input_dim , output_dim) if i == 0 else (output_dim , output_dim)\n",
    "            layers += [mod_tcn_block(inp_d, out_dim, dilation=dilation, dropout=dropout , kernel_size = kernel_size)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        output = self.net(inputs.permute(0,2,1)).permute(0,2,1)\n",
    "        return output\n",
    "    \n",
    "class mod_transformer(nn.Module):\n",
    "    def __init__(self , input_dim , output_dim , dropout=0.0 , num_layers = 2):\n",
    "        super().__init__()\n",
    "        num_heads , ffn_dim = 8 , 4 * output_dim\n",
    "        assert output_dim % num_heads == 0\n",
    "        num_layers = max(2,num_layers)\n",
    "        self.fc_in = nn.Sequential(nn.Linear(input_dim, output_dim),nn.Tanh())\n",
    "        self.pos_enc = PositionalEncoding(output_dim,dropout=dropout)\n",
    "        enc_layer = nn.TransformerEncoderLayer(output_dim , num_heads, dim_feedforward=ffn_dim , dropout=dropout , batch_first=True)\n",
    "        self.trans = nn.TransformerEncoder(enc_layer , num_layers)\n",
    "    def forward(self, inputs):\n",
    "        hidden = self.fc_in(inputs)\n",
    "        hidden = self.pos_enc(hidden)\n",
    "        return self.trans(hidden)\n",
    "\n",
    "class mod_lstm(nn.Module):\n",
    "    def __init__(self , input_dim , output_dim , dropout=0.0 , num_layers = 2):\n",
    "        super().__init__()\n",
    "        num_layers = min(3,num_layers)\n",
    "        self.lstm = nn.LSTM(input_dim , output_dim , num_layers = num_layers , dropout = dropout , batch_first = True)\n",
    "    def forward(self, inputs):\n",
    "        return self.lstm(inputs)[0]\n",
    "\n",
    "class mod_gru(nn.Module):\n",
    "    def __init__(self , input_dim , output_dim , dropout=0.0 , num_layers = 2):\n",
    "        super().__init__()\n",
    "        num_layers = min(3,num_layers)\n",
    "        self.gru = nn.GRU(input_dim , output_dim , num_layers = num_layers , dropout = dropout , batch_first = True)\n",
    "    def forward(self, inputs):\n",
    "        return self.gru(inputs)[0]\n",
    "    \n",
    "class mod_ewlinear(nn.Module):\n",
    "    def __init__(self, dim = -1 , keepdim = True):\n",
    "        super().__init__()\n",
    "        self.dim , self.keepdim = dim , keepdim\n",
    "    def forward(self, inputs):\n",
    "        return inputs.mean(dim = self.dim , keepdim = self.keepdim)\n",
    "    \n",
    "class mod_parallel(nn.Module):\n",
    "    def __init__(self, sub_mod , num_mod , feedforward = True , concat_output = False):\n",
    "        super().__init__()\n",
    "        self.mod_list = nn.ModuleList([deepcopy(sub_mod) for _ in range(num_mod)])\n",
    "        self.feedforward = feedforward\n",
    "        self.concat_output = concat_output\n",
    "    def forward(self, inputs):\n",
    "        output = tuple([mod(inputs[i] if self.feedforward else inputs) for i,mod in enumerate(self.mod_list)])\n",
    "        if self.concat_output:\n",
    "            if isinstance(output[0] , (list,tuple)):\n",
    "                output = tuple([torch.cat([out[i] for out in output] , dim = -1) for i in range(len(output[0]))])  \n",
    "            else:\n",
    "                output = torch.cat(output , dim = -1)\n",
    "        return output\n",
    "\n",
    "class rnn_univariate(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim ,\n",
    "        hidden_dim: int = 2**5,\n",
    "        rnn_layers: int = 2,\n",
    "        mlp_layers: int = 2,\n",
    "        dropout:  float = 0.1,\n",
    "        fc_att:    bool = False,\n",
    "        fc_in:     bool = False,\n",
    "        type_rnn:   str = 'gru',\n",
    "        type_act:   str = 'LeakyReLU',\n",
    "        num_output: int = 1 ,\n",
    "        dec_mlp_dim:int = None,\n",
    "        output_as_factors: bool = True,\n",
    "        hidden_as_factors: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_output = num_output\n",
    "        self.kwargs = kwargs\n",
    "        self.kwargs.update({'input_dim':input_dim,'hidden_dim':hidden_dim,'rnn_layers':rnn_layers,'mlp_layers':mlp_layers,'dropout':dropout,\n",
    "                            'fc_att':fc_att,'fc_in':fc_in,'type_rnn':type_rnn,'type_act':type_act,'num_output':num_output,'dec_mlp_dim':dec_mlp_dim,\n",
    "                            'output_as_factors':output_as_factors,'hidden_as_factors':hidden_as_factors, \n",
    "                           })\n",
    "        self.encoder = mod_parallel(uni_rnn_encoder(**self.kwargs) , num_mod = 1 , feedforward = False , concat_output = True)\n",
    "        self.decoder = mod_parallel(uni_rnn_decoder(**self.kwargs) , num_mod = num_output , feedforward = False , concat_output = False)\n",
    "        self.mapping = mod_parallel(uni_rnn_mapping(**self.kwargs) , num_mod = num_output , feedforward = True , concat_output = True)\n",
    "        self.set_multiloss_params()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs.shape : (bat_size, seq, input_dim)\n",
    "        hidden = self.encoder(inputs) # hidden.shape : (bat_size, hidden_dim)\n",
    "        hidden = self.decoder(hidden) # hidden.shape : tuple of (bat_size, hidden_dim) , len is num_output\n",
    "        output = self.mapping(hidden) # output.shape : (bat_size, num_output)   \n",
    "        return output , hidden[0]\n",
    "        \n",
    "    def set_multiloss_params(self):\n",
    "        self.multiloss_alpha = torch.nn.Parameter((torch.rand(self.num_output) + 1e-4).requires_grad_())\n",
    "        \n",
    "    def get_multiloss_params(self):\n",
    "        return {'alpha':self.multiloss_alpha}\n",
    "    \n",
    "class rnn_multivariate(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim ,\n",
    "        hidden_dim: int = 2**5,\n",
    "        rnn_layers: int = 2,\n",
    "        mlp_layers: int = 2,\n",
    "        dropout:  float = 0.1,\n",
    "        fc_att:    bool = False,\n",
    "        fc_in:     bool = False,\n",
    "        type_rnn:   str = 'gru',\n",
    "        type_act:   str = 'LeakyReLU',\n",
    "        num_output: int = 1 ,\n",
    "        rnn_att:   bool = False,\n",
    "        num_heads:  int = None,\n",
    "        dec_mlp_dim:int = None,\n",
    "        ordered_param_group: bool = False,\n",
    "        output_as_factors:   bool = True,\n",
    "        hidden_as_factors:   bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_output = num_output\n",
    "        self.num_rnn = len(input_dim) if isinstance(input_dim , (list,tuple)) else 1\n",
    "        self.ordered_param_group = ordered_param_group\n",
    "        self.kwargs = kwargs\n",
    "        self.kwargs.update({'input_dim':input_dim,'hidden_dim':hidden_dim,'rnn_layers':rnn_layers,'mlp_layers':mlp_layers,'dropout':dropout,\n",
    "                            'fc_att':fc_att,'fc_in':fc_in,'type_rnn':type_rnn,'type_act':type_act,'num_output':num_output,'dec_mlp_dim':dec_mlp_dim,\n",
    "                            'rnn_att':rnn_att,'num_heads':num_heads,'num_rnn':self.num_rnn,\n",
    "                            'ordered_param_group':ordered_param_group,'output_as_factors':output_as_factors,'hidden_as_factors':hidden_as_factors,\n",
    "                           })\n",
    "        mod_encoder = multi_rnn_encoder if self.num_rnn > 1 else uni_rnn_encoder\n",
    "        mod_decoder = multi_rnn_decoder if self.num_rnn > 1 else uni_rnn_decoder\n",
    "        mod_mapping = multi_rnn_mapping if self.num_rnn > 1 else uni_rnn_mapping\n",
    "\n",
    "        self.encoder = mod_parallel(mod_encoder(**self.kwargs) , num_mod = 1 , feedforward = False , concat_output = True)\n",
    "        self.decoder = mod_parallel(mod_decoder(**self.kwargs) , num_mod = num_output , feedforward = False , concat_output = False)\n",
    "        self.mapping = mod_parallel(mod_mapping(**self.kwargs) , num_mod = num_output , feedforward = True , concat_output = True)\n",
    "\n",
    "        self.set_multiloss_params()\n",
    "        self.set_param_groups()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # inputs.shape : tuple of (bat_size, seq , input_dim[i_rnn]) , len is num_rnn\n",
    "        hidden = self.encoder(inputs) # hidden.shape : tuple of (bat_size, hidden_dim) , len is num_rnn\n",
    "        hidden = self.decoder(hidden) # hidden.shape : tuple of (bat_size, num_rnn * hidden_dim) , len is num_output\n",
    "        output = self.mapping(hidden) # output.shape : (bat_size, 1)      \n",
    "        return output , hidden[0]\n",
    "    \n",
    "    def max_round(self):\n",
    "        return len(self.param_groups)\n",
    "    \n",
    "    def set_param_groups(self):\n",
    "        self.param_groups = []\n",
    "        if self.ordered_param_group and self.num_rnn > 1:\n",
    "            for i in range(self.num_rnn):\n",
    "                _exclude_strings = np.array([[f'enc_list.{j}.',f'dec_list.{j}.'] for j in range(self.num_rnn) if j!=i]).flatten()\n",
    "                self.param_groups.append([param for k,param in self.named_parameters() if all([k.find(_str) < 0 for _str in _exclude_strings])]) \n",
    "                assert len(self.param_groups[-1]) > 0\n",
    "        else:\n",
    "            self.param_groups.append(list(self.parameters())) \n",
    "    \n",
    "    def training_round(self , round_num):\n",
    "        [par.requires_grad_(round_num >= self.max_round()) for par in self.parameters()]\n",
    "        [par.requires_grad_(True) for par in self.param_groups[round_num]]\n",
    "        \n",
    "    def set_multiloss_params(self):\n",
    "        self.multiloss_alpha = torch.nn.Parameter((torch.rand(self.num_output) + 1e-4).requires_grad_())\n",
    "        \n",
    "    def get_multiloss_params(self):\n",
    "        return {'alpha':self.multiloss_alpha}\n",
    "    \n",
    "class MyGRU(rnn_univariate):\n",
    "    def __init__(self , input_dim , type_rnn = 'gru' , num_output = 1 , **kwargs):\n",
    "        super().__init__(input_dim , type_rnn = 'gru' , num_output = 1 , **kwargs)\n",
    "        \n",
    "class MyLSTM(rnn_univariate):\n",
    "    def __init__(self , input_dim , type_rnn = 'lstm' , num_output = 1 , **kwargs):\n",
    "        super().__init__(input_dim , type_rnn = 'lstm' , num_output = 1 , **kwargs)\n",
    "        \n",
    "class MyTransformer(rnn_univariate):\n",
    "    def __init__(self , input_dim , type_rnn = 'transformer' , num_output = 1 , **kwargs):\n",
    "        super().__init__(input_dim , type_rnn = 'transformer' , num_output = 1 , **kwargs)\n",
    "        \n",
    "class MyTCN(rnn_univariate):\n",
    "    def __init__(self , input_dim , type_rnn = 'tcn' , num_output = 1 , **kwargs):\n",
    "        super().__init__(input_dim , type_rnn = 'tcn' , num_output = 1 , **kwargs)\n",
    "        \n",
    "class MynTaskRNN(rnn_univariate):\n",
    "    def __init__(self , input_dim , num_output = 1 , **kwargs):\n",
    "        super().__init__(input_dim , num_output = num_output , **kwargs)\n",
    "\n",
    "class MyGeneralRNN(rnn_multivariate):\n",
    "    def __init__(self , input_dim , **kwargs):\n",
    "        super().__init__(input_dim , **kwargs)\n",
    "\n",
    "class uni_rnn_encoder(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,rnn_layers,dropout,fc_att,fc_in,type_rnn,**kwargs):\n",
    "        super().__init__()\n",
    "        self.mod_rnn = {'transformer':mod_transformer,'lstm':mod_lstm,'gru':mod_gru,'tcn':mod_tcn,}[type_rnn]\n",
    "        if type_rnn == 'transformer': fc_in , fc_att = False , False\n",
    "        \n",
    "        self.rnn_kwargs = {'input_dim':hidden_dim if fc_in else input_dim, 'output_dim':hidden_dim,'num_layers':rnn_layers, 'dropout':dropout}\n",
    "        if 'kernel_size' in kwargs.keys() and type_rnn == 'tcn': self.rnn_kwargs['kernel_size'] = kwargs['kernel_size']\n",
    "        \n",
    "        self.fc_in = nn.Sequential(nn.Linear(input_dim, hidden_dim),nn.Tanh()) if fc_in else nn.Sequential()\n",
    "        self.fc_rnn = self.mod_rnn(**self.rnn_kwargs)\n",
    "        self.fc_enc_att = TimeWiseAttention(hidden_dim,hidden_dim,dropout=dropout) if fc_att else None\n",
    "    def forward(self, inputs):\n",
    "        # inputs.shape : (bat_size, seq, input_dim)\n",
    "        # output.shape : (bat_size, hidden_dim)\n",
    "        output = self.fc_in(inputs)\n",
    "        output = self.fc_rnn(output)\n",
    "        output = self.fc_enc_att(output) if self.fc_enc_att else output[:,-1]\n",
    "        return output\n",
    "    \n",
    "class uni_rnn_decoder(nn.Module):\n",
    "    def __init__(self,hidden_dim,dec_mlp_dim,mlp_layers,dropout,type_act,hidden_as_factors,map_to_one=False,**kwargs):\n",
    "        super().__init__()\n",
    "        assert type_act in ['LeakyReLU' , 'ReLU']\n",
    "        self.mod_act = getattr(nn , type_act)\n",
    "        self.fc_dec_mlp = nn.Sequential()\n",
    "        mlp_dim = dec_mlp_dim if dec_mlp_dim else hidden_dim\n",
    "        for i in range(mlp_layers): \n",
    "            self.fc_dec_mlp.append(nn.Sequential(nn.Linear(hidden_dim if i == 0 else mlp_dim , mlp_dim), self.mod_act(), nn.Dropout(dropout)))\n",
    "        if hidden_as_factors:\n",
    "            self.fc_hid_out = nn.Sequential(nn.Linear(mlp_dim , 1 if map_to_one else hidden_dim) , nn.BatchNorm1d(1 if map_to_one else hidden_dim)) \n",
    "        else:\n",
    "            self.fc_hid_out = nn.Linear(mlp_dim , 1 if map_to_one else hidden_dim)\n",
    "    def forward(self, inputs):\n",
    "        # inputs.shape : (bat_size, hidden_dim)\n",
    "        # output.shape : (bat_size, out_dim/hidden_dim)\n",
    "        output = self.fc_dec_mlp(inputs)\n",
    "        output = self.fc_hid_out(output)\n",
    "        return output\n",
    "    \n",
    "class uni_rnn_mapping(nn.Module):\n",
    "    def __init__(self,hidden_dim,output_as_factors,hidden_as_factors,**kwargs):\n",
    "        super().__init__()\n",
    "        self.fc_map_out = nn.Sequential(mod_ewlinear()) if hidden_as_factors else nn.Sequential(nn.Linear(hidden_dim, 1))\n",
    "        if output_as_factors: self.fc_map_out.append(nn.BatchNorm1d(1))\n",
    "    def forward(self, inputs):\n",
    "        # inputs.shape : (bat_size, hidden_dim)\n",
    "        # output.shape : (bat_size, 1)\n",
    "        return self.fc_map_out(inputs)\n",
    "\n",
    "class multi_rnn_encoder(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,**kwargs):\n",
    "        super().__init__()\n",
    "        self.enc_list = nn.ModuleList([uni_rnn_encoder(d_inp,hidden_dim,**kwargs) for d_inp in input_dim])\n",
    "    def forward(self, inputs):\n",
    "        # inputs.shape : tuple of (bat_size, seq , input_dim[i_rnn]) , seq can be different \n",
    "        # output.shape : tuple of (bat_size, hidden_dim) or tuple of (bat_size, 1) if ordered_param_group\n",
    "        output = [mod(inp) for inp , mod in zip(inputs , self.enc_list)]\n",
    "        return output\n",
    "    \n",
    "class multi_rnn_decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim,num_rnn,rnn_att,ordered_param_group,hidden_as_factors,**kwargs):\n",
    "        super().__init__()\n",
    "        self.dec_list = nn.ModuleList([uni_rnn_decoder(hidden_dim , hidden_as_factors = False , map_to_one = ordered_param_group , **kwargs) for _ in range(num_rnn)])\n",
    "        self.fc_mod_att = nn.Sequential()\n",
    "        if ordered_param_group:\n",
    "            self.fc_hid_out =  nn.BatchNorm1d(num_rnn)\n",
    "        else:\n",
    "            if rnn_att: \n",
    "                self.fc_mod_att = ModuleWiseAttention(hidden_dim,num_rnn , num_heads=kwargs['num_heads'] , dropout=kwargs['dropout'] , seperate_output=True)\n",
    "            if hidden_as_factors:\n",
    "                self.fc_hid_out = nn.Sequential(nn.Linear(num_rnn*hidden_dim , hidden_dim) , nn.BatchNorm1d(hidden_dim))\n",
    "            else:\n",
    "                self.fc_hid_out = nn.Linear(num_rnn*hidden_dim , hidden_dim)\n",
    "    def forward(self, inputs):\n",
    "        # inputs.shape : tuple of (bat_size, hidden_dim) , len is num_rnn\n",
    "        # output.shape : (bat_size, hidden_dim) or (bat_size, num_rnn) if ordered_param_group\n",
    "        output = [mod(inp) for inp , mod in zip(inputs , self.dec_list)]\n",
    "        output = torch.cat(self.fc_mod_att(output) , dim = -1)\n",
    "        output = self.fc_hid_out(output)\n",
    "        return output\n",
    "    \n",
    "class multi_rnn_mapping(nn.Module):\n",
    "    def __init__(self,hidden_dim,num_rnn,ordered_param_group,output_as_factors, hidden_as_factors,**kwargs):\n",
    "        super().__init__()\n",
    "        if ordered_param_group or hidden_as_factors: \n",
    "            self.fc_map_out = nn.Sequential(mod_ewlinear())\n",
    "        else:\n",
    "            self.fc_map_out = nn.Sequential(nn.Linear(hidden_dim, 1))\n",
    "        if output_as_factors:  self.fc_map_out.append(nn.BatchNorm1d(1))\n",
    "    def forward(self, inputs):\n",
    "        # inputs.shape : (bat_size, hidden_dim) or (bat_size, num_rnn) if ordered_param_group\n",
    "        # output.shape : (bat_size, 1)\n",
    "        return self.fc_map_out(inputs)\n",
    "    \n",
    "class TimeWiseAttention(nn.Module):\n",
    "    def __init__(self , input_dim, output_dim=None, att_dim = None, dropout = 0.0):\n",
    "        super().__init__()\n",
    "        if output_dim is None: output_dim = input_dim\n",
    "        if att_dim is None: att_dim = output_dim\n",
    "        self.fc_in = nn.Linear(input_dim, att_dim)\n",
    "        self.att_net = nn.Sequential(nn.Dropout(dropout),nn.Tanh(),nn.Linear(att_dim,1,bias=False),nn.Softmax(dim=0))\n",
    "        self.fc_out = nn.Linear(2*att_dim,output_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.fc_in(inputs)\n",
    "        att_score = self.att_net(inputs)  # [batch, seq_len, 1]\n",
    "        output = torch.mul(inputs, att_score).sum(dim=1)\n",
    "        output = torch.cat((inputs[:, -1], output), dim=1)\n",
    "        return self.fc_out(output)\n",
    "    \n",
    "class ModuleWiseAttention(nn.Module):\n",
    "    def __init__(self , input_dim , mod_num = None , att_dim = None , num_heads = None , dropout=0.0 , seperate_output = True):\n",
    "        super().__init__()\n",
    "        if isinstance(input_dim , (list,tuple)):\n",
    "            assert mod_num == len(input_dim)\n",
    "        else:\n",
    "            input_dim = [input_dim for _ in range(mod_num)]\n",
    "        \n",
    "        att_dim = max(input_dim) if att_dim is None else att_dim\n",
    "        num_heads = att_dim // 8 if num_heads is None else num_heads\n",
    "        \n",
    "        self.in_fc = nn.ModuleList([nn.Linear(inp_d , att_dim) for inp_d in input_dim])\n",
    "        self.task_mha = nn.MultiheadAttention(att_dim, num_heads = num_heads, batch_first=True , dropout = dropout)\n",
    "        self.seperate_output = seperate_output\n",
    "    def forward(self, inputs):\n",
    "        hidden = torch.stack([f(x) for x,f in zip(inputs,self.in_fc)],dim=-2)\n",
    "        hidden = self.task_mha(hidden , hidden , hidden)[0] + hidden\n",
    "        if self.seperate_output:\n",
    "            return tuple([hidden.select(-2,i) for i in range(hidden.shape[-2])])\n",
    "        else:\n",
    "            return hidden\n",
    "        \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, input_dim, dropout=0.0, max_len=1000,**kwargs):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.seq_len = max_len\n",
    "        self.P = torch.zeros(1 , self.seq_len, input_dim)\n",
    "        X = torch.arange(self.seq_len, dtype=torch.float).reshape(-1,1) / torch.pow(10000,torch.arange(0, input_dim, 2 ,dtype=torch.float) / input_dim)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X[:,:input_dim//2])\n",
    "    def forward(self, inputs):\n",
    "        return self.dropout(inputs + self.P[:,:inputs.shape[1],:].to(inputs.device))\n",
    "\n",
    "class SampleWiseTranformer(nn.Module):\n",
    "    def __init__(self , hidden_dim , ffn_dim = None , num_heads = 8 , encoder_layers = 2 , dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert hidden_dim % num_heads == 0\n",
    "        ffn_dim = 4 * hidden_dim if ffn_dim is None else ffn_dim\n",
    "        self.fc_att = TimeWiseAttention(hidden_dim,hidden_dim)\n",
    "        enc_layer = nn.TransformerEncoderLayer(hidden_dim, num_heads, dim_feedforward=ffn_dim , dropout=dropout , batch_first=True)\n",
    "        self.trans = nn.TransformerEncoder(enc_layer , encoder_layers)\n",
    "    def forward(self, inputs , pad_mask = None):\n",
    "        if inputs.isnan().any():\n",
    "            pad_mask = self.pad_mask_nan(inputs) if pad_mask is None else (self.pad_mask_nan(inputs) + pad_mask) > 0\n",
    "            inputs = inputs.nan_to_num()\n",
    "        hidden = hidden.unsqueeze(0) if hidden.dim() == 2 else self.fc_att(inputs).unsqueeze(0)\n",
    "        return self.trans(hidden , src_key_padding_mask = pad_mask).squeeze(0)\n",
    "    def pad_mask_rand(self , inputs , mask_ratio = 0.1):\n",
    "        return (torch.rand(1,inputs.shape[0]) < mask_ratio).to(inputs.device)\n",
    "    def pad_mask_nan(self , inputs):\n",
    "        return inputs.sum(dim = tuple(torch.arange(inputs.dim())[1:])).isnan().unsqueeze(0)    \n",
    "\n",
    "class TimeWiseTranformer(nn.Module):\n",
    "    def __init__(self , input_dim , hidden_dim , ffn_dim = None , num_heads = 8 , encoder_layers = 2 , dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert hidden_dim % num_heads == 0\n",
    "        ffn_dim = 4 * hidden_dim if ffn_dim is None else ffn_dim\n",
    "        self.pos_enc = PositionalEncoding(hidden_dim,dropout=dropout)\n",
    "        enc_layer = nn.TransformerEncoderLayer(hidden_dim , num_heads, dim_feedforward=ffn_dim , dropout=dropout , batch_first=True)\n",
    "        self.trans = nn.TransformerEncoder(enc_layer , encoder_layers)\n",
    "    def forward(self, inputs):\n",
    "        hidden = self.pos_enc(hidden)\n",
    "        return self.trans(hidden)\n",
    "    \n",
    "## gen_data\n",
    "import torch , h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, shutil , gc , copy , time\n",
    "import yaml\n",
    "# from globalvars import *\n",
    "\n",
    "NBARS      = {'day' : 1 , '15m' : 16 ,}\n",
    "BEFORE_DAY = 20170101\n",
    "STEP_DAY   = 5\n",
    "DATATYPE   = get_config('data_type')['DATATYPE']\n",
    "\n",
    "update_files = ['day_trading_data' , 'day_ylabels_data' , '15m_trading_data']\n",
    "data_index_dict = {'day' : ('SecID' , 'TradeDate') , '15m' : ('SecID' , 'TradeDateTime') , '30m' : ('SecID' , 'TradeDateTime') ,\n",
    "                   'gp' : ('SecID' , 'TradeDate') ,}\n",
    "\n",
    "dir_nas    = None # f'/root/autodl-nas'\n",
    "dir_data   = f'./data'\n",
    "dir_update = f'{dir_data}/update_data'\n",
    "\n",
    "path_ydata = f'{dir_data}/Ys.npz'\n",
    "path_xdata = lambda x:f'{dir_data}/Xs_{x}.npz'\n",
    "path_norm_param = f'{dir_data}/norm_param.pt'\n",
    "\n",
    "logger = get_logger()\n",
    "\n",
    "def fetch_update_from_nas():\n",
    "    source_dir = dir_nas\n",
    "    if source_dir is None: return\n",
    "    target_dir = dir_update\n",
    "    os.makedirs(target_dir , exist_ok=True)\n",
    "    fetch_list = []\n",
    "    for file_starter in update_files:\n",
    "        f_list = [f for f in os.listdir(source_dir) if f.startswith(file_starter)]\n",
    "        for f in f_list:\n",
    "            shutil.copy(f'{source_dir}/{f}', f'{target_dir}/{f}')\n",
    "            os.remove(f'{source_dir}/{f}')\n",
    "            fetch_list.append(f)\n",
    "    if len(fetch_list) > 0 : print('{:s} copy file finished!'.format(', '.join(fetch_list)))\n",
    "    return\n",
    "\n",
    "def update_trading_data(remove_update_file = True):\n",
    "    fetch_update_from_nas()\n",
    "    target_dir = dir_data\n",
    "    source_dir = dir_update\n",
    "    for file_starter in update_files:\n",
    "        row_var , col_var = data_index_dict[file_starter.split('_')[0]]\n",
    "        \n",
    "        target_path = f'{target_dir}/{file_starter}.h5'\n",
    "        source_path = sorted([f'{source_dir}/{f}' for f in os.listdir(source_dir) if f.startswith(file_starter)])\n",
    "        if len(source_path) > 0 and os.path.exists(target_path) == 0:\n",
    "            shutil.copy(source_path[0] , target_path)\n",
    "            source_path = source_path[1:]\n",
    "        if len(source_path) == 0: continue\n",
    "        target_file = h5py.File(target_path , mode='r+')\n",
    "        source_file = [h5py.File(f , mode='r') for f in source_path]\n",
    "\n",
    "        row_tuple = tuple([f.get(row_var)[:] for f in [target_file] + source_file])\n",
    "        col_tuple = tuple([f.get(col_var)[:] for f in [target_file] + source_file])\n",
    "        row_all , col_all = None , None\n",
    "        \n",
    "        for k in sorted(list(target_file.keys() - [row_var , col_var])):\n",
    "            t0 = time.time()\n",
    "            data = tuple([f.get(k)[:] for f in [target_file] + source_file])\n",
    "            data , row_all , col_all = merge_data_2d(data , row_tuple , col_tuple , row_all , col_all)\n",
    "            row_all , col_all = np.array(row_all).astype(int) , np.array(col_all).astype(int)\n",
    "            \n",
    "            del target_file[k]\n",
    "            target_file.create_dataset(k , data = data , compression=\"gzip\")\n",
    "            print(f'{file_starter} -> {k} cost {(time.time() - t0):.2f}')\n",
    "        \n",
    "        del target_file[row_var]\n",
    "        target_file.create_dataset(row_var , data = row_all , compression=\"gzip\")\n",
    "\n",
    "        del target_file[col_var]\n",
    "        target_file.create_dataset(col_var , data = col_all , compression=\"gzip\")\n",
    "\n",
    "        [f.close() for f in source_file]\n",
    "        target_file.close()\n",
    "        if remove_update_file: [os.remove(f'{source_dir}/{f}') for f in os.listdir(source_dir) if f.startswith(file_starter)]\n",
    "        print(f'Update {file_starter} Finished! From {min(col_all)} to {max(col_all)} , of {len(row_all)} stocks')\n",
    "    return\n",
    "\n",
    "def prepare_model_data():\n",
    "    source_dir = dir_data\n",
    "    target_dir = dir_data\n",
    "\n",
    "    for file_starter in update_files:\n",
    "        print(f'Preparing {file_starter} data...')\n",
    "        model_data_type , feature_type = file_starter.split('_')[0] , file_starter.split('_')[1]\n",
    "        row_var , col_var = data_index_dict[model_data_type]\n",
    "        \n",
    "        source_file = h5py.File(f'{source_dir}/{file_starter}.h5' , mode = 'r')\n",
    "        row , col = source_file.get(row_var)[:] , source_file.get(col_var)[:]\n",
    "        \n",
    "        if feature_type == 'ylabels':\n",
    "            feat = ['Y10Delay' , 'Y5Delay']\n",
    "            file_path = path_ydata\n",
    "        elif feature_type == 'trading':\n",
    "            feat = ['OpenPrice','HighPrice','LowPrice','ClosePrice','TradeVolume','VWPrice']\n",
    "            file_path = path_xdata(model_data_type)\n",
    "        else:\n",
    "            raise Exception(f'KeyError : {feature_type}')\n",
    "            \n",
    "        arr = np.array([source_file.get(k)[:] for k in feat]).transpose(1,2,0)\n",
    "        \n",
    "        if col_var == 'TradeDateTime':\n",
    "            col = col // 100\n",
    "            assert sum([j != NBARS[model_data_type] for j in [list(col).count(i) for i in set(col)]]) == 0\n",
    "            col = col[::NBARS[model_data_type]]\n",
    "            arr = arr.reshape(arr.shape[0] , -1 , NBARS[model_data_type] , arr.shape[2])\n",
    "        else:\n",
    "            arr = arr.reshape(arr.shape[0] , -1 , 1 , arr.shape[2])\n",
    "        \n",
    "        assert(arr.shape[0] , arr.shape[1]) == (len(row) , len(col))\n",
    "        \n",
    "        save_data_file(file_path , row , col , feat , arr)\n",
    "        source_file.close()\n",
    "        print(f'arr shape : {arr.shape} , row shape : {row.shape} , col shape : {col.shape}')\n",
    "    return\n",
    "\n",
    "def save_data_file(file_path , row , col , feat , arr):\n",
    "    if len(arr.shape) == 3:\n",
    "        arr = arr.reshape(arr.shape[0],arr.shape[1],1,arr.shape[3])\n",
    "    elif len(arr.shape) == 2:\n",
    "        arr = arr.reshape(arr.shape[0],arr.shape[1],1,1)\n",
    "    elif len(arr.shape) == 1:\n",
    "        raise Exception(f'DimError: shape is {str(arr.shape)}')\n",
    "    assert (arr.shape[0] , arr.shape[1] , arr.shape[-1]) == (len(row) , len(col) , len(feat))\n",
    "    np.savez_compressed(file_path , row = row , col = col , feat = feat , arr = arr)\n",
    "\n",
    "def cal_norm_param(maxday = 60 , before_day = BEFORE_DAY , step_day = STEP_DAY):\n",
    "    norm_param = {}\n",
    "    for model_data_type in DATATYPE['trade']:\n",
    "        if not os.path.exists(path_xdata(model_data_type)): continue\n",
    "        logger.error(f'[{model_data_type}] Data avg and std generation start!')\n",
    "        t0 = time.time()\n",
    "        x_dict = np.load(path_xdata(model_data_type))\n",
    "        \n",
    "        row_data , col_data = np.array(x_dict['row'] , dtype = int) , np.array(x_dict['col'] , dtype = int)\n",
    "        beg_col_id = (col_data < before_day).sum()\n",
    "        x = torch.tensor(np.array(x_dict['arr'])[:, :beg_col_id, :]).to(dtype = torch.float)\n",
    "        \n",
    "        print(f'Loading {model_data_type} trading data finished, cost {time.time() - t0:.2f} Secs')\n",
    "        stock_n , day_len , _ , feat_dim = x.shape\n",
    "        step_len = day_len // step_day\n",
    "        bars_len = maxday * NBARS[model_data_type]\n",
    "        padd_len = (0,0,0,0,0,max(0 , maxday - step_day),0,0)\n",
    "        \n",
    "        x = torch.nn.functional.pad(x,padd_len,value=np.nan)\n",
    "        avg_x = torch.zeros(bars_len , feat_dim)\n",
    "        std_x = torch.zeros(bars_len , feat_dim)\n",
    "        \n",
    "        x_div = torch.ones(stock_n , step_len , 1 , feat_dim)\n",
    "        x_div.copy_(x[:,(maxday - 1):(maxday - 1 + day_len):step_day,-1:])\n",
    "        print(x_div.shape)\n",
    "        \n",
    "        nan_sample = (x_div == 0).sum(dim = (-2,-1)) > 0\n",
    "        for i in range(maxday):\n",
    "            nan_sample += x[:,i:(i+day_len):step_day,:,:].reshape(stock_n,step_len,-1).isnan().any(dim = -1)\n",
    "\n",
    "        for i in range(maxday):\n",
    "            # (stock_n , step_len)(nonnan_sample) , day_bars , feat_dim\n",
    "            vijs = (x[:,i:(i+day_len):step_day,:,:] / x_div)[nan_sample == 0]\n",
    "            avg_x[i*NBARS[model_data_type]:(i+1)*NBARS[model_data_type]] = vijs.mean(dim = 0)\n",
    "            std_x[i*NBARS[model_data_type]:(i+1)*NBARS[model_data_type]] = vijs.std(dim = 0)\n",
    "        assert avg_x.isnan().sum() + std_x.isnan().sum() == 0\n",
    "\n",
    "        norm_param.update({model_data_type : {'avg' : avg_x , 'std' : std_x}})\n",
    "        del x\n",
    "        gc.collect()\n",
    "        \n",
    "    torch.save(norm_param , path_norm_param)\n",
    "\n",
    "def load_trading_data(model_data_type , precision = 'float'):\n",
    "    \n",
    "    t0 = time.time()\n",
    "    tensor_precision = getattr(torch , precision)\n",
    "    def set_precision(data):\n",
    "        if isinstance(data , dict):\n",
    "            return {k:set_precision(v) for k,v in data.items()}\n",
    "        elif isinstance(data , (list,tuple)):\n",
    "            return type(data)(map(set_precision , data))\n",
    "        else:\n",
    "            return data.to(tensor_precision)\n",
    "    \n",
    "    read_index = lambda x:(np.array(x['row'],dtype=int),np.array(x['col'],dtype=int))\n",
    "    read_data  = lambda x:torch.tensor(x['arr']).detach()\n",
    "    i_exact  = lambda x,y:np.intersect1d(x , y , assume_unique=True , return_indices = True)[1]\n",
    "    i_latest = lambda x,y:np.array([np.where(x<=i)[0][-1] for i in y])\n",
    "    \n",
    "    data_type_list = model_data_type.split('+')\n",
    "    y_file = np.load(path_ydata)\n",
    "    x_file = {mdt:np.load(path_xdata(mdt)) for mdt in data_type_list}\n",
    "    \n",
    "    # aligned row,col\n",
    "    yr , yc = read_index(y_file)\n",
    "    x_index = {mdt:read_index(f) for mdt,f in x_file.items()}\n",
    "    \n",
    "    row , xc_trade , xc_factor = yr , None , None\n",
    "    for mdt , (xr , xc) in x_index.items():\n",
    "        row = np.intersect1d(row , xr)\n",
    "        if mdt in DATATYPE['trade']:\n",
    "            xc_trade = xc if xc_trade is None else np.intersect1d(xc_trade , xc)\n",
    "        else:\n",
    "            xc_factor = xc if xc_factor is None else np.union1d(xc_factor , xc)\n",
    "\n",
    "    col = xc_factor if xc_trade is None else xc_trade\n",
    "    if xc_factor: col = col[col >= xc_factor.min()]\n",
    "    col , xc_tail = np.intersect1d(col , yc) , col[col > yc.max()]\n",
    "\n",
    "    y_data = read_data(y_file)[i_exact(yr,row),:][:,i_exact(yc,col)]\n",
    "    y_data = set_precision(torch.nn.functional.pad(y_data , (0,0,0,0,0,len(xc_tail),0,0) , value=np.nan))\n",
    "    col = np.concatenate((col , xc_tail))\n",
    "    \n",
    "    x_data = {}\n",
    "    for mdt,(xr , xc) in x_index.items():\n",
    "        i0 , i1 = i_exact(xr,row) , i_exact(xc,col) if mdt in DATATYPE['trade'] else i_latest(xc,col)\n",
    "        x_data.update({mdt:set_precision(read_data(x_file[mdt])[i0,:][:,i1])})\n",
    "    \n",
    "    # norm_param\n",
    "    norm_param = {k:set_precision(v) for k,v in torch.load(path_norm_param).items()}\n",
    "\n",
    "    # check\n",
    "    assert all([d.shape[0] == y_data.shape[0] == len(row) for mdt,d in x_data.items()])\n",
    "    assert all([d.shape[1] == y_data.shape[1] == len(col) for mdt,d in x_data.items()])\n",
    "    \n",
    "    return x_data , y_data , norm_param , (row , col)\n",
    "\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    t1 = time.time()\n",
    "    logger.critical('Data loading start!')\n",
    "        \n",
    "    update_trading_data()\n",
    "    prepare_model_data()\n",
    "    cal_norm_param()\n",
    "    \n",
    "    t2 = time.time()\n",
    "    logger.critical('Data loading Finished! Cost {:.2f} Seconds'.format(t2-t1))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.data_util.ModelData import ModelData2\n",
    "from scripts.data_util.ModelData import ModelData as ModelData3\n",
    "from scripts.util.trainer import trainer_parser , train_config\n",
    "\n",
    "\n",
    "class ModelData():\n",
    "    \"\"\"\n",
    "    A class to store relavant training data , includes:\n",
    "    1. Parameters: train_params , compt_params , model_data_type\n",
    "    2. Datas: x_data , y_data , norm_param , index_stock , index_date\n",
    "    3. Dataloader : yield x , y of training samples , create new ones if necessary\n",
    "    \"\"\"\n",
    "    def __init__(self):     \n",
    "        args = {'process':1,'rawname':1,'resume':0,'anchoring':0}\n",
    "        try:\n",
    "            parser = trainer_parser(args).parse_args()\n",
    "        except:\n",
    "            parser = trainer_parser(args).parse_args(args=[])\n",
    "\n",
    "        config2 = train_config(parser = parser , do_process=True , config_files=['train2'])\n",
    "        self.data2 = ModelData2(['day'] , config2 , debug_type=debug_type)\n",
    "        self.x_data = self.data2.x_data\n",
    "        self.y_data , self.norm_param , (self.index_stock , self.index_date) = self.data2.y_data , self.data2.norms , self.data2.index \n",
    "        self.stock_n , self.all_day_len = self.y_data.shape[:2]\n",
    "        self.labels_n = self.y_data.shape[-1] if any([smp['num_output'] > 1 for smp in ShareNames.model_params]) else 1\n",
    "        self.feat_dims = {mdt:v.shape[-1] for mdt,v in self.x_data.items()}\n",
    "        if len(ShareNames.data_type_list) > 1: \n",
    "            [smp.update({'input_dim':tuple([self.feat_dims[mdt] for mdt in ShareNames.data_type_list])}) for smp in ShareNames.model_params]\n",
    "        else:\n",
    "            [smp.update({'input_dim':self.feat_dims[ShareNames.data_type_list[0]]}) for smp in ShareNames.model_params]\n",
    "        #self.x_feat_dim = {mdt:v.shape[-1] for mdt,v in self.x_data.items()}\n",
    "        #self.input_dim = [self.x_feat_dim[mdt] for mdt in ShareNames.data_type_list if mdt in config['DATATYPE']['trade']]\n",
    "        #self.factor_dim = [self.x_feat_dim[mdt] for mdt in ShareNames.data_type_list if mdt in config['DATATYPE']['factor']]\n",
    "        #if len(self.input_dim) > 0: [smp.update({'input_dim':self.input_dim[0]}) for smp in ShareNames.model_params]\n",
    "        #if len(self.factor_dim) > 0: [smp.update({'factor_dim':self.factor_dim[0]}) for smp in ShareNames.model_params]\n",
    "        \n",
    "        self.input_step = config['INPUT_STEP_DAY']\n",
    "        self.test_step  = config['TEST_STEP_DAY']\n",
    "\n",
    "        ShareNames.model_date_list = self.index_date[(self.index_date >= config['BEG_DATE']) & (self.index_date <= config['END_DATE'])][::config['INTERVAL']]\n",
    "        ShareNames.test_full_dates = self.index_date[(self.index_date > config['BEG_DATE']) & (self.index_date <= config['END_DATE'])]\n",
    "        self.reset_dataloaders()\n",
    "    \n",
    "    def reset_dataloaders(self):        \n",
    "        \"\"\"\n",
    "        Reset dataloaders and dataloader_param\n",
    "        \"\"\"\n",
    "        self.dataloaders = {}\n",
    "        self.dataloader_param = ()\n",
    "        gc.collect() , torch.cuda.empty_cache()\n",
    "    \n",
    "    def new_train_dataloader(self , model_date , seqlens):\n",
    "        \"\"\"\n",
    "        Create train/valid dataloaders\n",
    "        \"\"\"\n",
    "        assert ShareNames.process_name in ['train' , 'instance']\n",
    "        self.dataloader_param = (model_date , seqlens)\n",
    "\n",
    "        if debug_method1 == 0:\n",
    "            if debug_type == 0:\n",
    "                self.data2.new_train_dataloader(model_date , seqlens)\n",
    "            else:\n",
    "                self.data2.create_dataloader(ShareNames.process_name , 'train' , model_date , seqlens)\n",
    "            self.dataloaders['train'] = self.data2.dataloaders['train']\n",
    "            self.dataloaders['valid'] = self.data2.dataloaders['valid']\n",
    "        else:\n",
    "            self.data2.new_train_dataloader(model_date , seqlens)\n",
    "\n",
    "            if starting_point <= 0:\n",
    "                self.i_train , self.i_valid , self.ii_train , self.ii_valid = None , None , None , None\n",
    "                self.y_train , self.y_valid , self.train_nonnan_sample = None , None , None\n",
    "                gc.collect() , torch.cuda.empty_cache()\n",
    "                \n",
    "                seqlens = {mdt:(seqlens[mdt] if mdt in seqlens.keys() else 1) for mdt in ShareNames.data_type_list}\n",
    "                self.seq0 = max(seqlens.values())\n",
    "                self.seq = {mdt:self.seq0 + seqlens[mdt] if seqlens[mdt] <= 0 else seqlens[mdt] for mdt in ShareNames.data_type_list}\n",
    "                model_date_col = (self.index_date < model_date).sum()    \n",
    "                d0 , d1 = max(0 , model_date_col - 15 - config['INPUT_SPAN']) , max(0 , model_date_col - 15)\n",
    "                self.day_len  = d1 - d0\n",
    "                self.step_len = self.day_len // self.input_step\n",
    "                self.lstepped = np.arange(0 , self.day_len , self.input_step)[:self.step_len]\n",
    "                \n",
    "                data_func = lambda x:torch.nn.functional.pad(x[:,d0:d1] , (0,0,0,0,0,self.seq0-self.input_step,0,0) , value=np.nan)\n",
    "                x = {k:data_func(v) for k,v in self.x_data.items()}\n",
    "                y = data_func(self.y_data).squeeze(2)[:,:,:self.labels_n]\n",
    "            else:\n",
    "                self.seq0 = self.data2.seq0\n",
    "                self.seq = self.data2.seq\n",
    "                self.day_len  = self.data2.day_len\n",
    "                self.step_len = self.data2.step_len\n",
    "                self.lstepped = self.data2.lstepped\n",
    "                x , y = self.data2.tmp_x , self.data2.tmp_y\n",
    "            \n",
    "            if starting_point <=1:\n",
    "                self._train_nonnan_sample(x , y)\n",
    "            else:\n",
    "                self.train_nonnan_sample = self.data2.tmp_train_nonnan_sample\n",
    "            \n",
    "            if starting_point <=2:\n",
    "                self._train_tv_split()\n",
    "            else:\n",
    "                self.ii_train , self.ii_valid = self.data2.tmp_ii_train , self.data2.tmp_ii_valid\n",
    "\n",
    "            if starting_point <=3:\n",
    "                self._train_y_data(y)\n",
    "            else:\n",
    "                self.i_train , self.i_valid = self.data2.tmp_i_train , self.data2.tmp_i_valid\n",
    "                self.y_train , self.y_valid = self.data2.tmp_y_train , self.data2.tmp_y_valid\n",
    "\n",
    "            if starting_point <=4:\n",
    "                self._train_dataloader(x)\n",
    "            else:\n",
    "                self.dataloaders['train'] = self.data2.dataloaders['train']\n",
    "                self.dataloaders['valid'] = self.data2.dataloaders['valid']\n",
    "                \n",
    "            x , y = None , None\n",
    "            self.i_train , self.i_valid , self.ii_train , self.ii_valid = None , None , None , None\n",
    "            self.y_train , self.y_valid , self.train_nonnan_sample = None , None , None\n",
    "            gc.collect() , torch.cuda.empty_cache()\n",
    "        \n",
    "    def new_test_dataloader(self , model_date , seqlens):\n",
    "        \"\"\"\n",
    "        Create test dataloaders\n",
    "        \"\"\"\n",
    "        assert ShareNames.process_name in ['test' , 'instance']\n",
    "        self.dataloader_param = (model_date , seqlens)\n",
    "        \n",
    "        self.x_test , self.y_test = None , None\n",
    "        gc.collect() , torch.cuda.empty_cache()\n",
    "        \n",
    "        seqlens = {mdt:(seqlens[mdt] if mdt in seqlens.keys() else 1) for mdt in ShareNames.data_type_list}\n",
    "        self.seq0 = max(seqlens.values())\n",
    "        self.seq = {mdt:self.seq0 + seqlens[mdt] if seqlens[mdt] <= 0 else seqlens[mdt] for mdt in ShareNames.data_type_list}\n",
    "        \n",
    "        if model_date == ShareNames.model_date_list[-1]:\n",
    "            next_model_date = config['END_DATE'] + 1\n",
    "        else:\n",
    "            next_model_date = ShareNames.model_date_list[ShareNames.model_date_list > model_date][0]\n",
    "        _step = (1 if ShareNames.process_name == 'instance' else self.test_step)\n",
    "        _dates_list = ShareNames.test_full_dates[::_step]\n",
    "        self.model_test_dates = _dates_list[(_dates_list > model_date) * (_dates_list <= next_model_date)]\n",
    "        d0 , d1 = np.where(self.index_date == self.model_test_dates[0])[0][0] , np.where(self.index_date == self.model_test_dates[-1])[0][0] + 1\n",
    "        self.day_len  = d1 - d0\n",
    "        self.step_len = (self.day_len // _step) + (0 if self.day_len % _step == 0 else 1)\n",
    "        self.lstepped = np.arange(0 , self.day_len , _step)[:self.step_len]\n",
    "        \n",
    "        data_func = lambda x:x[:,d0 - self.seq0 + 1:d1]\n",
    "        x = {k:data_func(v) for k,v in self.x_data.items()}\n",
    "        y = data_func(self.y_data).squeeze(2)[:,:,:self.labels_n]\n",
    "        \n",
    "        self._test_nonnan_sample(x , y)\n",
    "        self._test_y_data(y)\n",
    "        self._test_dataloader(x)\n",
    "        x , y = None , None\n",
    "        self.x_test = None\n",
    "        gc.collect() , torch.cuda.empty_cache()\n",
    "        \n",
    "    def _train_nonnan_sample(self , x , y):\n",
    "        \"\"\"\n",
    "        return non-nan sample position (with shape of stock_n * step_len)\n",
    "        \"\"\"\n",
    "        nansamp = y[:,self.lstepped + self.seq0 - 1].isnan().sum(-1)\n",
    "        for mdt in ShareNames.data_type_list:\n",
    "            for i in range(self.seq[mdt]): nansamp += x[mdt][:,(self.seq0 - self.seq[mdt] + i):][:,self.lstepped].isnan().sum((2,3))\n",
    "            if mdt in config['DATATYPE']['trade']: nansamp += (x[mdt][:,self.lstepped + self.seq0 - 1][:,:,-1] == 0).sum(-1)\n",
    "        self.train_nonnan_sample = (nansamp == 0)\n",
    "            \n",
    "    def _train_tv_split(self):\n",
    "        \"\"\"\n",
    "        update index of train/valid sub-samples of flattened all-samples(with in 0:stock_n * step_len - 1)\n",
    "        \"\"\"\n",
    "        ii_stock_wise = np.arange(self.stock_n * self.step_len)[self.train_nonnan_sample.flatten()]\n",
    "        ii_time_wise  = np.arange(self.stock_n * self.step_len).reshape(self.step_len , self.stock_n).transpose().flatten()[ii_stock_wise]\n",
    "        train_samples = int(len(ii_stock_wise) * ShareNames.train_params['dataloader']['train_ratio'])\n",
    "        random.seed(ShareNames.train_params['dataloader']['random_seed'])\n",
    "        if ShareNames.train_params['dataloader']['random_tv_split']:\n",
    "            random.shuffle(ii_stock_wise)\n",
    "            ii_train , ii_valid = ii_stock_wise[:train_samples] , ii_stock_wise[train_samples:]\n",
    "        else:\n",
    "            early_samples = ii_time_wise < sorted(ii_time_wise)[train_samples]\n",
    "            ii_train , ii_valid = ii_stock_wise[early_samples] , ii_stock_wise[early_samples == 0]\n",
    "        random.shuffle(ii_train) , random.shuffle(ii_valid)\n",
    "        self.ii_train , self.ii_valid = ii_train , ii_valid\n",
    "    \n",
    "    def _train_y_data(self , y):\n",
    "        \"\"\"\n",
    "        update position (stock_i , date_i) of and normalized (maybe include w) train/valid ydata\n",
    "        \"\"\"\n",
    "        # init i (row , col position) and y (labels) matrix\n",
    "        i_tv = torch.zeros(self.stock_n , self.step_len , 2 , dtype = int) # i_row (sec) , i_col_x (end)\n",
    "        i_tv[:,:,0] = torch.tensor(np.arange(self.stock_n , dtype = int)).reshape(-1,1) \n",
    "        i_tv[:,:,1] = torch.tensor(self.lstepped + self.seq0 - 1)\n",
    "        i_tv = i_tv.reshape(-1,i_tv.shape[-1])\n",
    "        self.i_train , self.i_valid = (i_tv[self.ii_train] , i_tv[self.ii_valid])\n",
    "        \n",
    "        y_tv = torch.zeros(self.stock_n , self.step_len , self.labels_n)\n",
    "        y_tv[:] = y[:,self.lstepped + self.seq0 - 1].nan_to_num(0)\n",
    "        y_tv[self.train_nonnan_sample == 0] = np.nan\n",
    "        y_tv , w_tv = tensor_standardize_and_weight(y_tv , dim = 0)\n",
    "        y_tv , w_tv = y_tv.reshape(-1,y_tv.shape[-1]) , w_tv.reshape(-1,w_tv.shape[-1]) \n",
    "        self.y_train , self.y_valid = (y_tv[self.ii_train] , y_tv[self.ii_valid])\n",
    "        # self.w_train , self.w_valid = (w_tv[self.ii_train] , w_tv[self.ii_valid])\n",
    "        \n",
    "    def _train_dataloader(self , x):\n",
    "        \"\"\"\n",
    "        1. if model_data_type == 'day' , update dataloaders dict(dict.key = ['train' , 'valid']), by using a oneshot method\n",
    "        2. update dataloaders dict(set_name = ['train' , 'valid']), save batch_data to './model/{model_name}/{set_name}_batch_data' and later load them\n",
    "        \"\"\"\n",
    "        if ShareNames.model_data_type == 'day' and False:\n",
    "            mdt = 'day'\n",
    "            x_tv = self._norm_x(torch.cat([x[mdt][:,self.lstepped + i] for i in range(self.seq[mdt])] , dim=2) , mdt)\n",
    "            x_tv = x_tv.reshape(-1 , self.seq[mdt] , self.feat_dims[mdt])\n",
    "            x_train , x_valid = x_tv[self.ii_train] , x_tv[self.ii_valid]\n",
    "            num_worker = min(os.cpu_count() , ShareNames.compt_params['num_worker'])\n",
    "            self.dataloaders['train'] = self.dataloader_oneshot((x_train , self.y_train) , num_worker , ShareNames.compt_params['cuda_first'])\n",
    "            self.dataloaders['valid'] = self.dataloader_oneshot((x_valid , self.y_valid) , num_worker , ShareNames.compt_params['cuda_first'])\n",
    "        else:\n",
    "            storage_loader.del_group('train')\n",
    "            set_iter = [('train' , self.i_train , self.y_train) , ('valid' , self.i_valid , self.y_valid)]\n",
    "            for set_name , set_i , set_y in set_iter:\n",
    "                batch_sampler = torch.utils.data.BatchSampler(range(len(set_i)) , ShareNames.batch_size , drop_last = False)\n",
    "                batch_file_list = []\n",
    "                for batch_num , batch_pos in enumerate(batch_sampler):\n",
    "                    batch_file_list.append(ShareNames.batch_dir[set_name] + f'/{set_name}.{batch_num}.pt')\n",
    "                    i0 , i1 , batch_y , batch_x = set_i[batch_pos , 0] , set_i[batch_pos , 1] , set_y[batch_pos] , []\n",
    "                    for mdt in ShareNames.data_type_list:\n",
    "                        batch_x.append(self._norm_x(torch.cat([x[mdt][i0,i1+i+1-self.seq[mdt]] for i in range(self.seq[mdt])],dim=1),mdt))\n",
    "                    batch_x = batch_x[0] if len(batch_x) == 1 else tuple(batch_x)\n",
    "                    storage_loader.save((batch_x, batch_y), batch_file_list[-1] , group = 'train')\n",
    "                self.dataloaders[set_name] = self.dataloader_saved(batch_file_list)\n",
    "\n",
    "    def _test_nonnan_sample(self , x , y):\n",
    "        \"\"\"\n",
    "        return non-nan sample position (with shape of stock_n * day_len)\n",
    "        \"\"\"\n",
    "        nansamp = y[:,self.lstepped + self.seq0 - 1].isnan().sum(-1)\n",
    "        for mdt in ShareNames.data_type_list:\n",
    "            for i in range(self.seq[mdt]): nansamp += x[mdt][:,(self.seq0 - self.seq[mdt] + i):][:,self.lstepped].isnan().sum((2,3))\n",
    "            if mdt in config['DATATYPE']['trade']: nansamp += (x[mdt][:,self.lstepped + self.seq0 - 1][:,:,-1] == 0).sum(-1)\n",
    "        self.test_nonnan_sample = (nansamp == 0)\n",
    "    \n",
    "    def _test_y_data(self , y):\n",
    "        \"\"\"\n",
    "        update normalized (maybe include w) test ydata\n",
    "        \"\"\"\n",
    "        y_test = torch.zeros(self.stock_n , self.step_len , self.labels_n)\n",
    "        y_test[:] = y[:,self.lstepped + self.seq0 - 1].nan_to_num(0)\n",
    "        y_test[self.test_nonnan_sample == 0] = np.nan\n",
    "        self.y_test , _ = tensor_standardize_and_weight(y_test , dim = 0)\n",
    "    \n",
    "    def _test_dataloader(self , x):\n",
    "        \"\"\"\n",
    "        1. if model_data_type == 'day' , update dataloaders dict(dict.key = ['test']), by using a oneshot method (seperate dealing by TEST_INTERVAL days)\n",
    "        2. update dataloaders dict(set_name = ['test']), save batch_data to './model/{model_name}/{set_name}_batch_data' and later load them\n",
    "        \"\"\"\n",
    "        if ShareNames.model_data_type == 'day' and False:\n",
    "            mdt = 'day'\n",
    "            x_test = self._norm_x(torch.cat([x[mdt][:,i+self.lstepped] for i in range(self.seq[mdt])],dim=2) , mdt)\n",
    "            self.dataloaders['test'] = self.dataloader_oneshot((x_test , self.y_test) , 0 , ShareNames.compt_params['cuda_first'] , 1) # iter over col(date)\n",
    "        else:\n",
    "            storage_loader.del_group('test')\n",
    "            batch_sampler = [(np.where(self.test_nonnan_sample[:,i])[0] , self.lstepped[i]) for i in range(self.step_len)] # self.test_nonnan_sample.permute(1,0)\n",
    "            batch_file_list = []\n",
    "            for batch_num , batch_pos in enumerate(batch_sampler):\n",
    "                batch_file_list.append(ShareNames.batch_dir['test'] + f'/test.{batch_num}.pt')\n",
    "                i0 , i1 , batch_y , batch_x = batch_pos[0] , batch_pos[1] + self.seq0 - 1 , self.y_test[batch_pos[0] , batch_num] , []\n",
    "                for mdt in ShareNames.data_type_list:\n",
    "                    batch_x.append(self._norm_x(torch.cat([x[mdt][i0,i1+i+1-self.seq[mdt]] for i in range(self.seq[mdt])],dim=1),mdt))\n",
    "                batch_x = batch_x[0] if len(batch_x) == 1 else tuple(batch_x)\n",
    "                storage_loader.save((batch_x, batch_y), batch_file_list[-1] , group = 'test')\n",
    "            self.dataloaders['test'] = self.dataloader_saved(batch_file_list)\n",
    "        \n",
    "    def _norm_x(self , x , key):\n",
    "        \"\"\"\n",
    "        return panel_normalized x\n",
    "        1.for ts-cols , divide by the last value, get seq-mormalized x\n",
    "        2.for seq-mormalized x , normalized by history avg and std\n",
    "        \"\"\"\n",
    "        if key in config['DATATYPE']['trade']:\n",
    "            x /= x.select(-2,-1).unsqueeze(-2) + 1e-6\n",
    "            x -= self.norm_param[key]['avg'][-x.shape[-2]:]\n",
    "            x /= self.norm_param[key]['std'][-x.shape[-2]:] + 1e-6\n",
    "        else:\n",
    "            pass\n",
    "        return x\n",
    "    \n",
    "    class dataloader_oneshot:\n",
    "        \"\"\"\n",
    "        class of oneshot dataloader\n",
    "        \"\"\"\n",
    "        def __init__(self, data , num_worker = 0 , cuda_first = True , batch_by_axis = None):\n",
    "            if cuda_first: data = cuda(data)\n",
    "            self.batch_by_axis = batch_by_axis\n",
    "            if self.batch_by_axis is None:\n",
    "                self.dataset = Mydataset(*data)  \n",
    "                self.dataloader = torch.utils.data.DataLoader(self.dataset , batch_size = ShareNames.batch_size , num_workers = (1 - cuda_first)*num_worker)\n",
    "            else:\n",
    "                self.x , self.y = data\n",
    "                \n",
    "        def __iter__(self):\n",
    "            if self.batch_by_axis is None:\n",
    "                for batch_data in self.dataloader: \n",
    "                    yield cuda(batch_data)\n",
    "            else:\n",
    "                for batch_i in range(self.y.shape[self.batch_by_axis]):\n",
    "                    x , y = self.x.select(self.batch_by_axis , batch_i) , self.y.select(self.batch_by_axis , batch_i)\n",
    "                    if y.dim() == 1:\n",
    "                        valid_row = y.isnan() == 0\n",
    "                    elif y.dim() == 2:\n",
    "                        valid_row = y.isnan().sum(-1) == 0\n",
    "                    else:\n",
    "                        valid_row = y.isnan().sum(list(range(y.dim()))[1:]) == 0\n",
    "                    batch_data = (x[valid_row] , y[valid_row])\n",
    "                    yield cuda(batch_data)\n",
    "                \n",
    "    class dataloader_saved:\n",
    "        \"\"\"\n",
    "        class of saved dataloader , retrieve batch_data from './model/{model_name}/{set_name}_batch_data'\n",
    "        \"\"\"\n",
    "        def __init__(self, batch_file_list):\n",
    "            self.batch_file_list = batch_file_list\n",
    "        def __iter__(self):\n",
    "            for batch_file in self.batch_file_list: \n",
    "                yield cuda(storage_loader.load(batch_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_type = 0\n",
    "\n",
    "debug_method1 = 1\n",
    "starting_point = 5\n",
    "# 0: good steady\n",
    "# 1:\n",
    "# 2:\n",
    "# 3: good steady , stand alone\n",
    "# 4: good !!\n",
    "# 5: no good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-02-03 23:33:34|MOD:30757165    |\u001b[0m: \u001b[1m\u001b[31mProcess Queue : Data + Train\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-02-03 23:33:34|MOD:30757165    |\u001b[0m: \u001b[1m\u001b[31mDirectories of [GeneralRNN_day_SHORTTEST] deletion Confirmed!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-02-03 23:33:34|MOD:30757165    |\u001b[0m: \u001b[1m\u001b[31mStart Process [Load Data]!\u001b[0m\n",
      "usage: ipykernel_launcher.py [-h] [--process PROCESS] [--rawname RAWNAME]\n",
      "                             [--resume RESUME] [--anchoring ANCHORING]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/home/mengkjin/.local/share/jupyter/runtime/kernel-v2-7625epp6ZNGIKf8Q.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Process Queue : Data + Train\n",
      "--Start Training New!\n",
      "--Model_name is set to LSTM_day_SHORTTEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m24-02-03 23:33:50|MOD:30757165    |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Load Data]! Cost 15.5Secs\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-02-03 23:33:50|MOD:30757165    |\u001b[0m: \u001b[1m\u001b[31mStart Process [Train Model]!\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[45m24-02-03 23:33:50|MOD:30757165    |\u001b[0m: \u001b[1m\u001b[35mStart Training Models!\u001b[0m\n",
      "\u001b[32mGeneralRNN_day_SHORTTEST #0 @20170103 LoadData Cost    1.4Secs\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  0.99814, train 0.00190, valid 0.01278, max 0.0128, best 0.0128, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.86911, train 0.14036, valid 0.16530, max 0.1653, best 0.1653, lr3.8e-03\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-02-03 23:34:10|MOD:30757165    |\u001b[0m: \u001b[1m\u001b[34mGeneralRNN_day_SHORTTEST #0 @20170103|Round 0 FirstBite Ep#  9 Max Epoch|Train 0.1749 Valid 0.1996 BestVal 0.1996|Cost  0.3Min,  1.8Sec/Ep\u001b[0m\n",
      "\u001b[32mGeneralRNN_day_SHORTTEST #0 @20170704 LoadData Cost    1.4Secs\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  0 : loss  1.00161, train-0.00156, valid-0.01343, max-0.0134, best-0.0134, lr1.0e-07\u001b[0m\n",
      "\u001b[32mFirstBite Ep#  5 : loss  0.88218, train 0.12543, valid 0.14029, max 0.1403, best 0.1403, lr3.8e-03\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[44m24-02-03 23:34:28|MOD:30757165    |\u001b[0m: \u001b[1m\u001b[34mGeneralRNN_day_SHORTTEST #0 @20170704|Round 0 FirstBite Ep#  9 Max Epoch|Train 0.1576 Valid 0.1703 BestVal 0.1703|Cost  0.3Min,  1.7Sec/Ep\u001b[0m\n",
      "\u001b[1m\u001b[37m\u001b[41m24-02-03 23:34:28|MOD:30757165    |\u001b[0m: \u001b[1m\u001b[31mFinish Process [Train Model]! Cost 0.0 Hours, 0.3 Min/Training\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time : ${2023-6-27} ${21:05}\n",
    "# @Author : Mathew Jin\n",
    "# @File : ${run_model.py}\n",
    "# chmod +x run_model.py\n",
    "# ./run_model.py --process=0 --rawname=1 --resume=0 --anchoring=0\n",
    "'''\n",
    "1.TRA\n",
    "https://arxiv.org/pdf/2106.12950.pdf\n",
    "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/src/model.py\n",
    "1.1 HIST\n",
    "https://arxiv.org/pdf/2110.13716.pdf\n",
    "https://github.com/Wentao-Xu/HIST\n",
    "2.Lightgbm\n",
    "https://github.com/microsoft/LightGBM/blob/master/examples/python-guide/plot_example.py\n",
    "https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.plot_tree.html\n",
    "3.other factors\n",
    "'''\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import itertools , random , os, shutil , gc , time ,h5py\n",
    "\n",
    "from torch.optim.swa_utils import AveragedModel , update_bn\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from copy import deepcopy\n",
    "\n",
    "# from globalvars import *\n",
    "\n",
    "# from audtorch.metrics.functional import *\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "TIME_RECODER = False\n",
    "logger = get_logger()\n",
    "config = get_config()\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "storage_model  = versatile_storage(config['STORAGE_TYPE'])\n",
    "storage_loader = versatile_storage(config['STORAGE_TYPE'])\n",
    "\n",
    "class ShareNames_conctroller():\n",
    "    \"\"\"\n",
    "    1. Assign variables into shared namespace.\n",
    "    2. Ask what process would anyone want to run : 0 : train & test(default) , 1 : train only , 2 : test only , 3 : copy to instance only\n",
    "    3. Ask if model_name and model_base_path should be changed if old dir exists\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.assign_variables(if_process = True , if_rawname = True)\n",
    "        \n",
    "    def assign_variables(self , if_process = False , if_rawname = False):\n",
    "        ShareNames.max_epoch       = config['MAX_EPOCH']\n",
    "        ShareNames.batch_size      = config['BATCH_SIZE']\n",
    "        ShareNames.precision       = config['PRECISION']\n",
    "        \n",
    "        ShareNames.model_module    = config['MODEL_MODULE']\n",
    "        ShareNames.model_data_type = config['MODEL_DATATYPE'][ShareNames.model_module]\n",
    "        ShareNames.model_nickname  = config['MODEL_NICKNAME']\n",
    "        \n",
    "        ShareNames.model_num_list  = list(range(config['MODEL_NUM']))\n",
    "        ShareNames.data_type_list  = ShareNames.model_data_type.split('+')\n",
    "        \n",
    "        ShareNames.model_name      = self._model_name()\n",
    "        ShareNames.model_base_path = f'./model/{ShareNames.model_name}'\n",
    "        ShareNames.instance_path   = f'./instance/{ShareNames.model_name}'\n",
    "        ShareNames.batch_dir       = {k:f'./data/{k}_batch_path' for k in ['train' , 'valid' , 'test']}\n",
    "        \n",
    "        if if_process  : self._process_confirmation()\n",
    "        if if_rawname  : self._rawname_confirmation()\n",
    "        \n",
    "        ShareNames.train_params = deepcopy(config['TRAIN_PARAM'])\n",
    "        ShareNames.compt_params = deepcopy(config['COMPT_PARAM'])\n",
    "        ShareNames.raw_model_params = deepcopy(config['MODEL_PARAM'])\n",
    "        ShareNames.model_params = self._load_model_param()\n",
    "        ShareNames.output_types = ShareNames.train_params['output_types']\n",
    "\n",
    "    def _model_name(self):\n",
    "        name_element = [\n",
    "            ShareNames.model_module ,\n",
    "            ShareNames.model_data_type , \n",
    "            ShareNames.model_nickname\n",
    "        ]\n",
    "        return '_'.join([x for x in name_element if x is not None])\n",
    "                          \n",
    "    def _load_model_param(self):\n",
    "        \"\"\"\n",
    "        Load and return model_params of each model_num , or save one for later use\n",
    "        \"\"\"\n",
    "        try:\n",
    "            model_params = torch.load(f'{ShareNames.model_base_path}/model_params.pt')\n",
    "        except:\n",
    "            model_params = []\n",
    "            for mm in ShareNames.model_num_list:\n",
    "                dict_mm = {'path':f'{ShareNames.model_base_path}/{mm}'}\n",
    "                dict_mm.update({k:(v[mm % len(v)] if isinstance(v,list) else v) for k,v in ShareNames.raw_model_params.items()})\n",
    "                model_params.append(dict_mm)\n",
    "        return model_params\n",
    "        \n",
    "    def _process_confirmation(self):\n",
    "        if ShareNames.process < 0:\n",
    "            logger.critical(f'What process would you want to run? 0: all (default), 1: train only, 2: test only , 3: copy to instance')\n",
    "            promt_text = f'[0,all] , [1,train] , [2,test] , [3,instance]: '\n",
    "            _text , _cond = ask_for_confirmation(promt_text , proceed_condition = lambda x:False)\n",
    "            key = _text[0]\n",
    "        else:\n",
    "            key = str(ShareNames.process)\n",
    "\n",
    "        if key in ['' , '0' , 'all']:\n",
    "            ShareNames.process_queue = ['data' , 'train' , 'test' , 'instance']\n",
    "        elif key in ['1' , 'train']:\n",
    "            ShareNames.process_queue = ['data' , 'train']\n",
    "        elif key in ['2' , 'test']:\n",
    "            ShareNames.process_queue = ['data' , 'test' , 'instance']\n",
    "        elif key in ['3' , 'instance']:\n",
    "            ShareNames.process_queue = ['data' , 'instance']\n",
    "        else:\n",
    "            raise Exception(f'Error input : {key}')\n",
    "        logger.critical('Process Queue : {:s}'.format(' + '.join(map(lambda x:(x[0].upper() + x[1:]),ShareNames.process_queue))))\n",
    "                \n",
    "    def _rawname_confirmation(self , recurrent = 1):\n",
    "        \"\"\"\n",
    "        Confirm the model_name and model_base_path if multifple model_name dirs exists.\n",
    "        If include train: check if dir of model_name exists, if so ask to remove the old ones or continue with a sequential one\n",
    "        If test only :    check if model_name exists multiple dirs, if so ask to use the raw one or the last one(default)\n",
    "        Also ask if resume training, since unexpected end of training may happen\n",
    "        \"\"\"\n",
    "        if_rawname = None if (ShareNames.rawname < 0) else (ShareNames.rawname > 0)\n",
    "        if_resume  = None if (ShareNames.resume < 0)  else (ShareNames.resume > 0)\n",
    "        \n",
    "        if 'train' in ShareNames.process_queue:\n",
    "            if os.path.exists(ShareNames.model_base_path) == False:\n",
    "                if_rawname = True\n",
    "                if_resume = False\n",
    "              \n",
    "            if if_resume is None:\n",
    "                logger.critical(f'[{ShareNames.model_base_path}] exists, input [yes] to resume training, or start a new one!')\n",
    "                promt_text = f'Confirm resume training [{ShareNames.model_name}]? [yes/no] : '\n",
    "                _text , _cond = ask_for_confirmation(promt_text ,  recurrent = recurrent)\n",
    "                if_resume = all([_t.lower() in ['' , 'yes' , 'y'] for _t in _text])\n",
    "            \n",
    "            if if_resume:\n",
    "                logger.critical(f'Resume training {ShareNames.model_name}!') \n",
    "                file_appendix = sorted([int(x.split('.')[-1]) for x in os.listdir(f'./model') if x.startswith(ShareNames.model_name + '.')])\n",
    "                if if_rawname is None and len(file_appendix) > 0:\n",
    "                    logger.critical(f'Multiple model path of {ShareNames.model_name} exists, input [yes] to confirm using the raw one, or [no] the latest!')\n",
    "                    promt_text = f'Use the raw one? [yes/no] : '\n",
    "                    _text , _cond = ask_for_confirmation(promt_text ,  recurrent = recurrent)\n",
    "                    if_rawname = all([_t.lower() in ['' , 'yes' , 'y'] for _t in _text])\n",
    "                    \n",
    "                if if_rawname or len(file_appendix) == 0:\n",
    "                    logger.critical(f'model_name is still {ShareNames.model_name}!') \n",
    "                else:\n",
    "                    ShareNames.model_name = f'{ShareNames.model_name}.{file_appendix[-1]}'\n",
    "                    ShareNames.model_base_path = f'./model/{ShareNames.model_name}'\n",
    "                    logger.critical(f'model_name is now {ShareNames.model_name}!')\n",
    "            else:\n",
    "                if if_rawname is None:\n",
    "                    logger.critical(f'[{ShareNames.model_base_path}] exists, input [yes] to confirm deletion, or a new directory will be made!')\n",
    "                    promt_text = f'Confirm Deletion of all old directories with model name [{ShareNames.model_name}]? [yes/no] : '\n",
    "                    _text , _cond = ask_for_confirmation(promt_text ,  recurrent = recurrent)\n",
    "                    if_rawname = all([_t.lower() in ['' , 'yes' , 'y'] for _t in _text])\n",
    "\n",
    "                if if_rawname:\n",
    "                    rmdir([f'./model/{d}' for d in os.listdir(f'./model') if d.startswith(ShareNames.model_name)])\n",
    "                    logger.critical(f'Directories of [{ShareNames.model_name}] deletion Confirmed!')\n",
    "                else:\n",
    "                    ShareNames.model_name += '.'+str(max([1]+[int(d.split('.')[-1])+1 for d in os.listdir(f'./model') if d.startswith(ShareNames.model_name+'.')]))\n",
    "                    ShareNames.model_base_path = f'./model/{ShareNames.model_name}'\n",
    "                    logger.critical(f'A new directory [{ShareNames.model_name}] will be made!')\n",
    "\n",
    "                os.makedirs(ShareNames.model_base_path, exist_ok = True)\n",
    "                [os.makedirs(f'{ShareNames.model_base_path}/{mm}' , exist_ok = True) for mm in ShareNames.model_num_list]\n",
    "                for copy_filename in ['configs/config_train.yaml']:\n",
    "                    shutil.copyfile(f'./{copy_filename}', f'{ShareNames.model_base_path}/{os.path.basename(copy_filename)}')\n",
    "                    \n",
    "        elif 'test' in ShareNames.process_queue:\n",
    "            file_appendix = sorted([int(x.split('.')[-1]) for x in os.listdir(f'./model') if x.startswith(ShareNames.model_name + '.')])\n",
    "            if if_rawname is None and len(file_appendix) > 0:\n",
    "                logger.critical(f'Multiple model path of {ShareNames.model_name} exists, input [yes] to confirm using the raw one, or [no] the latest!')\n",
    "                promt_text = f'Use the raw one? [yes/no] : '\n",
    "                _text , _cond = ask_for_confirmation(promt_text ,  recurrent = recurrent)\n",
    "                if_rawname = all([_t.lower() in ['' , 'yes' , 'y'] for _t in _text])\n",
    "\n",
    "            if if_rawname or len(file_appendix) == 0:\n",
    "                logger.critical(f'model_name is still {ShareNames.model_name}!') \n",
    "            else:\n",
    "                ShareNames.model_name = f'{ShareNames.model_name}.{file_appendix[-1]}'\n",
    "                ShareNames.model_base_path = f'./model/{ShareNames.model_name}'\n",
    "                logger.critical(f'model_name is now {ShareNames.model_name}!')\n",
    "                \n",
    "        ShareNames.resume_training = if_resume\n",
    "                \n",
    "class model_controller():\n",
    "    \"\"\"\n",
    "    A class to control the whole process of training , includes:\n",
    "    1. Display controls: tqdm , once , step\n",
    "    2. Parameters: train_params , compt_params , model_data_type\n",
    "    3. Data : class of train_data\n",
    "    3. loop status: model , round , attempt , epoch\n",
    "    4. file path: model , lastround , transfer(last model date)\n",
    "    5. text: model , round , attempt , epoch , exit , stat , time , trainer\n",
    "    \"\"\"\n",
    "    def __init__(self , **kwargs):\n",
    "        self.init_time = time.time()\n",
    "        self.display = {\n",
    "            'tqdm' : True if config['VERBOSITY'] >= 10 else False ,\n",
    "            'once' : True if config['VERBOSITY'] <=  2 else False ,\n",
    "            'step' : [10,5,5,3,3,1][min(config['VERBOSITY'] // 2 , 5)],\n",
    "        }\n",
    "        self.process_time = {}\n",
    "        self.shared_ctrl = ShareNames_conctroller()\n",
    "        \n",
    "    def main_process(self):\n",
    "        \"\"\"\n",
    "        Main process of load_data + train + test + instance\n",
    "        \"\"\"\n",
    "\n",
    "        for process_name in ShareNames.process_queue:\n",
    "            self.SetProcessName(process_name)\n",
    "            self.__getattribute__(f'model_process_{process_name.lower()}')()\n",
    "            rmdir([v for v in ShareNames.batch_dir.values()] , remake_dir = True)\n",
    "    \n",
    "    def SetProcessName(self , key = 'data'):\n",
    "        ShareNames.process_name = key.lower()\n",
    "        self.model_count = 0\n",
    "        if 'data' in vars(self) : self.data.reset_dataloaders()\n",
    "        if ShareNames.process_name == 'data': \n",
    "            pass\n",
    "        elif ShareNames.process_name == 'train': \n",
    "            self.f_loss    = loss_function(ShareNames.train_params['criterion']['loss'])\n",
    "            self.f_metric  = metric_function(ShareNames.train_params['criterion']['metric'])\n",
    "            self.f_penalty = {k:[penalty_function(k),v] for k,v in ShareNames.train_params['criterion']['penalty'].items() if v > 0.}\n",
    "        elif ShareNames.process_name == 'test':\n",
    "            self.f_metric  = metric_function(ShareNames.train_params['criterion']['metric'])\n",
    "            self.ic_by_date , self.ic_by_model = None , None\n",
    "        elif ShareNames.process_name == 'instance':\n",
    "            self.ic_by_date , self.ic_by_model = None , None\n",
    "        else:\n",
    "            raise Exception(f'KeyError : {key}')\n",
    "        \n",
    "    def model_process_data(self):\n",
    "        \"\"\"\n",
    "        Main process of loading basic data\n",
    "        \"\"\"\n",
    "        self.data_time = time.time()\n",
    "        logger.critical(f'Start Process [Load Data]!')\n",
    "        self.data = ModelData()\n",
    "        logger.critical('Finish Process [Load Data]! Cost {:.1f}Secs'.format(time.time() - self.data_time))\n",
    "        \n",
    "    def model_process_train(self):\n",
    "        \"\"\"\n",
    "        Main process of training\n",
    "        1. loop over model(model_date , model_num)\n",
    "        2. loop over round(if necessary) , attempt(if converge too soon) , epoch(most prevailing loops)\n",
    "        \"\"\"\n",
    "        self.train_time = time.time()\n",
    "        logger.critical(f'Start Process [Train Model]!')\n",
    "        # self.printer('model_specifics')\n",
    "        logger.error(f'Start Training Models!')\n",
    "        torch.save(ShareNames.model_params , f'{ShareNames.model_base_path}/model_params.pt')    \n",
    "        for model_date , model_num in self.ModelIter():\n",
    "            self.model_date , self.model_num = model_date , model_num\n",
    "            self.ModelPreparation('train')\n",
    "            self.TrainModel()\n",
    "        total_time = time.time() - self.train_time\n",
    "        logger.critical('Finish Process [Train Model]! Cost {:.1f} Hours, {:.1f} Min/Training'.format(total_time / 3600 , total_time / 60 / max(self.model_count , 1)))\n",
    "    \n",
    "    def model_process_test(self):\n",
    "        self.test_time = time.time()\n",
    "        logger.critical(f'Start Process [Test Model]!')        \n",
    "        logger.warning('Each Model Date Testing Mean Rank_ic:')\n",
    "        self.test_result_model_num = np.repeat(ShareNames.model_num_list,len(ShareNames.output_types))\n",
    "        self.test_result_output_type = np.tile(ShareNames.output_types,len(ShareNames.model_num_list))\n",
    "        logger.info('{: <11s}'.format('Models') + ('{: >8d}'*len(self.test_result_model_num)).format(*self.test_result_model_num))\n",
    "        logger.info('{: <11s}'.format('Output') + ('{: >8s}'*len(self.test_result_model_num)).format(*self.test_result_output_type))\n",
    "        for model_date , model_num in self.ModelIter():\n",
    "            self.model_date , self.model_num = model_date , model_num\n",
    "            self.ModelPreparation('test')\n",
    "            self.TestModel()\n",
    "        self.ModelResult()\n",
    "        logger.critical('Finish Process [Test Model]! Cost {:.1f} Secs'.format(time.time() - self.test_time))\n",
    "        \n",
    "    def model_process_instance(self):\n",
    "        if ShareNames.anchoring < 0:\n",
    "            logger.critical(f'Do you want to copy the model to instance?')\n",
    "            promt_text = f'[yes/else no]: '\n",
    "            _text , _cond = ask_for_confirmation(promt_text , timeout = -1)\n",
    "            anchoring = all([_t.lower() in ['yes','y'] for _t in _text])\n",
    "        else:\n",
    "            anchoring = ShareNames.anchoring > 0\n",
    "        if anchoring == 0:\n",
    "            logger.critical(f'Will not copy to instance!')\n",
    "            return\n",
    "        else:\n",
    "            self.instance_time = time.time()\n",
    "            logger.critical(f'Start Process [Copy to Instance]!')        \n",
    "            if os.path.exists(ShareNames.instance_path): \n",
    "                logger.critical(f'Old instance {ShareNames.instance_path} exists , remove manually first to override!')\n",
    "                logger.critical(f'The command can be \"rm -r {ShareNames.instance_path}\"')\n",
    "                return\n",
    "            else:\n",
    "                shutil.copytree(ShareNames.model_base_path , ShareNames.instance_path)\n",
    "                \n",
    "        logger.warning('Copy from model to instance finished , Start going forward')\n",
    "        self.InstanceStart()\n",
    "        for model_date , model_num in self.ModelIter():\n",
    "            self.model_date , self.model_num = model_date , model_num\n",
    "            self.ModelPreparation('instance')\n",
    "            self.TestModel()\n",
    "            self.StorePreds()\n",
    "        self.ModelResult()\n",
    "        logger.critical('Finish Process [Copy to Instance]! Cost {:.1f} Secs'.format(time.time() - self.instance_time))  \n",
    "        \n",
    "    def print_vars(self):\n",
    "        print(vars(self))\n",
    "\n",
    "    def ModelIter(self):\n",
    "        model_iter = itertools.product(ShareNames.model_date_list , ShareNames.model_num_list)\n",
    "        if ShareNames.resume_training and (ShareNames.process_name == 'train'):\n",
    "            models_trained = [os.path.exists(f'{ShareNames.model_base_path}/{mn}/{md}.pt') for md,mn in model_iter]\n",
    "            print(models_trained)\n",
    "            if models_trained[0] == 0: \n",
    "                models_trained[:] = False\n",
    "            else:\n",
    "                resume_point = -1 if all(models_trained) else (np.where(np.array(models_trained) == 0)[0][0] - 1)\n",
    "                models_trained[resume_point:] = False\n",
    "            model_iter = FilteredIterator(itertools.product(ShareNames.model_date_list , ShareNames.model_num_list), iter(models_trained == 0))\n",
    "        return model_iter\n",
    "    \n",
    "    def ModelPreparation(self , process , last_n = 30 , best_n = 5):\n",
    "        assert process in ['train' , 'test' , 'instance']\n",
    "        _start_time = time.time()\n",
    "        param = ShareNames.model_params[self.model_num]\n",
    "        \n",
    "        # variable updates for train_params\n",
    "        if process in ['train' , 'instance']:\n",
    "            if 'hidden_orthogonality' in self.f_penalty.keys(): self.f_penalty['hidden_orthogonality'][1] = 1 * (param.get('hidden_as_factors') == True)\n",
    "        \n",
    "        path_prefix = '{}/{}'.format(param.get('path') , self.model_date)\n",
    "        path = {k:f'{path_prefix}.{k}.pt' for k in ShareNames.output_types} #['best','swalast','swabest']\n",
    "        path.update({f'src_model.{k}':[] for k in ShareNames.output_types})\n",
    "        if 'swalast' in ShareNames.output_types: \n",
    "            path['lastn'] = [f'{path_prefix}.lastn.{i}.pt' for i in range(last_n)]\n",
    "        if 'swabest' in ShareNames.output_types: \n",
    "            path['bestn'] = [f'{path_prefix}.bestn.{i}.pt' for i in range(best_n)]\n",
    "            path['bestn_ic'] = [-10000. for i in range(best_n)]\n",
    "        \n",
    "        if ShareNames.train_params['transfer'] and self.model_date > ShareNames.model_date_list[0]:\n",
    "            path['transfer'] = '{}/{}.best.pt'.format(param.get('path') , max([d for d in ShareNames.model_date_list if d < self.model_date])) \n",
    "            \n",
    "        self.Param = param\n",
    "        self.path = path\n",
    "        self.time_recoder(_start_time , ['ModelPreparation' , process])\n",
    "    \n",
    "    def TrainModel(self):\n",
    "        self.TrainModelStart()\n",
    "        while self.cond.get('loop_status') != 'model':\n",
    "            self.NewLoop()\n",
    "            self.TrainerInit()\n",
    "            self.TrainEpoch()\n",
    "            self.LoopCondition()\n",
    "        self.TrainModelEnd()\n",
    "        gc.collect() , torch.cuda.empty_cache()\n",
    "    \n",
    "    def TestModel(self):\n",
    "        self.TestModelStart()\n",
    "        self.Forecast()\n",
    "        self.TestModelEnd()\n",
    "        gc.collect() , torch.cuda.empty_cache()\n",
    "        \n",
    "    def TrainModelStart(self):\n",
    "        \"\"\"\n",
    "        Reset model specific variables\n",
    "        \"\"\"\n",
    "        _start_time = time.time()\n",
    "        \n",
    "        self._init_variables('model')\n",
    "        self.nanloss_life = ShareNames.train_params['trainer']['nanloss']['retry']\n",
    "        \n",
    "        self.text['model'] = '{:s} #{:d} @{:4d}'.format(ShareNames.model_name , self.model_num , self.model_date)\n",
    "\n",
    "        if (self.data.dataloader_param != (self.model_date , self.Param['seqlens'])):\n",
    "            self.data.new_train_dataloader(self.model_date , self.Param['seqlens']) \n",
    "            self.time[1] = time.time()\n",
    "            self.printer('train_dataloader')\n",
    "            \n",
    "        self.time_recoder(_start_time , ['TrainModelStart'])\n",
    "            \n",
    "    def TrainModelEnd(self):\n",
    "        \"\"\"\n",
    "        Do necessary things of ending a model(model_data , model_num)\n",
    "        \"\"\"\n",
    "        _start_time = time.time()\n",
    "        \n",
    "        storage_model.del_path(self.path.get('rounds') , self.path.get('lastn') , self.path.get('bestn'))\n",
    "        if ShareNames.process_name == 'train' : self.model_count += 1\n",
    "        self.time[2] = time.time()\n",
    "        self.printer('model_end')\n",
    "        \n",
    "        self.time_recoder(_start_time , ['TrainModelEnd'])\n",
    "        \n",
    "    def NewLoop(self):\n",
    "        \"\"\"\n",
    "        Reset and loop variables giving loop_status\n",
    "        \"\"\"\n",
    "        _start_time = time.time()\n",
    "        \n",
    "        self._init_variables(self.cond.get('loop_status'))\n",
    "        self.epoch_i += 1\n",
    "        self.epoch_all += 1\n",
    "        if self.cond.get('loop_status') in ['attempt' , 'round']:\n",
    "            self.attempt_i += 1\n",
    "            self.text['attempt'] = f'FirstBite' if self.attempt_i == 0 else f'Retrain#{self.attempt_i}'\n",
    "        if self.cond.get('loop_status') in ['round']:\n",
    "            self.round_i += 1\n",
    "            self.text['round'] = 'Round{:2d}'.format(self.round_i)\n",
    "            \n",
    "        self.time_recoder(_start_time , ['NewLoop'])\n",
    "        \n",
    "    def TrainerInit(self):\n",
    "        \"\"\"\n",
    "        Initialize net , optimizer , scheduler if loop_status in ['round' , 'attempt']\n",
    "        net : 1. Create an instance of f'My{ShareNames.model_module}' or inherit from 'lastround'/'transfer'\n",
    "              2. In transfer mode , p_late and p_early with be trained with different lr's. If not net.parameters are trained by same lr\n",
    "        optimizer : Adam or SGD\n",
    "        scheduler : Cosine or StepLR\n",
    "        \"\"\"\n",
    "        _start_time = time.time()\n",
    "\n",
    "        if self.cond.get('loop_status') == 'epoch': return\n",
    "        self.net       = self.load_model('train')\n",
    "        self.max_round = self.net.max_round() if 'max_round' in self.net.__dir__() else 1\n",
    "        self.optimizer = self.load_optimizer()\n",
    "        self.scheduler = self.load_scheduler() \n",
    "        self.multiloss = self.load_multiloss()\n",
    "\n",
    "        self.time_recoder(_start_time , ['TrainerInit'])\n",
    "        \n",
    "    def TrainEpoch(self):\n",
    "        \"\"\"\n",
    "        Iterate train and valid dataset, calculate loss/metrics , update values\n",
    "        If nan loss occurs, turn to _deal_nanloss\n",
    "        \"\"\"\n",
    "        _start_time = time.time()\n",
    "        loss_train , loss_valid , ic_train , ic_valid = [] , [] , [] , []\n",
    "        clip_value = ShareNames.train_params['trainer']['gradient'].get('clip_value')\n",
    "        \n",
    "        if self.display.get('tqdm'):\n",
    "            iter_train , iter_valid = tqdm(self.data.dataloaders['train']) , tqdm(self.data.dataloaders['valid'])\n",
    "            disp_train = lambda x:iter_train.set_description(f'Ep#{self.epoch_i:3d} train loss:{np.mean(x):.5f}')\n",
    "            disp_valid = lambda x:iter_valid.set_description(f'Ep#{self.epoch_i:3d} valid ic:{np.mean(x):.5f}')\n",
    "        else:\n",
    "            iter_train , iter_valid = self.data.dataloaders['train'] , self.data.dataloaders['valid']\n",
    "            disp_train = disp_valid = lambda x:0\n",
    "\n",
    "        self.time_recoder(_start_time , ['TrainEpoch' , 'assign_loader'])\n",
    "        _start_time = time.time()\n",
    "\n",
    "        self.net.train()\n",
    "        _start_time_1 = time.time()\n",
    "        for i , (x , y) in enumerate(iter_train):\n",
    "            self.time_recoder(_start_time_1 , ['TrainEpoch' , 'train' , 'fetch'])\n",
    "            self.optimizer.zero_grad()\n",
    "            _start_time_1 = time.time()\n",
    "            pred , hidden = self.net(x)\n",
    "            self.time_recoder(_start_time_1 , ['TrainEpoch' , 'train' , 'forward'])\n",
    "            _start_time_1 = time.time()\n",
    "            loss , metric = self._loss_and_metric(y , pred , 'train' , hidden = hidden)\n",
    "            self.time_recoder(_start_time_1 , ['TrainEpoch' , 'train' , 'loss'])\n",
    "            _start_time_1 = time.time()\n",
    "            loss.backward()\n",
    "            self.time_recoder(_start_time_1 , ['TrainEpoch' , 'train' , 'backward'])\n",
    "            _start_time_1 = time.time()\n",
    "            if clip_value is not None : nn.utils.clip_grad_value_(self.net.parameters(), clip_value = clip_value)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            loss_train.append(loss.item()) , ic_train.append(metric)\n",
    "            disp_train(loss_train)\n",
    "            _start_time_1 = time.time()\n",
    "        if np.isnan(sum(loss_train)): return self._deal_nanloss()\n",
    "        self.loss_list['train'].append(np.mean(loss_train)) , self.ic_list['train'].append(np.mean(ic_train))\n",
    "        \n",
    "        self.time_recoder(_start_time , ['TrainEpoch' , 'train_epochs'])\n",
    "        _start_time = time.time()\n",
    "\n",
    "        self.net.eval()     \n",
    "        _start_time_1 = time.time()  \n",
    "        for i , (x , y) in enumerate(iter_valid):\n",
    "            # print(torch.cuda.memory_allocated(DEVICE) / 1024**3 , torch.cuda.memory_reserved(DEVICE) / 1024**3)\n",
    "            self.time_recoder(_start_time_1 , ['TrainEpoch' , 'valid' , 'fetch'])\n",
    "            _start_time_1 = time.time()\n",
    "            pred , _ = self.net(x)\n",
    "            self.time_recoder(_start_time_1 , ['TrainEpoch' , 'valid' , 'forward'])\n",
    "            _start_time_1 = time.time()\n",
    "            loss , metric = self._loss_and_metric(y , pred , 'valid')\n",
    "            self.time_recoder(_start_time_1 , ['TrainEpoch' , 'valid' , 'loss'])\n",
    "            _start_time_1 = time.time()\n",
    "            loss_valid.append(loss) , ic_valid.append(metric)\n",
    "            disp_valid(ic_valid)\n",
    "            _start_time_1 = time.time()\n",
    "        self.loss_list['valid'].append(np.mean(loss_valid)) , self.ic_list['valid'].append(np.mean(ic_valid))\n",
    "        self.lr_list.append(self.scheduler.get_last_lr()[0])\n",
    "        self.scheduler.step()\n",
    "        self.reset_scheduler()\n",
    "\n",
    "        self.time_recoder(_start_time , ['TrainEpoch' , 'valid_epochs'])\n",
    "\n",
    "    def LoopCondition(self):\n",
    "        \"\"\"\n",
    "        Update condition of continuing training epochs , restart attempt if early exit , proceed to next round if convergence , reset round if nan loss\n",
    "        \"\"\"\n",
    "        _start_time = time.time()\n",
    "\n",
    "        if self.cond['nan_loss']:\n",
    "            logger.error(f'Initialize a new model to retrain! Lives remaining {self.nanloss_life}')\n",
    "            self._init_variables('model')\n",
    "            self.cond['loop_status'] = 'round'\n",
    "            return\n",
    "            \n",
    "        valid_ic = self.ic_list['valid'][-1]\n",
    "        \n",
    "        save_targets = [] \n",
    "        if valid_ic > self.ic_attempt_best: \n",
    "            self.epoch_attempt_best  = self.epoch_i \n",
    "            self.ic_attempt_best = valid_ic\n",
    "            \n",
    "        if valid_ic > self.ic_round_best:\n",
    "            self.ic_round_best = valid_ic\n",
    "            self.path['src_model.best']  = [self.path['best']]\n",
    "            save_targets.append(self.path['best'])\n",
    "\n",
    "        if 'swalast' in ShareNames.output_types:\n",
    "            self.path['lastn'] = self.path['lastn'][1:] + self.path['lastn'][:1]\n",
    "            save_targets.append(self.path['lastn'][-1])\n",
    "            \n",
    "            p_valid = self.path['lastn'][-len(self.ic_list['valid']):]\n",
    "            arg_max = np.argmax(self.ic_list['valid'][-len(p_valid):])\n",
    "            arg_swa = (lambda x:x[(x>=0) & (x<len(p_valid))])(min(5,len(p_valid)//3)*np.arange(-5,3)+arg_max)[-5:]\n",
    "            self.path['src_model.swalast'] = [p_valid[i] for i in arg_swa]\n",
    "            \n",
    "        if 'swabest' in ShareNames.output_types:\n",
    "            arg_min = np.argmin(self.path['bestn_ic'])\n",
    "            if valid_ic > self.path['bestn_ic'][arg_min]:\n",
    "                self.path['bestn_ic'][arg_min] = valid_ic\n",
    "                save_targets.append(self.path['bestn'][arg_min])\n",
    "                if self.path['bestn'][arg_min] not in self.path['src_model.swabest']: self.path['src_model.swabest'].append(self.path['bestn'][arg_min])\n",
    "            \n",
    "        storage_model.save_model_state(self.net , save_targets)\n",
    "        self.printer('epoch_step')\n",
    "        self.time_recoder(_start_time , ['LoopCondition' , 'assess'])\n",
    "        _start_time = time.time()\n",
    "        \n",
    "        self.cond['terminate'] = {k:self._terminate_cond(k,v) for k , v in ShareNames.train_params['terminate'].get('overall' if self.max_round <= 1 else 'round').items()}\n",
    "        if any(self.cond.get('terminate').values()):\n",
    "            self.text['exit'] = {\n",
    "                'max_epoch'      : 'Max Epoch' , \n",
    "                'early_stop'     : 'EarlyStop' ,\n",
    "                'tv_converge'    : 'T&V Convg' , \n",
    "                'train_converge' : 'Tra Convg' , \n",
    "                'valid_converge' : 'Val Convg' ,\n",
    "            }[[k for k,v in self.cond.get('terminate').items() if v][0]] \n",
    "            if (self.epoch_i < ShareNames.train_params['trainer']['retrain'].get('min_epoch' if self.max_round <= 1 else 'min_epoch_round') - 1 and \n",
    "                self.attempt_i < ShareNames.train_params['trainer']['retrain']['attempts'] - 1):\n",
    "                self.cond['loop_status'] = 'attempt'\n",
    "                self.printer('new_attempt')\n",
    "            elif self.round_i < self.max_round - 1:\n",
    "                self.cond['loop_status'] = 'round'\n",
    "                self.save_model('best')\n",
    "                self.printer('new_round')\n",
    "            else:\n",
    "                self.cond['loop_status'] = 'model'\n",
    "                self.save_model(ShareNames.output_types)\n",
    "        else:\n",
    "            self.cond['loop_status'] = 'epoch'\n",
    "\n",
    "        _start_time = time.time()\n",
    "        self.time_recoder(_start_time , ['LoopCondition' , 'confirm_status'])\n",
    "        \n",
    "            \n",
    "    def TestModelStart(self):\n",
    "        \"\"\"\n",
    "        Reset model specific variables\n",
    "        \"\"\"\n",
    "        self._init_variables('model')        \n",
    "        if (self.data.dataloader_param != (self.model_date , self.Param['seqlens'])):\n",
    "            self.data.new_test_dataloader(self.model_date , self.Param['seqlens'])\n",
    "            \n",
    "        if self.model_num == 0:\n",
    "            ic_date_0 = np.zeros((len(self.data.model_test_dates) , len(self.test_result_model_num)))\n",
    "            ic_model_0 =  np.zeros((1 , len(self.test_result_model_num)))\n",
    "            self.ic_by_date = ic_date_0 if self.ic_by_date is None else np.concatenate([self.ic_by_date , ic_date_0])\n",
    "            self.ic_by_model = ic_model_0 if self.ic_by_model is None else np.concatenate([self.ic_by_model , ic_model_0])\n",
    "                \n",
    "    def Forecast(self):\n",
    "        if not os.path.exists(self.path['best']): self.TrainModel()\n",
    "        \n",
    "        #self.y_pred = cuda(torch.zeros(self.data.stock_n,len(self.data.model_test_dates),self.data.labels_n,len(ShareNames.output_types)).fill_(np.nan))\n",
    "        self.y_pred = cuda(torch.zeros(self.data.stock_n,len(self.data.model_test_dates),len(ShareNames.output_types)).fill_(np.nan))\n",
    "        for oi , okey in enumerate(ShareNames.output_types):\n",
    "            self.net = self.load_model('test' , okey)\n",
    "            self.net.eval()\n",
    "\n",
    "            if self.display.get('tqdm'):\n",
    "                iter_test = tqdm(self.data.dataloaders['test'])\n",
    "                disp_test = lambda x:iter_test.set_description(f'Date#{x[0]:3d} :{np.mean(x[1]):.5f}')\n",
    "            else:\n",
    "                iter_test = self.data.dataloaders['test']\n",
    "                disp_test = lambda x:0\n",
    "\n",
    "            m_test = []         \n",
    "            with torch.no_grad():\n",
    "                for i , (x , y) in enumerate(iter_test):\n",
    "                    stock_pos = np.where(self.data.test_nonnan_sample[:,i])[0]\n",
    "                    for batch_j in torch.utils.data.DataLoader(np.arange(len(y)) , batch_size = ShareNames.batch_size):\n",
    "                        x_j = tuple([xx[batch_j] for xx in x]) if isinstance(x , tuple) else x[batch_j]\n",
    "                        output , _ = self.net(x_j)\n",
    "                        self.y_pred[stock_pos[batch_j],i,oi] = output.select(-1,0).detach()\n",
    "                    metric = self.f_metric(y.select(-1,0) , self.y_pred[stock_pos,i,oi]).item()\n",
    "                    if (i + 1) % 20 == 0 : torch.cuda.empty_cache()\n",
    "                    m_test.append(metric) \n",
    "                    disp_test((i , m_test))\n",
    "                    \n",
    "            self.ic_by_date[-len(self.data.model_test_dates):,self.model_num*len(ShareNames.output_types) + oi] = torch.tensor(m_test).nan_to_num(0).cpu().numpy()   \n",
    "        self.y_pred = self.y_pred.cpu().numpy()\n",
    "        \n",
    "    def TestModelEnd(self):\n",
    "        \"\"\"\n",
    "        Do necessary things of ending a model(model_data , model_num)\n",
    "        \"\"\"\n",
    "        if self.model_num == ShareNames.model_num_list[-1]:\n",
    "            self.ic_by_model[-1,:] = np.nanmean(self.ic_by_date[-len(self.data.model_test_dates):,],axis = 0)\n",
    "            logger.info('{: <11d}'.format(self.model_date)+('{:>8.4f}'*len(self.test_result_model_num)).format(*self.ic_by_model[-1,:]))\n",
    "        #if False:\n",
    "        #    df = pd.DataFrame(self.y_pred.T, index = self.data.model_test_dates, columns = self.data.index_stock.astype(str))\n",
    "        #    with open(f'{ShareNames.instance_path}/{ShareNames.model_name}_fac{self.model_num}.csv', 'a') as f:\n",
    "        #        df.to_csv(f , mode = 'a', header = f.tell()==0, index = True)\n",
    "        \n",
    "    def StorePreds(self):\n",
    "        assert ShareNames.process_name == 'instance'\n",
    "        if self.model_num == 0:\n",
    "            self.y_pred_models = []\n",
    "            gc.collect()\n",
    "        self.y_pred_models.append(self.y_pred)\n",
    "        if self.model_num == ShareNames.model_num_list[-1]:\n",
    "            self.y_pred_models = np.concatenate(self.y_pred_models,axis=-1).transpose(1,0,2)\n",
    "            # idx = np.array(np.meshgrid(self.data.model_test_dates , self.data.index_stock)).T.reshape(-1,2)\n",
    "            mode = 'r+' if os.path.exists(f'{ShareNames.instance_path}/{ShareNames.model_name}.h5') else 'w'\n",
    "            with h5py.File(f'{ShareNames.instance_path}/{ShareNames.model_name}.h5' , mode = mode) as f:\n",
    "                for di in range(len(self.data.model_test_dates)):\n",
    "                    arr , row = self.y_pred_models[di] , self.data.index_stock \n",
    "                    arr , row = arr[np.isnan(arr).all(axis=1) == 0] , row[np.isnan(arr).all(axis=1) == 0]\n",
    "                    col = [f'{mn}.{o}' for mn,o in zip(self.test_result_model_num,self.test_result_output_type)]\n",
    "                    if str(self.data.model_test_dates[di]) in f.keys():\n",
    "                        del f[str(self.data.model_test_dates[di])]\n",
    "                    g = f.create_group(str(self.data.model_test_dates[di]))\n",
    "                    g.create_dataset('arr' , data=arr , compression='gzip')\n",
    "                    g.create_dataset('row' , data=row , compression='gzip')\n",
    "                    g.create_dataset('col' , data=col , compression='gzip')     \n",
    "  \n",
    "    def ModelResult(self):\n",
    "        # date ic writed down\n",
    "        _step = (1 if ShareNames.process_name == 'instance' else self.data.test_step)\n",
    "        _dates_list = ShareNames.test_full_dates[::_step]\n",
    "        for model_num in ShareNames.model_num_list:\n",
    "            df = {'dates' : _dates_list}\n",
    "            for oi , okey in enumerate(ShareNames.output_types):\n",
    "                df.update({f'rank_ic.{okey}' : self.ic_by_date[:,model_num*len(ShareNames.output_types) + oi], \n",
    "                           f'cum_ic.{okey}' : np.nancumsum(self.ic_by_date[:,model_num*len(ShareNames.output_types) + oi])})\n",
    "            df = pd.DataFrame(df , index = map(lambda x:f'{x[:4]}-{x[4:6]}-{x[6:]}' , _dates_list.astype(str)))\n",
    "            df.to_csv(ShareNames.model_params[model_num]['path'] + f'/{ShareNames.model_name}_ic_by_date_{model_num}.csv')\n",
    "\n",
    "        # model ic presentation\n",
    "        add_row_key   = ['AllTimeAvg' , 'AllTimeSum' , 'Std'      , 'TValue'   , 'AnnIR']\n",
    "        add_row_fmt   = ['{:>8.4f}'   , '{:>8.2f}'   , '{:>8.4f}' , '{:>8.2f}' , '{:>8.4f}']\n",
    "        ic_mean   = np.nanmean(self.ic_by_date , axis = 0)\n",
    "        ic_sum    = np.nansum(self.ic_by_date , axis = 0) \n",
    "        ic_std    = np.nanstd(self.ic_by_date , axis = 0)\n",
    "        ic_tvalue = ic_mean / ic_std * (len(self.ic_by_date)**0.5) # 10 days return predicted\n",
    "        ic_annir  = ic_mean / ic_std * ((240 / 10)**0.5) # 10 days return predicted\n",
    "        add_row_value = (ic_mean , ic_sum , ic_std , ic_tvalue , ic_annir)\n",
    "        df = pd.DataFrame(np.concatenate([self.ic_by_model , np.stack(add_row_value)]) , \n",
    "                          index = [str(d) for d in ShareNames.model_date_list] + add_row_key , \n",
    "                          columns = [f'{mn}.{o}' for mn,o in zip(self.test_result_model_num,self.test_result_output_type)])\n",
    "        df.to_csv(f'{ShareNames.model_base_path}/{ShareNames.model_name}_ic_by_model.csv')\n",
    "        for i in range(len(add_row_key)):\n",
    "            logger.info('{: <11s}'.format(add_row_key[i]) + (add_row_fmt[i]*len(self.test_result_model_num)).format(*add_row_value[i]))\n",
    "    \n",
    "    def InstanceStart(self):\n",
    "        exec(open(f'{ShareNames.instance_path}/globalvars.py').read())\n",
    "        self.shared_ctrl.assign_variables()\n",
    "        for mm in range(len(ShareNames.model_params)): ShareNames.model_params[mm].update({'path':f'{ShareNames.instance_path}/{mm}'})\n",
    "    \n",
    "    def printer(self , key):\n",
    "        \"\"\"\n",
    "        Print out status giving display conditions and looping conditions\n",
    "        \"\"\"\n",
    "        _detail_print = (self.display.get('once') == 0 or self.model_count <= max(ShareNames.model_num_list))\n",
    "        if key == 'model_specifics':\n",
    "            logger.warning('Model Parameters:')\n",
    "            logger.info(f'Basic Parameters : ')\n",
    "            print(f'STORAGE [{config[\"STORAGE_TYPE\"]}] | DEVICE [{DEVICE}] | PRECISION [{ShareNames.precision}] | BATCH_SIZE [{ShareNames.batch_size}].') \n",
    "            print(f'NAME [{ShareNames.model_name}] | MODULE [{ShareNames.model_module}] | DATATYPE [{ShareNames.model_data_type}] | MODEL_NUM [{len(ShareNames.model_num_list)}].')\n",
    "            print(f'BEG_DATE [{config[\"BEG_DATE\"]}] | END_DATE [{ShareNames.test_full_dates[-1]}] | ' +\n",
    "                  f'INTERVAL [{config[\"INTERVAL\"]}] | INPUT_STEP_DAY [{config[\"INPUT_STEP_DAY\"]}] | TEST_STEP_DAY [{config[\"TEST_STEP_DAY\"]}].') \n",
    "            logger.info(f'MODEL_PARAM : ')\n",
    "            pretty_print_dict(ShareNames.raw_model_params)\n",
    "            logger.info(f'TRAIN_PARAM : ')\n",
    "            pretty_print_dict(ShareNames.train_params)\n",
    "            logger.info(f'COMPT_PARAM : ')\n",
    "            pretty_print_dict(ShareNames.compt_params)\n",
    "        elif key == 'model_end':\n",
    "            self.text['epoch'] = 'Ep#{:3d}'.format(self.epoch_all)\n",
    "            self.text['stat']  = 'Train{: .4f} Valid{: .4f} BestVal{: .4f}'.format(self.ic_list['train'][-1],self.ic_list['valid'][-1],self.ic_round_best)\n",
    "            self.text['time']  = 'Cost{:5.1f}Min,{:5.1f}Sec/Ep'.format((self.time[2]-self.time[0])/60 , (self.time[2]-self.time[1])/(self.epoch_all+1))\n",
    "            sdout = self.text['model'] + '|' + self.text['round'] + ' ' + self.text['attempt'] + ' ' +\\\n",
    "            self.text['epoch'] + ' ' + self.text['exit'] + '|' + self.text['stat'] + '|' + self.text['time']\n",
    "            logger.warning(sdout)\n",
    "        elif key == 'epoch_step':\n",
    "            self.text['trainer'] = 'loss {: .5f}, train{: .5f}, valid{: .5f}, max{: .4f}, best{: .4f}, lr{:.1e}'.format(\n",
    "                self.loss_list['train'][-1] , self.ic_list['train'][-1] , self.ic_list['valid'][-1] , self.ic_attempt_best , self.ic_round_best , self.lr_list[-1])\n",
    "            if self.epoch_i % self.display.get('step') == 0:\n",
    "                sdout = ' '.join([self.text['attempt'],'Ep#{:3d}'.format(self.epoch_i),':', self.text['trainer']])\n",
    "                logger.info(sdout) if _detail_print else logger.debug(sdout) \n",
    "        elif key == 'reset_learn_rate':\n",
    "            speedup = ShareNames.train_params['trainer']['learn_rate']['reset']['speedup2x']\n",
    "            sdout = 'Reset learn rate and scheduler at the end of epoch {} , effective at epoch {}'.format(self.epoch_i , self.epoch_i+1 , ', and will speedup2x' * speedup)\n",
    "            logger.info(sdout) if _detail_print else logger.debug(sdout) \n",
    "        elif key == 'new_attempt':\n",
    "            sdout = ' '.join([self.text['attempt'],'Epoch #{:3d}'.format(self.epoch_i),':',self.text['trainer'],', Next attempt goes!'])\n",
    "            logger.info(sdout) if _detail_print else logger.debug(sdout) \n",
    "        elif key == 'new_round':\n",
    "            sdout = self.text['round'] + ' ' + self.text['exit'] + ': ' + self.text['trainer'] + ', Next round goes!'\n",
    "            logger.info(sdout) if _detail_print else logger.debug(sdout)\n",
    "        elif key == 'train_dataloader':\n",
    "            sdout = ' '.join([self.text['model'],'LoadData Cost {:>6.1f}Secs'.format(self.time[1]-self.time[0])])  \n",
    "            logger.info(sdout) if _detail_print else logger.debug(sdout)\n",
    "        else:\n",
    "            raise Exception(f'KeyError : {key}')        \n",
    "            \n",
    "    def _init_variables(self , key = 'model'):\n",
    "        \"\"\"\n",
    "        Reset variables of 'model' , 'round' , 'attempt' start\n",
    "        \"\"\"\n",
    "        if key == 'epoch' : return\n",
    "        assert key in ['model' , 'round' , 'attempt'] , f'KeyError : {key}'\n",
    "\n",
    "        self.epoch_i = -1\n",
    "        self.epoch_attempt_best = -1\n",
    "        self.ic_attempt_best = -10000.\n",
    "        self.loss_list = {'train' : [] , 'valid' : []}\n",
    "        self.ic_list   = {'train' : [] , 'valid' : []}\n",
    "        self.lr_list   = []\n",
    "        \n",
    "        if key in ['model' , 'round']:\n",
    "            self.attempt_i = -1\n",
    "            self.ic_round_best = -10000.\n",
    "        \n",
    "        if key in ['model']:\n",
    "            self.round_i = -1\n",
    "            self.epoch_all = -1\n",
    "            self.time = np.ones(10) * time.time()\n",
    "            self.text = {k : '' for k in ['model','round','attempt','epoch','exit','stat','time','trainer']}\n",
    "            self.cond = {'terminate' : {} , 'nan_loss' : False , 'loop_status' : 'round'}\n",
    "            \n",
    "    def _loss_and_metric(self, labels , pred , key , **kwargs):\n",
    "        \"\"\"\n",
    "        Calculate loss(with gradient), metric\n",
    "        Inputs : \n",
    "            cal_options : 'l'for loss , 'm' as metric , 'p' for penalty (add to l) , (1,1,1) as default\n",
    "            kwargs : other inputs used in calculating loss , penalty and metric\n",
    "        Possible Methods :\n",
    "        loss:    pearsonr , mse , ccc\n",
    "        penalty: none , hidden_orthogonality\n",
    "        metric:  pearsonr , rankic , mse , ccc\n",
    "        \"\"\"\n",
    "        assert key in ['train' , 'valid'] , key\n",
    "        if labels.shape != pred.shape:\n",
    "            # if more labels than output\n",
    "            assert labels.shape[:-1] == pred.shape[:-1] , (labels.shape , pred.shape)\n",
    "            labels = labels.transpose(0,-1)[:pred.shape[-1]].transpose(0,-1)\n",
    "            \n",
    "        if key == 'train':\n",
    "            if self.Param['num_output'] > 1:\n",
    "                loss = self.f_loss(labels , pred , dim = 0)[:self.Param['num_output']]\n",
    "                loss = self.multiloss.calculate_multi_loss(loss , self.net.get_multiloss_params())\n",
    "            else:\n",
    "                loss    = self.f_loss(labels.select(-1,0) , pred.select(-1,0))\n",
    "            metric  = self.f_metric(labels.select(-1,0) , pred.select(-1,0)).item()\n",
    "            penalty = sum([w * f(**kwargs) for k,(f,w) in self.f_penalty.items()])\n",
    "            loss = loss + penalty  \n",
    "        else:\n",
    "            metric  = self.f_metric(labels.select(-1,0) , pred.select(-1,0)).item()\n",
    "            loss    = 0.\n",
    "        return loss , metric\n",
    "    \n",
    "    def _deal_nanloss(self):\n",
    "        \"\"\"\n",
    "        Deal with nan loss, life -1 and change nan_loss condition to True\n",
    "        \"\"\"\n",
    "        logger.error(f'{self.text[\"model\"]} Attempt{self.attempt_i}, epoch{self.epoch_i} got nan loss!')\n",
    "        if self.nanloss_life > 0:\n",
    "            self.nanloss_life -= 1\n",
    "            self.cond['nan_loss'] = True\n",
    "        else:\n",
    "            raise Exception('Nan loss life exhausted, possible gradient explosion/vanish!')\n",
    "    \n",
    "    def _terminate_cond(self , key , arg):\n",
    "        \"\"\"\n",
    "        Whether terminate condition meets\n",
    "        \"\"\"\n",
    "        if key == 'early_stop':\n",
    "            return self.epoch_i - self.epoch_attempt_best >= arg\n",
    "        elif key == 'train_converge':\n",
    "            return list_converge(self.loss_list['train'] , arg.get('min_epoch') , arg.get('eps'))\n",
    "        elif key == 'valid_converge':\n",
    "            return list_converge(self.ic_list['valid'] , arg.get('min_epoch') , arg.get('eps'))\n",
    "        elif key == 'tv_converge':\n",
    "            return (list_converge(self.loss_list['train'] , arg.get('min_epoch') , arg.get('eps')) and\n",
    "                    list_converge(self.ic_list['valid'] , arg.get('min_epoch') , arg.get('eps')))\n",
    "        elif key == 'max_epoch':\n",
    "            return self.epoch_i >= min(arg , ShareNames.max_epoch) - 1\n",
    "        else:\n",
    "            raise Exception(f'KeyError : {key}')\n",
    "    \n",
    "    def save_model(self , key = 'best'):\n",
    "        assert isinstance(key , (list,tuple,str))\n",
    "        _start_time = time.time()\n",
    "        if isinstance(key , (list,tuple)):\n",
    "            [self.save_model(k) for k in key]\n",
    "        else:\n",
    "            assert key in ['best' , 'swalast' , 'swabest']\n",
    "            if key == 'best':\n",
    "                model_state = storage_model.load(self.path['best'])\n",
    "                if self.round_i < self.max_round - 1:\n",
    "                    if 'rounds' not in self.path.keys():\n",
    "                        self.path['rounds'] = ['{}/{}.round.{}.pt'.format(self.Param.get('path') , self.model_date , r) for r in range(self.max_round - 1)]\n",
    "                    # self.path[f'round.{self.round_i}'] = '{}/{}.round.{}.pt'.format(self.Param.get('path') , self.model_date , self.round_i)\n",
    "                    storage_model.save(model_state , self.path['rounds'][self.round_i])\n",
    "                storage_model.save(model_state , self.path['best'] , to_disk = True)\n",
    "            else:\n",
    "                p_exists = storage_model.valid_paths(self.path[f'src_model.{key}'])\n",
    "                if len(p_exists) == 0:\n",
    "                    print(key , self.path[f'bestn'] , self.path[f'bestn_ic'] , self.path[f'src_model.{key}'])\n",
    "                    raise Exception(f'Model Error')\n",
    "                else:\n",
    "                    model = self.swa_model(p_exists)\n",
    "                    storage_model.save_model_state(model , self.path[key] , to_disk = True) \n",
    "        self.time_recoder(_start_time , ['save_model'])\n",
    "    \n",
    "    def load_model(self , process , key = 'best'):\n",
    "        assert process in ['train' , 'test']\n",
    "        _start_time = time.time()\n",
    "        net = globals()[f'My{ShareNames.model_module}'](**self.Param)\n",
    "        if process == 'train':           \n",
    "            if self.round_i > 0:\n",
    "                model_path = self.path['rounds'][self.round_i-1]\n",
    "            elif 'transfer' in self.path.keys():\n",
    "                model_path = self.path['transfer']\n",
    "            else:\n",
    "                model_path = -1\n",
    "            if os.path.exists(model_path): net = storage_model.load_model_state(net , model_path , from_disk = True)\n",
    "            if 'training_round' in net.__dir__(): net.training_round(self.round_i)\n",
    "        else:\n",
    "            net = storage_model.load_model_state(net , self.path[key] , from_disk = True)\n",
    "        net = cuda(net)\n",
    "        self.time_recoder(_start_time , ['load_model'])\n",
    "        return net\n",
    "    \n",
    "    def swa_model(self , model_path_list = []):\n",
    "        net = globals()[f'My{ShareNames.model_module}'](**self.Param)\n",
    "        swa_net = AveragedModel(net)\n",
    "        for p in model_path_list:\n",
    "            swa_net.update_parameters(storage_model.load_model_state(net , p))\n",
    "        swa_net = cuda(swa_net)\n",
    "        update_bn(self.data.dataloaders['train'] , swa_net)\n",
    "        return swa_net.module\n",
    "    \n",
    "    def load_optimizer(self , new_opt_kwargs = None , new_lr_kwargs = None):\n",
    "        if new_opt_kwargs is None:\n",
    "            opt_kwargs = ShareNames.train_params['trainer']['optimizer']\n",
    "        else:\n",
    "            opt_kwargs = deepcopy(ShareNames.train_params['trainer']['optimizer'])\n",
    "            opt_kwargs.update(new_opt_kwargs)\n",
    "        \n",
    "        if new_lr_kwargs is None:\n",
    "            lr_kwargs = ShareNames.train_params['trainer']['learn_rate']\n",
    "        else:\n",
    "            lr_kwargs = deepcopy(ShareNames.train_params['trainer']['learn_rate'])\n",
    "            lr_kwargs.update(new_lr_kwargs)\n",
    "\n",
    "        base_lr = lr_kwargs['base'] * lr_kwargs['ratio']['attempt'][:self.attempt_i+1][-1] * lr_kwargs['ratio']['round'][:self.round_i+1][-1]\n",
    "        if 'transfer' in self.path.keys():\n",
    "            # define param list to train with different learn rate\n",
    "            p_enc = [(p if p.dim()<=1 else nn.init.xavier_uniform_(p)) for x,p in self.net.named_parameters() if 'encoder' in x.split('.')[:3]]\n",
    "            p_dec = [p for x,p in self.net.named_parameters() if 'encoder' not in x.split('.')[:3]]\n",
    "            self.net_param_gourps = [{'params': p_dec , 'lr': base_lr , 'lr_param' : base_lr},\n",
    "                                     {'params': p_enc , 'lr': base_lr * lr_kwargs['ratio']['transfer'] , 'lr_param': base_lr * lr_kwargs['ratio']['transfer']}]\n",
    "        else:\n",
    "            self.net_param_gourps = [{'params': [p for p in self.net.parameters()] , 'lr' : base_lr , 'lr_param' : base_lr} ]\n",
    "\n",
    "        optimizer = {\n",
    "            'Adam': torch.optim.Adam ,\n",
    "            'SGD' : torch.optim.SGD ,\n",
    "        }[opt_kwargs['name']](self.net_param_gourps , **opt_kwargs['param'])\n",
    "        return optimizer\n",
    "    \n",
    "    def load_scheduler(self , new_shd_kwargs = None):\n",
    "        if new_shd_kwargs is None:\n",
    "            shd_kwargs = ShareNames.train_params['trainer']['scheduler']\n",
    "        else:\n",
    "            shd_kwargs = deepcopy(ShareNames.train_params['trainer']['scheduler'])\n",
    "            shd_kwargs.update(new_shd_kwargs)\n",
    "\n",
    "        if shd_kwargs['name'] == 'cos':\n",
    "            scheduler = lr_cosine_scheduler(self.optimizer, **shd_kwargs['param'])\n",
    "        elif shd_kwargs['name'] == 'step':\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, **shd_kwargs['param'])\n",
    "        elif shd_kwargs['name'] == 'cycle':\n",
    "            scheduler = torch.optim.lr_scheduler.CyclicLR(self.optimizer, max_lr=[pg['lr_param'] for pg in self.optimizer.param_groups],cycle_momentum=False,mode='triangular2',**shd_kwargs['param'])\n",
    "\n",
    "        return scheduler\n",
    "    \n",
    "    def reset_scheduler(self):\n",
    "        rst_kwargs = ShareNames.train_params['trainer']['learn_rate']['reset']\n",
    "        if rst_kwargs['num_reset'] <= 0 or (self.epoch_i + 1) < rst_kwargs['trigger']: return\n",
    "\n",
    "        trigger_intvl = rst_kwargs['trigger'] // 2 if rst_kwargs['speedup2x'] else rst_kwargs['trigger']\n",
    "        if (self.epoch_i + 1 - rst_kwargs['trigger']) % trigger_intvl != 0: return\n",
    "        \n",
    "        trigger_times = ((self.epoch_i + 1 - rst_kwargs['trigger']) // trigger_intvl) + 1\n",
    "        if trigger_times > rst_kwargs['num_reset']: return\n",
    "        \n",
    "        # confirm reset : change back optimizor learn rate\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr_param']  * rst_kwargs['recover_level']\n",
    "        \n",
    "        # confirm reset : reassign scheduler\n",
    "        if rst_kwargs['speedup2x']:\n",
    "            shd_kwargs = deepcopy(ShareNames.train_params['trainer']['scheduler'])\n",
    "            for k in np.intersect1d(list(shd_kwargs['param'].keys()),['step_size' , 'warmup_stage' , 'anneal_stage' , 'step_size_up' , 'step_size_down']):\n",
    "                shd_kwargs['param'][k] //= 2\n",
    "        else:\n",
    "            shd_kwargs = None\n",
    "        self.scheduler = self.load_scheduler(shd_kwargs)\n",
    "        self.printer('reset_learn_rate')\n",
    "        \n",
    "    def load_multiloss(self):\n",
    "        multiloss = None\n",
    "        if self.Param['num_output'] > 1:\n",
    "            multiloss = multiloss_calculator(multi_type = ShareNames.train_params['multitask']['type'])\n",
    "            multiloss.reset_multi_type(self.Param['num_output'] , **ShareNames.train_params['multitask']['param_dict'][multiloss.multi_type])\n",
    "        return multiloss\n",
    "\n",
    "    def time_recoder(self , start_time , keys , init_length = 100):\n",
    "        if TIME_RECODER:\n",
    "            if isinstance(keys , (list , tuple)): k = '/'.join(keys)\n",
    "            if self.process_time.get(k) is None: \n",
    "                self.process_time[k] = {\n",
    "                    'value' : np.zeros(init_length) , \n",
    "                    'index' : -1 , \n",
    "                    'length' : init_length , \n",
    "                }\n",
    "            d = self.process_time[k]\n",
    "            d['index'] += 1\n",
    "            if d['length'] <= d['index']: \n",
    "                d['value'] = np.append(d['value'] , np.zeros(init_length))\n",
    "                d['length'] += init_length\n",
    "            d['value'][d['index']] = time.time() - start_time\n",
    "    \n",
    "    def print_time_recorder(self):\n",
    "        if TIME_RECODER:\n",
    "            keys = list(self.process_time.keys())\n",
    "            num_calls = [self.process_time[k]['index']+1 for k in keys]\n",
    "            total_time = [self.process_time[k]['value'].sum() for k in keys]\n",
    "            tb = pd.DataFrame({'keys':keys , 'num_calls': num_calls, 'total_time': total_time})\n",
    "            tb['avg_time'] = tb['total_time'] / tb['num_calls']\n",
    "            print(tb.sort_values(by=['total_time'],ascending=False))\n",
    "                \n",
    "def cuda(x):\n",
    "    if isinstance(x , (list,tuple)):\n",
    "        return type(x)(map(cuda , x))\n",
    "    else:\n",
    "        return x.to(DEVICE)\n",
    "    \n",
    "class FilteredIterator:\n",
    "    def __init__(self, iterable, condition):\n",
    "        self.iterable = iterable\n",
    "        self.condition = condition\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            item = next(self.iterable)\n",
    "            cond = self.condition(item) if callable(self.condition) else next(self.condition)\n",
    "            if cond: return item\n",
    "\n",
    "def loss_function(key):\n",
    "    \"\"\"\n",
    "    loss function , metric should * -1.\n",
    "    \"\"\"\n",
    "    assert key in ('mse' , 'pearson' , 'ccc')\n",
    "    def decorator(func , key):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            v = func(*args, **kwargs)\n",
    "            if key != 'mse':  \n",
    "                v = torch.exp(-v)\n",
    "            return v\n",
    "        return wrapper\n",
    "    func = globals()[key]\n",
    "    return decorator(func , key)\n",
    "\n",
    "def metric_function(key):\n",
    "    assert key in ('mse' , 'pearson' , 'ccc' , 'spearman')\n",
    "    def decorator(func , key , item_only = False):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            with torch.no_grad():\n",
    "                v = func(*args, **kwargs)\n",
    "            if key == 'mse' : v = -v\n",
    "            return v\n",
    "        return wrapper\n",
    "    func = globals()[key]\n",
    "    return decorator(globals()[key] , key)\n",
    "    \n",
    "def penalty_function(key):\n",
    "    _cat_tensor = lambda x:(torch.cat(x,dim=-1) if isinstance(x,(tuple,list)) else x)\n",
    "    def _none(**kwargs):\n",
    "        return 0.\n",
    "    def _hidden_orthogonality(**kwargs):\n",
    "        _cat_tensor = lambda x:(torch.cat(x,dim=-1) if isinstance(x,(tuple,list)) else x)\n",
    "        return _cat_tensor(kwargs.get('hidden')).T.corrcoef().triu(1).nan_to_num().square().sum()\n",
    "    return locals()[f'_{key}']\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='manual to this script')\n",
    "    parser.add_argument(\"--process\",     type=int, default=1)\n",
    "    parser.add_argument(\"--rawname\",     type=int, default=1)\n",
    "    parser.add_argument(\"--resume\",      type=int, default=0)\n",
    "    parser.add_argument(\"--anchoring\",   type=int, default=0)\n",
    "    ShareNames = parser.parse_args([])\n",
    "\n",
    "    Controller = model_controller()\n",
    "    Controller.main_process()\n",
    "    Controller.print_time_recorder()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
