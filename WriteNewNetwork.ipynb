{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  2 DataBlocks...... finished! Cost 0.08 secs\n",
      "Align 2 DataBlocks...... finished! Cost 0.17 secs\n",
      "Pre-Norming method of [day] : {'divlast': True, 'histnorm': True}\n"
     ]
    }
   ],
   "source": [
    "from src.api import DataAPI\n",
    "batch_data = DataAPI.get_realistic_batch_data('day')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5057, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.nn_model.nn import layer as Layer\n",
    "\n",
    "class ple_gru(nn.Module):\n",
    "    '''\n",
    "    Progressive Layered Extraction\n",
    "    '''\n",
    "    def __init__(\n",
    "            self, \n",
    "            input_dim    = 6 ,\n",
    "            hidden_dim   = 2**5,\n",
    "            dropout      = 0.1,\n",
    "            act_type     = 'leaky',\n",
    "            expert_layers = 2 ,\n",
    "            rnn_layers   = 2 ,\n",
    "            num_output   = 2 ,\n",
    "            **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        assert num_output > 1 and num_output < 5 , num_output\n",
    "        assert expert_layers >= 2 , expert_layers\n",
    "        \n",
    "        self.expert_layers = expert_layers\n",
    "        self.num_output = num_output\n",
    "\n",
    "        layers = []\n",
    "        for i in range(expert_layers):\n",
    "            layers.append(ExpertLayer(\n",
    "                input_dim = input_dim , \n",
    "                hidden_dim = hidden_dim , \n",
    "                first_layer = i == 0 , \n",
    "                dropout = dropout , \n",
    "                rnn_layers = rnn_layers , \n",
    "                num_output = num_output))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                Layer.Act.get_activation_fn(act_type),\n",
    "                nn.Linear(hidden_dim , 1) , \n",
    "                nn.BatchNorm1d(1)\n",
    "            ) for _ in range(num_output)\n",
    "        ])\n",
    "\n",
    "    def forward(self , x):\n",
    "        shared_output , task_outputs = None , []\n",
    "        for layer in self.layers:\n",
    "            shared_output , task_outputs = layer(x , shared_output , task_outputs)\n",
    "        z = torch.concat([head(output[:,-1]) for head , output in zip(self.heads , task_outputs)] , dim = -1)\n",
    "        return z\n",
    "    \n",
    "class ExpertLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            input_dim    = 6 ,\n",
    "            hidden_dim   = 2**5,\n",
    "            first_layer  = True ,\n",
    "            dropout      = 0.1,\n",
    "            rnn_layers   = 2 ,\n",
    "            num_output   = 2 ,\n",
    "            **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.first_layer = first_layer\n",
    "        expert_dim = input_dim if first_layer else hidden_dim\n",
    "\n",
    "        self.shared_expert = ExpertNetwork(expert_dim , hidden_dim , rnn_layers , dropout)\n",
    "        self.task_experts = nn.ModuleList([ExpertNetwork(expert_dim , hidden_dim , rnn_layers , dropout) for i in range(num_output)])\n",
    "        self.shared_gate = GatingNetwork(input_dim , 1 + num_output , rnn_layers , dropout)\n",
    "        self.task_gates = nn.ModuleList([GatingNetwork(input_dim , 2 , rnn_layers , dropout) for i in range(num_output)])\n",
    "        \n",
    "    def forward(self , x , *vecs):\n",
    "        if self.first_layer:\n",
    "            shared_vector = self.shared_expert(x)\n",
    "            task_vectors  = [expert(x) for expert in self.task_experts]\n",
    "        else:\n",
    "            shared_input , tasks_inputs = vecs\n",
    "            shared_vector = self.shared_expert(shared_input)\n",
    "            task_vectors  = [expert(input) for expert , input in zip(self.task_experts , tasks_inputs)]\n",
    "\n",
    "        shared_output = self.shared_gate(x , shared_vector , *task_vectors)\n",
    "        task_outputs  = [gate(x , shared_vector , vector) for gate , vector  in zip(self.task_gates , task_vectors)]\n",
    "        return shared_output , task_outputs\n",
    "    \n",
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            selector_dim = 6 ,\n",
    "            feature_dim  = 2 ,\n",
    "            num_layers = 1 , \n",
    "            dropout = 0.1 ,\n",
    "            **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.selector = nn.GRU(selector_dim , feature_dim , num_layers , batch_first=True , dropout=dropout)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "    def forward(self , x , *vecs):\n",
    "        v = self.selector(x)[0][:,-1]\n",
    "        v = self.softmax(v)\n",
    "        v = torch.stack([v[:,i].reshape(-1,1,1) * vec for i , vec in enumerate(vecs)] , dim = 0).sum(0)\n",
    "        return v\n",
    "    \n",
    "class ExpertNetwork(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            input_dim = 6 ,\n",
    "            hidden_dim  = 2**5 ,\n",
    "            num_layers = 1 , \n",
    "            dropout = 0.1 ,\n",
    "            **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.expert = nn.GRU(input_dim , hidden_dim , num_layers , batch_first=True , dropout=dropout)\n",
    "\n",
    "    def forward(self , x):\n",
    "        return self.expert(x)[0]\n",
    "\n",
    "net = ple_gru(expert_layers=3)\n",
    "net(batch_data.x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
