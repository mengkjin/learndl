{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src import layer as Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 14, 16]), (2, 6, 14, 16))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TSMixerEmbed(nn.Module):\n",
    "    '''\n",
    "    in : [bs x seq_len x nvars]\n",
    "    out: [bs x nvars x num_patch x d_model] \n",
    "    same as patchTST\n",
    "    '''\n",
    "    def __init__(self, nvars , d_model , patch_len, stride , mask_ratio = 0. , shared = True):\n",
    "        super().__init__()\n",
    "        self.nvars = nvars\n",
    "        self.d_model = d_model\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = stride\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.shared = shared \n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(patch_len, d_model) \n",
    "            for _ in range(1 if shared else nvars)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x , mask = False):\n",
    "        '''\n",
    "        in : [bs x seq_len x nvars]\n",
    "        out: [bs x nvars x num_patch x d_model] \n",
    "        '''\n",
    "        if mask:\n",
    "            x = self.patch_masking(x)[0]\n",
    "        else:\n",
    "            x = self.create_patch(x , self.patch_len, self.stride)[0] \n",
    "        # [bs x num_patch x nvars x patch_len]\n",
    "        x = x.permute(0,2,1,3)  # x_p: [bs x nvars x num_patch x patch_len]\n",
    "        if self.shared:\n",
    "            x = self.layers[0](x)\n",
    "        else:\n",
    "            x_i = [layer(x[:,i,:,:]) for i,layer in enumerate(self.layers)]\n",
    "            x = torch.stack(x_i, dim=1)\n",
    "        return x # [bs x nvars x num_patch x d_model]\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_patch(x , patch_len, stride):\n",
    "        num_patch = (max(x.shape[1], patch_len)-patch_len) // stride + 1\n",
    "        s_begin = x.shape[1] - patch_len - stride*(num_patch-1)\n",
    "        x_patch = x[:, s_begin:, :].unfold(dimension=1, size=patch_len, step=stride)                 \n",
    "        return x_patch, x_patch.shape[1] # x_patch: [bs x num_patch x nvars x patch_len]\n",
    "    \n",
    "    def patch_masking(self , x):\n",
    "        x_patch, _ = self.create_patch(x , self.patch_len, self.stride)    # xb_patch: [bs x num_patch x nvars x patch_len]\n",
    "        x_patch_mask , _ , mask , _ = self.random_masking(x_patch, self.mask_ratio)   # xb_mask: [bs x num_patch x nvars x patch_len]\n",
    "        mask = mask.bool()    # mask: [bs x num_patch x nvars]\n",
    "        return x_patch_mask , mask\n",
    "\n",
    "    @staticmethod\n",
    "    def random_masking(x_patch , mask_ratio):\n",
    "        # x_patch: [bs x num_patch x nvars x patch_len]\n",
    "        bs, L, nvars, D = x_patch.shape\n",
    "        x = x_patch.clone()\n",
    "        \n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "            \n",
    "        noise = torch.rand(bs, L, nvars,device=x_patch.device)  # noise in [0, 1], bs x L x nvars\n",
    "            \n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)                                     # ids_restore: [bs x L x nvars]\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep, :]                                             # ids_keep: [bs x len_keep x nvars]         \n",
    "        x_kept = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, 1, D))    # x_kept: [bs x len_keep x nvars  x patch_len]\n",
    "    \n",
    "        # removed x\n",
    "        x_removed = torch.zeros(bs, L-len_keep, nvars, D, device=x_patch.device)            # x_removed: [bs x (L-len_keep) x nvars x patch_len]\n",
    "        x_ = torch.cat([x_kept, x_removed], dim=1)                                          # x_: [bs x L x nvars x patch_len]\n",
    "\n",
    "        # combine the kept part and the removed one\n",
    "        x_masked = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1,1,1,D)) # x_masked: [bs x num_patch x nvars x patch_len]\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([bs, L, nvars], device=x.device)                                  # mask: [bs x num_patch x nvars]\n",
    "        mask[:, :len_keep, :] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)                                 # [bs x num_patch x nvars]\n",
    "        return x_masked, x_kept, mask, ids_restore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MixerNormLayer(nn.Module):\n",
    "    '''\n",
    "    Batch / Layer Norm\n",
    "    in : [bs x nvars x num_patch x d_model] \n",
    "    out: [bs x nvars x num_patch x d_model] \n",
    "    same as patchTST\n",
    "    '''\n",
    "    def __init__(self, norm_type : str , d_model : int):\n",
    "        super().__init__()\n",
    "        self.batch_norm = 'batch' in norm_type.lower()\n",
    "        if self.batch_norm:\n",
    "            self.norm = nn.BatchNorm1d(d_model) # 默认格式为(N,C) 或 (N,C,L)\n",
    "        else:\n",
    "            self.norm = nn.LayerNorm(d_model)  # 默认对最后一个维度进行LayerNorm\n",
    "            \n",
    "    def forward(self, inputs : torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        in : [bs x nvars x num_patch x d_model] \n",
    "        out: [bs x nvars x num_patch x d_model] \n",
    "        '''\n",
    "        if self.batch_norm:\n",
    "            bs = inputs.shape[0]\n",
    "            inputs = inputs.permute(0,1,3,2)                    # [bs x nvars x d_model x num_patch] \n",
    "            inputs = inputs.reshape(-1,*inputs.shape[-2:])      # [bs * nvars x d_model x num_patch] \n",
    "            inputs = self.norm(inputs)\n",
    "            inputs = inputs.reshape(bs,-1,*inputs.shape[-2:])   # [bs x nvars x d_model x num_patch] \n",
    "            inputs = inputs.permute(0,1,3,2)                    # [bs x nvars x num_patch x d_model] \n",
    "        else:\n",
    "            inputs = self.norm(inputs)\n",
    "        return inputs\n",
    "\n",
    "class GatedAttention(nn.Module):\n",
    "    '''\n",
    "    in : [... x in_features]\n",
    "    out: [... x out_features]\n",
    "    '''\n",
    "    def __init__(self, in_size: int, out_size: int):\n",
    "        super().__init__()\n",
    "        self.attn_layer = nn.Linear(in_size, out_size)\n",
    "        self.attn_softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, inputs : torch.Tensor) -> torch.Tensor:\n",
    "        attn_weight = self.attn_softmax(self.attn_layer(inputs))\n",
    "        inputs = inputs * attn_weight\n",
    "        return inputs\n",
    " \n",
    "class MixerMLP(nn.Module):\n",
    "    '''\n",
    "    similar to FFN\n",
    "    in : [... x in_features]\n",
    "    out: [... x out_features]\n",
    "    '''\n",
    "    def __init__(self, in_size: int, out_size: int, expansion_factor=1,dropout=0.,act_type='gelu'):\n",
    "        super().__init__()\n",
    "        num_hidden = in_size * expansion_factor\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_size, num_hidden) ,\n",
    "            Layer.Act.get_activation_fn(act_type) ,\n",
    "            nn.Dropout(dropout) ,\n",
    "            nn.Linear(num_hidden , out_size) ,\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs : torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(inputs)\n",
    "\n",
    "class PatchMixerBlock(nn.Module):\n",
    "    '''\n",
    "    inter patch information extraction with channel independence\n",
    "    in : [bs x nvars x num_patch x d_model] \n",
    "    out: [bs x nvars x num_patch x d_model] \n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 num_patch,\n",
    "                 d_model,\n",
    "                 dropout = 0.,\n",
    "                 expansion_factor = 2,\n",
    "                 gated_attn=True,\n",
    "                 norm_type='batch',\n",
    "                 act_type='gelu' ,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm = MixerNormLayer(norm_type , d_model)\n",
    "        self.mlp = MixerMLP(num_patch,num_patch,expansion_factor,dropout,act_type)\n",
    "        self.gating = GatedAttention(num_patch, num_patch) if gated_attn else nn.Sequential()\n",
    "\n",
    "    def forward(self, inputs : torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        in : [bs x nvars x num_patch x d_model] \n",
    "        out: [bs x nvars x num_patch x d_model] \n",
    "        '''\n",
    "        residual = inputs\n",
    "        inputs = self.norm(inputs)          # [bs x nvars x num_patch x d_model] \n",
    "        inputs = inputs.permute(0,1,3,2)    # [bs x nvars x d_model x num_patch] \n",
    "        inputs = self.mlp(inputs)           # [bs x nvars x d_model x num_patch] \n",
    "        inputs = self.gating(inputs)        # [bs x nvars x d_model x num_patch] \n",
    "        inputs = inputs.permute(0,1,3,2)    # [bs x nvars x num_patch x d_model] \n",
    "        return residual + inputs            # [bs x nvars x num_patch x d_model] \n",
    "    \n",
    "class FeatureMixerBlock(nn.Module):\n",
    "    '''\n",
    "    inter Feature information extraction\n",
    "    in : [bs x nvars x num_patch x d_model] \n",
    "    out: [bs x nvars x num_patch x d_model] \n",
    "    '''\n",
    "    def __init__(self , \n",
    "                 d_model ,\n",
    "                 dropout = 0. ,\n",
    "                 expansion_factor = 2 ,\n",
    "                 gated_attn=True ,\n",
    "                 norm_type = 'batch' ,\n",
    "                 act_type = 'gelu' ,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm = MixerNormLayer(norm_type,d_model)\n",
    "        self.mlp = MixerMLP(d_model,d_model,expansion_factor,dropout,act_type)\n",
    "        self.gating = GatedAttention(d_model, d_model) if gated_attn else nn.Sequential()\n",
    "\n",
    "    def forward(self, inputs : torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        in : [bs x nvars x num_patch x d_model] \n",
    "        out: [bs x nvars x num_patch x d_model] \n",
    "        '''\n",
    "        residual = inputs\n",
    "        inputs = self.norm(inputs)          # [bs x nvars x num_patch x d_model] \n",
    "        inputs = self.mlp(inputs)           # [bs x nvars x d_model x num_patch] \n",
    "        inputs = self.gating(inputs)        # [bs x nvars x d_model x num_patch] \n",
    "        return residual + inputs            # [bs x nvars x num_patch x d_model] \n",
    "\n",
    "class ChannelMixerBlock(nn.Module):\n",
    "    '''\n",
    "    inter Channel information extraction\n",
    "    in : [bs x nvars x num_patch x d_model] \n",
    "    out: [bs x nvars x num_patch x d_model] \n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 in_channel,\n",
    "                 dropout = 0.,\n",
    "                 expansion_factor = 2,\n",
    "                 gated_attn=True,\n",
    "                 norm_type = 'batch' ,\n",
    "                 act_type = 'gelu' ,\n",
    "                 ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm = MixerNormLayer(norm_type,d_model)\n",
    "        self.mlp = MixerMLP(in_channel,in_channel,expansion_factor,dropout,act_type)\n",
    "        self.gating = GatedAttention(in_channel, in_channel) if gated_attn else nn.Sequential()\n",
    "\n",
    "    def forward(self, inputs : torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        in : [bs x nvars x num_patch x d_model] \n",
    "        out: [bs x nvars x num_patch x d_model] \n",
    "        '''\n",
    "        residual = inputs\n",
    "        inputs = self.norm(inputs)          # [bs x nvars x num_patch x d_model]\n",
    "        inputs = inputs.permute(0,3,2,1)    # [bs x d_model x num_patch x nvars]\n",
    "        inputs = self.gating(inputs)        # [bs x d_model x num_patch x nvars]\n",
    "        inputs = self.mlp(inputs)           # [bs x d_model x num_patch x nvars]\n",
    "        inputs = inputs.permute(0,3,2,1)    # [bs x nvars x num_patch x d_model]\n",
    "        return inputs + residual            # [bs x nvars x num_patch x d_model]\n",
    "    \n",
    "class TSMixerEncoder(nn.Module):\n",
    "    '''\n",
    "    in : [bs x nvars x num_patch x d_model]\n",
    "    out: [bs x nvars x d_model x num_patch]\n",
    "    '''\n",
    "    def __init__(self, nvars, num_patch, d_model=128, channel_mixer = True, \n",
    "                 dropout=0., expansion_factor = 2, gated_attn = True , norm_type = 'batch', act_type='gelu', \n",
    "                 pe='zeros', learn_pe=True, verbose=False, **kwargs):\n",
    "\n",
    "        super().__init__()\n",
    "        self.nvars = nvars\n",
    "\n",
    "        # Positional encoding\n",
    "        self.W_pos = Layer.PE.positional_encoding(pe, learn_pe, num_patch, d_model)\n",
    "\n",
    "        # Residual dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        mixer_list = [PatchMixerBlock(num_patch , d_model , dropout , expansion_factor , gated_attn , norm_type , act_type),\n",
    "                      FeatureMixerBlock(d_model , dropout , expansion_factor , gated_attn , norm_type , act_type)]\n",
    "        if channel_mixer:\n",
    "            mixer_list.append(ChannelMixerBlock(d_model,nvars,dropout , expansion_factor , gated_attn , norm_type , act_type))\n",
    "        self.mixers = nn.ModuleList(mixer_list)\n",
    "        \n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor:          \n",
    "        '''\n",
    "        in : [bs x nvars x num_patch x d_model]   \n",
    "        out: [bs x nvars x d_model x num_patch]\n",
    "        '''\n",
    "        x = self.dropout(x + self.W_pos)            # [bs x nvars x num_patch x d_model]\n",
    "        for mixer in self.mixers: x = mixer(x)      # [bs x nvars x num_patch x d_model]\n",
    "        x = x.permute(0,1,3,2)                      # [bs x nvars x d_model x num_patch]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 30, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TSMixerPredictionHead(nn.Module):\n",
    "    '''\n",
    "    in : [bs x nvars x d_model x num_patch]\n",
    "    out: [bs x predict_steps]\n",
    "    '''\n",
    "    def __init__(self, nvars, d_model, num_patch, predict_steps = 1 , head_dropout=0, \n",
    "                 flatten=False , shared = False , act_type = 'gelu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shared = shared\n",
    "        self.nvars = nvars\n",
    "        self.flatten = flatten\n",
    "        head_dim = d_model * num_patch\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Flatten(start_dim=-2),\n",
    "                nn.Linear(head_dim, d_model),\n",
    "                Layer.Act.get_activation_fn(act_type),\n",
    "                nn.Dropout(head_dropout)\n",
    "            ) for _ in range(1 if shared else nvars)\n",
    "        ])\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Flatten(start_dim=-2),\n",
    "            nn.Linear(d_model * nvars , predict_steps),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):                     \n",
    "        '''\n",
    "        in : [bs x nvars x d_model x num_patch]\n",
    "        out: [bs x predict_steps]\n",
    "        '''\n",
    "        if self.shared:\n",
    "            x = self.layers[0](x)          # [bs x nvars x d_model]\n",
    "        else:\n",
    "            x_i = [layer(x[:,i,:,:]) for i,layer in enumerate(self.layers)] \n",
    "            x = torch.stack(x_i, dim=1)    # [bs x nvars x d_model]\n",
    "        x = x.transpose(2,1)               # [bs x d_model x nvars]\n",
    "        x = self.linear(x)                 # [bs x predict_steps]  \n",
    "        return x\n",
    "\n",
    "class TSMixerPretrainHead(nn.Module):\n",
    "    '''\n",
    "    in : [bs x nvars x d_model x num_patch]\n",
    "    out: [bs x seq_len x nvars]\n",
    "    '''\n",
    "    def __init__(self, d_model , num_patch , seq_len , dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(d_model * num_patch , seq_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        in : [bs x nvars x d_model x num_patch]\n",
    "        out: [bs x seq_len x nvars]\n",
    "        '''\n",
    "        x = self.dropout(x)                     # [bs x nvars x d_model x num_patch]\n",
    "        x = x.flatten(start_dim=2)              # [bs x nvars x d_model (x) num_patch]\n",
    "        x = self.linear(x)                      # [bs x nvars x seq_len]\n",
    "        x = x.permute(0,2,1)                    # [bs x seq_len x nvars]                     \n",
    "        \n",
    "        return x\n",
    "\n",
    "class TSMixer(nn.Module):\n",
    "    \"\"\"\n",
    "    in:  [bs x seq_len x nvars]\n",
    "    out: [bs x seq_len x nvars] for pretrain\n",
    "         [bs x predict_steps] for prediction\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        nvars : int , \n",
    "        seq_len: int , \n",
    "        patch_len:int = 4 , \n",
    "        stride:int|None = None, \n",
    "        d_model:int=16, \n",
    "        channel_mixer = True , expansion_factor = 2 , gated_attn = True ,\n",
    "        shared_embedding=True, shared_head = False,\n",
    "        revin:bool=True,\n",
    "        norm_type:str='batch', dropout:float=0., act_type:str='gelu', \n",
    "        pe:str='zeros', learn_pe:bool=True, head_dropout = 0, predict_steps:int = 1,\n",
    "        head_type = 'prediction', verbose:bool=False, **kwargs\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        assert head_type in ['pretrain', 'prediction'], 'head type should be either pretrain, prediction, or regression'\n",
    "        self.nvars = nvars\n",
    "        self.head_type = head_type\n",
    "        self.mask_fwd = head_type == 'pretrain'\n",
    "        if stride is None: stride = patch_len // 2\n",
    "        num_patch = (max(seq_len, patch_len)-patch_len) // stride + 1\n",
    "\n",
    "        # RevIN\n",
    "        self.revin = Layer.RevIN.RevIN(num_features = nvars) if revin else None\n",
    "\n",
    "        # Embedding\n",
    "        self.embed = TSMixerEmbed(nvars,d_model,patch_len,stride,mask_ratio=0.4,shared=shared_embedding)\n",
    "\n",
    "        # Backbone\n",
    "        self.backbone = TSMixerEncoder(            \n",
    "            nvars, num_patch, d_model=d_model, channel_mixer = channel_mixer, \n",
    "            dropout=dropout, expansion_factor = expansion_factor, gated_attn = gated_attn , norm_type = norm_type, act_type=act_type, \n",
    "            pe=pe, learn_pe=learn_pe, **kwargs)\n",
    "        \n",
    "        # Head\n",
    "        if head_type == 'pretrain':\n",
    "            self.head = TSMixerPretrainHead(d_model, num_patch ,seq_len, head_dropout)\n",
    "        elif head_type == 'prediction':\n",
    "            self.head = TSMixerPredictionHead(nvars, d_model, num_patch, predict_steps, \n",
    "                                              head_dropout , shared = shared_head , act_type = act_type)\n",
    "\n",
    "    def forward(self, x):                             \n",
    "        \"\"\"\n",
    "        in:  [bs x seq_len x nvars]\n",
    "        out: [bs x seq_len x nvars] for pretrain\n",
    "             [bs x predict_steps] for prediction\n",
    "        \"\"\"   \n",
    "\n",
    "        if self.revin is not None: \n",
    "            x = self.revin(x , 'norm')              # [bs x seq_len x nvars]\n",
    "        x = self.embed(x , self.mask_fwd)           # [bs x nvars x num_patch x d_model]\n",
    "        x = self.backbone(x)                        # [bs x nvars x d_model x num_patch]\n",
    "\n",
    "        if self.revin is not None: \n",
    "            x = x.permute(0,2,3,1)                  # [bs x d_model x num_patch x nvars]\n",
    "            x = self.revin(x , 'denorm')            # [bs x d_model x num_patch x nvars]\n",
    "            x = x.permute(0,3,1,2)                  # [bs x nvars x d_model x num_patch]\n",
    "\n",
    "        x = self.head(x)                            # [bs x seq_len x nvars] | [bs x predict_steps]\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size = 2 \n",
    "seq_len = 30\n",
    "patch_len = 3\n",
    "stride = 2\n",
    "nvars = 6\n",
    "mask_ratio = 0.4\n",
    "d_model = 16\n",
    "predict_steps = 1\n",
    "shared_embedding = True\n",
    "\n",
    "num_patch = max(seq_len - patch_len, 0) // stride + 1 # 15\n",
    "\n",
    "x = torch.rand(batch_size , seq_len , nvars)\n",
    "y = torch.rand(batch_size , predict_steps)\n",
    "\n",
    "print(x.shape , y.shape)\n",
    "embed = TSMixerEmbed(nvars,d_model,patch_len,stride,mask_ratio=0.4,shared=shared_embedding)\n",
    "print(embed(x).shape , (batch_size , nvars , num_patch , d_model))\n",
    "net = TSMixer(nvars , seq_len , patch_len , stride , d_model , shared_embedding=shared_embedding , head_type='pretrain' , predict_steps = predict_steps)\n",
    "print(net(x).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
