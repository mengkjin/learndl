{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml , os\n",
    "\n",
    "out_dict = {'a':10}\n",
    "out_path = f'./results/running_results.yaml'\n",
    "if os.path.exists(out_path):\n",
    "    out_type = 'a'\n",
    "else:\n",
    "    os.makedirs(os.path.dirname(out_path) , exist_ok=True)\n",
    "    out_type = 'w'\n",
    "\n",
    "with open(out_path , out_type , encoding = 'utf-8') as f:\n",
    "    yaml.dump(out_dict , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e173460f-7d63-4491-bee2-f5390e8032e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m23-11-09 00:13:41|MOD:gen_data    |\u001b[0m: \u001b[1m\u001b[31mData loading start!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing day_trading_data data...\n",
      "arr shape : (5264, 6202, 1, 6) , row shape : (5264,) , col shape : (6202,)\n",
      "Preparing day_ylabels_data data...\n",
      "arr shape : (5249, 6191, 1, 2) , row shape : (5249,) , col shape : (6191,)\n",
      "Preparing 15m_trading_data data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[45m23-11-09 00:20:02|MOD:gen_data    |\u001b[0m: \u001b[1m\u001b[35m[day] Data avg and std generation start!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr shape : (5204, 3302, 16, 6) , row shape : (5204,) , col shape : (3302,)\n",
      "Loading day trading data finished, cost 5.64 Secs\n",
      "torch.Size([5264, 920, 1, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[45m23-11-09 00:20:19|MOD:gen_data    |\u001b[0m: \u001b[1m\u001b[35m[15m] Data avg and std generation start!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 15m trading data finished, cost 45.60 Secs\n",
      "torch.Size([5204, 340, 1, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m\u001b[41m23-11-09 00:21:36|MOD:gen_data    |\u001b[0m: \u001b[1m\u001b[31mData loading Finished! Cost 475.09 Seconds\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%run gen_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352d70f2-fad6-447b-9ca3-f4804f0ce801",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run run_model.py --process=0 --rawname=1 --resume=0 --anchoring=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8859681-9a7a-482d-97d2-61879011475f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil , os\n",
    "folder = './model'\n",
    "save_model = ['.ipynb_checkpoints', 'LSTM_day','GRU_day','Transformer_day','GeneralRNN_day','GeneralRNN_day_Trans_vs_LSTM']\n",
    "# [print(f'{folder}/{p}') for p in os.listdir(folder) if not p in save_model]\n",
    "\n",
    "import shutil\n",
    "shutil.rmtree('./model/DoubleGRU_both_single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f368a198-7670-4c2b-a6ce-9595220a3711",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.TimeWiseTranformer"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "b , s , f , h = 2000 , 30 , 6 , 8\n",
    "x = torch.rand(b,s,f)\n",
    "x[int(b*0.9):] = torch.nan\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, input_dim, dropout=0.0, max_len=1000,**kwargs):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.seq_len = max_len\n",
    "        self.P = torch.zeros(1 , self.seq_len, input_dim)\n",
    "        X = torch.arange(self.seq_len, dtype=torch.float).reshape(-1,1) / torch.pow(10000,torch.arange(0, input_dim, 2 ,dtype=torch.float) / input_dim)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X[:,:input_dim//2])\n",
    "    def forward(self, inputs):\n",
    "        return self.dropout(inputs + self.P[:,:inputs.shape[1],:].to(inputs.device))\n",
    "    \n",
    "class TimeWiseTranformer(nn.Module):\n",
    "    def __init__(self , input_dim , hidden_dim , ffn_dim = None , num_heads = 8 , num_enclayer = 2 , dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert hidden_dim % num_heads == 0\n",
    "        ffn_dim = 4 * hidden_dim if ffn_dim is None else ffn_dim\n",
    "        self.fc_in = nn.Linear(input_dim,hidden_dim)\n",
    "        self.pos_enc = PositionalEncoding(hidden_dim,dropout=dropout)\n",
    "        enc_layer = nn.TransformerEncoderLayer(hidden_dim , num_heads, dim_feedforward=ffn_dim , dropout=dropout , batch_first=True)\n",
    "        self.trans = nn.TransformerEncoder(enc_layer , num_enclayer)\n",
    "    def forward(self, inputs):\n",
    "        hidden = self.fc_in(inputs)\n",
    "        hidden = self.pos_enc(hidden)\n",
    "        return self.trans(hidden)\n",
    "\n",
    "tf = TimeWiseTranformer(f,h,num_enclayer=6)\n",
    "# tf(x).select(-2,-1).shape\n",
    "tf(x).shape\n",
    "\n",
    "a = locals()['TimeWiseTranformer']\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c603919-0b25-4037-bc97-5d3deefe953c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 30, 8])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from mymodel import *\n",
    "\n",
    "b , s , f , h = 2000 , 30 , 6 , 8\n",
    "x = torch.rand(b,s,f)\n",
    "x[int(b*0.9):] = torch.nan\n",
    "\n",
    "net = mod_transformer(f,h)\n",
    "net(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "957b817f-1ea4-4842-b44c-e20a6b543fa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "base_lr = 5e-2\n",
    "max_lr  = 1e-1\n",
    "min_lr = base_lr * 1e-4\n",
    "\n",
    "net = torch.nn.Sequential(torch.nn.Linear(10,10) , torch.nn.Linear(10,1))\n",
    "net_base_lr = [{'params': p , 'lr':l , 'lr_param' : l} for l,p in zip([base_lr , base_lr/2] , net.parameters())]\n",
    "opt = torch.optim.Adam(net_base_lr , 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], maxlen=10)\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "a = deque(range(10) ,   maxlen = 10)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRA model\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "import json\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class TRAModel(Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_config,\n",
    "        tra_config,\n",
    "        model_type=\"LSTM\",\n",
    "        lr=1e-3,\n",
    "        n_epochs=500,\n",
    "        early_stop=50,\n",
    "        smooth_steps=5,\n",
    "        max_steps_per_epoch=None,\n",
    "        freeze_model=False,\n",
    "        model_init_state=None,\n",
    "        lamb=0.0,\n",
    "        rho=0.99,\n",
    "        seed=None,\n",
    "        logdir=None,\n",
    "        eval_train=True,\n",
    "        eval_test=False,\n",
    "        avg_params=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        self.logger = get_module_logger(\"TRA\")\n",
    "        self.logger.info(\"TRA Model...\")\n",
    "\n",
    "        self.model = eval(model_type)(**model_config).to(device)\n",
    "        if model_init_state:\n",
    "            self.model.load_state_dict(torch.load(model_init_state, map_location=\"cpu\")[\"model\"])\n",
    "        if freeze_model:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad_(False)\n",
    "        else:\n",
    "            self.logger.info(\"# model params: %d\" % sum([p.numel() for p in self.model.parameters()]))\n",
    "\n",
    "        self.tra = TRA(self.model.output_size, **tra_config).to(device)\n",
    "        self.logger.info(\"# tra params: %d\" % sum([p.numel() for p in self.tra.parameters()]))\n",
    "\n",
    "        self.optimizer = optim.Adam(list(self.model.parameters()) + list(self.tra.parameters()), lr=lr)\n",
    "\n",
    "        self.model_config = model_config\n",
    "        self.tra_config = tra_config\n",
    "        self.lr = lr\n",
    "        self.n_epochs = n_epochs\n",
    "        self.early_stop = early_stop\n",
    "        self.smooth_steps = smooth_steps\n",
    "        self.max_steps_per_epoch = max_steps_per_epoch\n",
    "        self.lamb = lamb\n",
    "        self.rho = rho\n",
    "        self.seed = seed\n",
    "        self.logdir = logdir\n",
    "        self.eval_train = eval_train\n",
    "        self.eval_test = eval_test\n",
    "        self.avg_params = avg_params\n",
    "\n",
    "        if self.tra.num_states > 1 and not self.eval_train:\n",
    "            self.logger.warn(\"`eval_train` will be ignored when using TRA\")\n",
    "\n",
    "        if self.logdir is not None:\n",
    "            if os.path.exists(self.logdir):\n",
    "                self.logger.warn(f\"logdir {self.logdir} is not empty\")\n",
    "            os.makedirs(self.logdir, exist_ok=True)\n",
    "\n",
    "        self.fitted = False\n",
    "        self.global_step = -1\n",
    "\n",
    "    def train_epoch(self, data_set):\n",
    "        self.model.train()\n",
    "        self.tra.train()\n",
    "\n",
    "        data_set.train()\n",
    "\n",
    "        max_steps = self.n_epochs\n",
    "        if self.max_steps_per_epoch is not None:\n",
    "            max_steps = min(self.max_steps_per_epoch, self.n_epochs)\n",
    "\n",
    "        count = 0\n",
    "        total_loss = 0\n",
    "        total_count = 0\n",
    "        for batch in tqdm(data_set, total=max_steps):\n",
    "            count += 1\n",
    "            if count > max_steps:\n",
    "                break\n",
    "\n",
    "            self.global_step += 1\n",
    "\n",
    "            data, label, index = batch[\"data\"], batch[\"label\"], batch[\"index\"]\n",
    "\n",
    "            feature = data[:, :, : -self.tra.num_states]\n",
    "            hist_loss = data[:, : -data_set.horizon, -self.tra.num_states :]\n",
    "\n",
    "            hidden = self.model(feature)\n",
    "            pred, all_preds, prob = self.tra(hidden, hist_loss)\n",
    "\n",
    "            loss = (pred - label).pow(2).mean()\n",
    "\n",
    "            L = (all_preds.detach() - label[:, None]).pow(2)\n",
    "            L -= L.min(dim=-1, keepdim=True).values  # normalize & ensure positive input\n",
    "\n",
    "            data_set.assign_data(index, L)  # save loss to memory\n",
    "\n",
    "            if prob is not None:\n",
    "                P = sinkhorn(-L, epsilon=0.01)  # sample assignment matrix\n",
    "                lamb = self.lamb * (self.rho**self.global_step)\n",
    "                reg = prob.log().mul(P).sum(dim=-1).mean()\n",
    "                loss = loss - lamb * reg\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_count += len(pred)\n",
    "\n",
    "        total_loss /= total_count\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def test_epoch(self, data_set, return_pred=False):\n",
    "        self.model.eval()\n",
    "        self.tra.eval()\n",
    "        data_set.eval()\n",
    "\n",
    "        preds = []\n",
    "        metrics = []\n",
    "        for batch in tqdm(data_set):\n",
    "            data, label, index = batch[\"data\"], batch[\"label\"], batch[\"index\"]\n",
    "\n",
    "            feature = data[:, :, : -self.tra.num_states]\n",
    "            hist_loss = data[:, : -data_set.horizon, -self.tra.num_states :]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                hidden = self.model(feature)\n",
    "                pred, all_preds, prob = self.tra(hidden, hist_loss)\n",
    "\n",
    "            L = (all_preds - label[:, None]).pow(2)\n",
    "\n",
    "            L -= L.min(dim=-1, keepdim=True).values  # normalize & ensure positive input\n",
    "\n",
    "            data_set.assign_data(index, L)  # save loss to memory\n",
    "\n",
    "            X = np.c_[\n",
    "                pred.cpu().numpy(),\n",
    "                label.cpu().numpy(),\n",
    "            ]\n",
    "            columns = [\"score\", \"label\"]\n",
    "            if prob is not None:\n",
    "                X = np.c_[X, all_preds.cpu().numpy(), prob.cpu().numpy()]\n",
    "                columns += [\"score_%d\" % d for d in range(all_preds.shape[1])] + [\n",
    "                    \"prob_%d\" % d for d in range(all_preds.shape[1])\n",
    "                ]\n",
    "\n",
    "            pred = pd.DataFrame(X, index=index.cpu().numpy(), columns=columns)\n",
    "\n",
    "            metrics.append(evaluate(pred))\n",
    "\n",
    "            if return_pred:\n",
    "                preds.append(pred)\n",
    "\n",
    "        metrics = pd.DataFrame(metrics)\n",
    "        metrics = {\n",
    "            \"MSE\": metrics.MSE.mean(),\n",
    "            \"MAE\": metrics.MAE.mean(),\n",
    "            \"IC\": metrics.IC.mean(),\n",
    "            \"ICIR\": metrics.IC.mean() / metrics.IC.std(),\n",
    "        }\n",
    "\n",
    "        if return_pred:\n",
    "            preds = pd.concat(preds, axis=0)\n",
    "            preds.index = data_set.restore_index(preds.index)\n",
    "            preds.index = preds.index.swaplevel()\n",
    "            preds.sort_index(inplace=True)\n",
    "\n",
    "        return metrics, preds\n",
    "\n",
    "    def fit(self, dataset, evals_result=dict()):\n",
    "        train_set, valid_set, test_set = dataset.prepare([\"train\", \"valid\", \"test\"])\n",
    "\n",
    "        best_score = -1\n",
    "        best_epoch = 0\n",
    "        stop_rounds = 0\n",
    "        best_params = {\n",
    "            \"model\": copy.deepcopy(self.model.state_dict()),\n",
    "            \"tra\": copy.deepcopy(self.tra.state_dict()),\n",
    "        }\n",
    "        params_list = {\n",
    "            \"model\": collections.deque(maxlen=self.smooth_steps),\n",
    "            \"tra\": collections.deque(maxlen=self.smooth_steps),\n",
    "        }\n",
    "        evals_result[\"train\"] = []\n",
    "        evals_result[\"valid\"] = []\n",
    "        evals_result[\"test\"] = []\n",
    "\n",
    "        # train\n",
    "        self.fitted = True\n",
    "        self.global_step = -1\n",
    "\n",
    "        if self.tra.num_states > 1:\n",
    "            self.logger.info(\"init memory...\")\n",
    "            self.test_epoch(train_set)\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            self.logger.info(\"Epoch %d:\", epoch)\n",
    "\n",
    "            self.logger.info(\"training...\")\n",
    "            self.train_epoch(train_set)\n",
    "\n",
    "            self.logger.info(\"evaluating...\")\n",
    "            # average params for inference\n",
    "            params_list[\"model\"].append(copy.deepcopy(self.model.state_dict()))\n",
    "            params_list[\"tra\"].append(copy.deepcopy(self.tra.state_dict()))\n",
    "            self.model.load_state_dict(average_params(params_list[\"model\"]))\n",
    "            self.tra.load_state_dict(average_params(params_list[\"tra\"]))\n",
    "\n",
    "            # NOTE: during evaluating, the whole memory will be refreshed\n",
    "            if self.tra.num_states > 1 or self.eval_train:\n",
    "                train_set.clear_memory()  # NOTE: clear the shared memory\n",
    "                train_metrics = self.test_epoch(train_set)[0]\n",
    "                evals_result[\"train\"].append(train_metrics)\n",
    "                self.logger.info(\"\\ttrain metrics: %s\" % train_metrics)\n",
    "\n",
    "            valid_metrics = self.test_epoch(valid_set)[0]\n",
    "            evals_result[\"valid\"].append(valid_metrics)\n",
    "            self.logger.info(\"\\tvalid metrics: %s\" % valid_metrics)\n",
    "\n",
    "            if self.eval_test:\n",
    "                test_metrics = self.test_epoch(test_set)[0]\n",
    "                evals_result[\"test\"].append(test_metrics)\n",
    "                self.logger.info(\"\\ttest metrics: %s\" % test_metrics)\n",
    "\n",
    "            if valid_metrics[\"IC\"] > best_score:\n",
    "                best_score = valid_metrics[\"IC\"]\n",
    "                stop_rounds = 0\n",
    "                best_epoch = epoch\n",
    "                best_params = {\n",
    "                    \"model\": copy.deepcopy(self.model.state_dict()),\n",
    "                    \"tra\": copy.deepcopy(self.tra.state_dict()),\n",
    "                }\n",
    "            else:\n",
    "                stop_rounds += 1\n",
    "                if stop_rounds >= self.early_stop:\n",
    "                    self.logger.info(\"early stop @ %s\" % epoch)\n",
    "                    break\n",
    "\n",
    "            # restore parameters\n",
    "            self.model.load_state_dict(params_list[\"model\"][-1])\n",
    "            self.tra.load_state_dict(params_list[\"tra\"][-1])\n",
    "\n",
    "        self.logger.info(\"best score: %.6lf @ %d\" % (best_score, best_epoch))\n",
    "        self.model.load_state_dict(best_params[\"model\"])\n",
    "        self.tra.load_state_dict(best_params[\"tra\"])\n",
    "\n",
    "        metrics, preds = self.test_epoch(test_set, return_pred=True)\n",
    "        self.logger.info(\"test metrics: %s\" % metrics)\n",
    "\n",
    "        if self.logdir:\n",
    "            self.logger.info(\"save model & pred to local directory\")\n",
    "\n",
    "            pd.concat({name: pd.DataFrame(evals_result[name]) for name in evals_result}, axis=1).to_csv(\n",
    "                self.logdir + \"/logs.csv\", index=False\n",
    "            )\n",
    "\n",
    "            torch.save(best_params, self.logdir + \"/model.bin\")\n",
    "\n",
    "            preds.to_pickle(self.logdir + \"/pred.pkl\")\n",
    "\n",
    "            info = {\n",
    "                \"config\": {\n",
    "                    \"model_config\": self.model_config,\n",
    "                    \"tra_config\": self.tra_config,\n",
    "                    \"lr\": self.lr,\n",
    "                    \"n_epochs\": self.n_epochs,\n",
    "                    \"early_stop\": self.early_stop,\n",
    "                    \"smooth_steps\": self.smooth_steps,\n",
    "                    \"max_steps_per_epoch\": self.max_steps_per_epoch,\n",
    "                    \"lamb\": self.lamb,\n",
    "                    \"rho\": self.rho,\n",
    "                    \"seed\": self.seed,\n",
    "                    \"logdir\": self.logdir,\n",
    "                },\n",
    "                \"best_eval_metric\": -best_score,  # NOTE: minux -1 for minimize\n",
    "                \"metric\": metrics,\n",
    "            }\n",
    "            with open(self.logdir + \"/info.json\", \"w\") as f:\n",
    "                json.dump(info, f)\n",
    "\n",
    "    def predict(self, dataset, segment=\"test\"):\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"model is not fitted yet!\")\n",
    "\n",
    "        test_set = dataset.prepare(segment)\n",
    "\n",
    "        metrics, preds = self.test_epoch(test_set, return_pred=True)\n",
    "        self.logger.info(\"test metrics: %s\" % metrics)\n",
    "\n",
    "        return preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRA dataset\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def _to_tensor(x):\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        return torch.tensor(x, dtype=torch.float, device=device)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _create_ts_slices(index, seq_len):\n",
    "    \"\"\"\n",
    "    create time series slices from pandas index\n",
    "\n",
    "    Args:\n",
    "        index (pd.MultiIndex): pandas multiindex with <instrument, datetime> order\n",
    "        seq_len (int): sequence length\n",
    "    \"\"\"\n",
    "    assert index.is_lexsorted(), \"index should be sorted\"\n",
    "\n",
    "    # number of dates for each code\n",
    "    sample_count_by_codes = pd.Series(0, index=index).groupby(level=0).size().values\n",
    "\n",
    "    # start_index for each code\n",
    "    start_index_of_codes = np.roll(np.cumsum(sample_count_by_codes), 1)\n",
    "    start_index_of_codes[0] = 0\n",
    "\n",
    "    # all the [start, stop) indices of features\n",
    "    # features btw [start, stop) are used to predict the `stop - 1` label\n",
    "    slices = []\n",
    "    for cur_loc, cur_cnt in zip(start_index_of_codes, sample_count_by_codes):\n",
    "        for stop in range(1, cur_cnt + 1):\n",
    "            end = cur_loc + stop\n",
    "            start = max(end - seq_len, 0)\n",
    "            slices.append(slice(start, end))\n",
    "    slices = np.array(slices)\n",
    "\n",
    "    return slices\n",
    "\n",
    "\n",
    "def _get_date_parse_fn(target):\n",
    "    \"\"\"get date parse function\n",
    "\n",
    "    This method is used to parse date arguments as target type.\n",
    "\n",
    "    Example:\n",
    "        get_date_parse_fn('20120101')('2017-01-01') => '20170101'\n",
    "        get_date_parse_fn(20120101)('2017-01-01') => 20170101\n",
    "    \"\"\"\n",
    "    if isinstance(target, pd.Timestamp):\n",
    "        _fn = lambda x: pd.Timestamp(x)  # Timestamp('2020-01-01')\n",
    "    elif isinstance(target, str) and len(target) == 8:\n",
    "        _fn = lambda x: str(x).replace(\"-\", \"\")[:8]  # '20200201'\n",
    "    elif isinstance(target, int):\n",
    "        _fn = lambda x: int(str(x).replace(\"-\", \"\")[:8])  # 20200201\n",
    "    else:\n",
    "        _fn = lambda x: x\n",
    "    return _fn\n",
    "\n",
    "\n",
    "class MTSDatasetH(DatasetH):\n",
    "    \"\"\"Memory Augmented Time Series Dataset\n",
    "\n",
    "    Args:\n",
    "        handler (DataHandler): data handler\n",
    "        segments (dict): data split segments\n",
    "        seq_len (int): time series sequence length\n",
    "        horizon (int): label horizon (to mask historical loss for TRA)\n",
    "        num_states (int): how many memory states to be added (for TRA)\n",
    "        batch_size (int): batch size (<0 means daily batch)\n",
    "        shuffle (bool): whether shuffle data\n",
    "        pin_memory (bool): whether pin data to gpu memory\n",
    "        drop_last (bool): whether drop last batch < batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        handler,\n",
    "        segments,\n",
    "        seq_len=60,\n",
    "        horizon=0,\n",
    "        num_states=1,\n",
    "        batch_size=-1,\n",
    "        shuffle=True,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        assert horizon > 0, \"please specify `horizon` to avoid data leakage\"\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.horizon = horizon\n",
    "        self.num_states = num_states\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "        self.pin_memory = pin_memory\n",
    "        self.params = (batch_size, drop_last, shuffle)  # for train/eval switch\n",
    "\n",
    "        super().__init__(handler, segments, **kwargs)\n",
    "\n",
    "    def setup_data(self, handler_kwargs: dict = None, **kwargs):\n",
    "        super().setup_data()\n",
    "\n",
    "        # change index to <code, date>\n",
    "        # NOTE: we will use inplace sort to reduce memory use\n",
    "        df = self.handler._data\n",
    "        df.index = df.index.swaplevel()\n",
    "        df.sort_index(inplace=True)\n",
    "\n",
    "        self._data = df[\"feature\"].values.astype(\"float32\")\n",
    "        self._label = df[\"label\"].squeeze().astype(\"float32\")\n",
    "        self._index = df.index\n",
    "\n",
    "        # add memory to feature\n",
    "        self._data = np.c_[self._data, np.zeros((len(self._data), self.num_states), dtype=np.float32)]\n",
    "\n",
    "        # padding tensor\n",
    "        self.zeros = np.zeros((self.seq_len, self._data.shape[1]), dtype=np.float32)\n",
    "\n",
    "        # pin memory\n",
    "        if self.pin_memory:\n",
    "            self._data = _to_tensor(self._data)\n",
    "            self._label = _to_tensor(self._label)\n",
    "            self.zeros = _to_tensor(self.zeros)\n",
    "\n",
    "        # create batch slices\n",
    "        self.batch_slices = _create_ts_slices(self._index, self.seq_len)\n",
    "\n",
    "        # create daily slices\n",
    "        index = [slc.stop - 1 for slc in self.batch_slices]\n",
    "        act_index = self.restore_index(index)\n",
    "        daily_slices = {date: [] for date in sorted(act_index.unique(level=1))}\n",
    "        for i, (code, date) in enumerate(act_index):\n",
    "            daily_slices[date].append(self.batch_slices[i])\n",
    "        self.daily_slices = list(daily_slices.values())\n",
    "\n",
    "    def _prepare_seg(self, slc, **kwargs):\n",
    "        fn = _get_date_parse_fn(self._index[0][1])\n",
    "\n",
    "        if isinstance(slc, slice):\n",
    "            start, stop = slc.start, slc.stop\n",
    "        elif isinstance(slc, (list, tuple)):\n",
    "            start, stop = slc\n",
    "        else:\n",
    "            raise NotImplementedError(f\"This type of input is not supported\")\n",
    "        start_date = fn(start)\n",
    "        end_date = fn(stop)\n",
    "        obj = copy.copy(self)  # shallow copy\n",
    "        # NOTE: Seriable will disable copy `self._data` so we manually assign them here\n",
    "        obj._data = self._data\n",
    "        obj._label = self._label\n",
    "        obj._index = self._index\n",
    "        new_batch_slices = []\n",
    "        for batch_slc in self.batch_slices:\n",
    "            date = self._index[batch_slc.stop - 1][1]\n",
    "            if start_date <= date <= end_date:\n",
    "                new_batch_slices.append(batch_slc)\n",
    "        obj.batch_slices = np.array(new_batch_slices)\n",
    "        new_daily_slices = []\n",
    "        for daily_slc in self.daily_slices:\n",
    "            date = self._index[daily_slc[0].stop - 1][1]\n",
    "            if start_date <= date <= end_date:\n",
    "                new_daily_slices.append(daily_slc)\n",
    "        obj.daily_slices = new_daily_slices\n",
    "        return obj\n",
    "\n",
    "    def restore_index(self, index):\n",
    "        if isinstance(index, torch.Tensor):\n",
    "            index = index.cpu().numpy()\n",
    "        return self._index[index]\n",
    "\n",
    "    def assign_data(self, index, vals):\n",
    "        if isinstance(self._data, torch.Tensor):\n",
    "            vals = _to_tensor(vals)\n",
    "        elif isinstance(vals, torch.Tensor):\n",
    "            vals = vals.detach().cpu().numpy()\n",
    "            index = index.detach().cpu().numpy()\n",
    "        self._data[index, -self.num_states :] = vals\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self._data[:, -self.num_states :] = 0\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"enable traning mode\"\"\"\n",
    "        self.batch_size, self.drop_last, self.shuffle = self.params\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"enable evaluation mode\"\"\"\n",
    "        self.batch_size = -1\n",
    "        self.drop_last = False\n",
    "        self.shuffle = False\n",
    "\n",
    "    def _get_slices(self):\n",
    "        if self.batch_size < 0:\n",
    "            slices = self.daily_slices.copy()\n",
    "            batch_size = -1 * self.batch_size\n",
    "        else:\n",
    "            slices = self.batch_slices.copy()\n",
    "            batch_size = self.batch_size\n",
    "        return slices, batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        slices, batch_size = self._get_slices()\n",
    "        if self.drop_last:\n",
    "            return len(slices) // batch_size\n",
    "        return (len(slices) + batch_size - 1) // batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        slices, batch_size = self._get_slices()\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(slices)\n",
    "\n",
    "        for i in range(len(slices))[::batch_size]:\n",
    "            if self.drop_last and i + batch_size > len(slices):\n",
    "                break\n",
    "            # get slices for this batch\n",
    "            slices_subset = slices[i : i + batch_size]\n",
    "            if self.batch_size < 0:\n",
    "                slices_subset = np.concatenate(slices_subset)\n",
    "            # collect data\n",
    "            data = []\n",
    "            label = []\n",
    "            index = []\n",
    "            for slc in slices_subset:\n",
    "                _data = self._data[slc].clone() if self.pin_memory else self._data[slc].copy()\n",
    "                if len(_data) != self.seq_len:\n",
    "                    if self.pin_memory:\n",
    "                        _data = torch.cat([self.zeros[: self.seq_len - len(_data)], _data], axis=0)\n",
    "                    else:\n",
    "                        _data = np.concatenate([self.zeros[: self.seq_len - len(_data)], _data], axis=0)\n",
    "                if self.num_states > 0:\n",
    "                    _data[-self.horizon :, -self.num_states :] = 0\n",
    "                data.append(_data)\n",
    "                label.append(self._label[slc.stop - 1])\n",
    "                index.append(slc.stop - 1)\n",
    "            # concate\n",
    "            index = torch.tensor(index, device=device)\n",
    "            if isinstance(data[0], torch.Tensor):\n",
    "                data = torch.stack(data)\n",
    "                label = torch.stack(label)\n",
    "            else:\n",
    "                data = _to_tensor(np.stack(data))\n",
    "                label = _to_tensor(np.stack(label))\n",
    "            # yield -> generator\n",
    "            yield {\"data\": data, \"label\": label, \"index\": index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TRA(nn.Module):\n",
    "    \"\"\"Temporal Routing Adaptor (TRA)\n",
    "\n",
    "    TRA takes historical prediction errors & latent representation as inputs,\n",
    "    then routes the input sample to a specific predictor for training & inference.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): input size (RNN/Transformer's hidden size)\n",
    "        num_states (int): number of latent states (i.e., trading patterns)\n",
    "            If `num_states=1`, then TRA falls back to traditional methods\n",
    "        hidden_size (int): hidden size of the router\n",
    "        tau (float): gumbel softmax temperature\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, num_states=1, hidden_size=8, tau=1.0, horizon = 20 , src_info=\"LR_TPE\"):\n",
    "        super().__init__()\n",
    "        self.num_states = num_states\n",
    "        self.tau = tau\n",
    "        self.horizon = horizon\n",
    "        self.src_info = src_info\n",
    "\n",
    "        if num_states > 1:\n",
    "            self.router = nn.LSTM(\n",
    "                input_size=num_states,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=1,\n",
    "                batch_first=True,\n",
    "            )\n",
    "            self.fc = nn.Linear(hidden_size + input_size, num_states)\n",
    "        self.predictors = nn.Linear(input_size, num_states)\n",
    "\n",
    "    def forward(self, hidden, hist_loss):\n",
    "        preds = self.predictors(hidden)\n",
    "\n",
    "        if self.num_states == 1:\n",
    "            final_pred = preds\n",
    "            prob = None\n",
    "        else:\n",
    "            # information type\n",
    "            router_out, _ = self.router(hist_loss[:,-self.horizon])\n",
    "            if \"LR\" in self.src_info:\n",
    "                latent_representation = hidden\n",
    "            else:\n",
    "                latent_representation = torch.randn_like(hidden)\n",
    "            if \"TPE\" in self.src_info:\n",
    "                temporal_pred_error = router_out[:, -1]\n",
    "            else:\n",
    "                temporal_pred_error = torch.randn_like(router_out[:, -1])\n",
    "\n",
    "            out = self.fc(torch.cat([temporal_pred_error, latent_representation], dim=-1))\n",
    "            prob = nn.functional.gumbel_softmax(out, dim=-1, tau=self.tau, hard=False)\n",
    "\n",
    "            if self.training:\n",
    "                final_pred = (preds * prob).sum(dim=-1 , keepdim = True)\n",
    "            else:\n",
    "                final_pred = preds[range(len(preds)), prob.argmax(dim=-1)].unsqueeze(-1)\n",
    "\n",
    "        return final_pred, preds, prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 60, 16])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 100\n",
    "s = 60\n",
    "f = 10\n",
    "horizon = 20\n",
    "num_states = 3\n",
    "x = torch.rand(b , s , f)\n",
    "y = torch.rand(b , s)\n",
    "label = y[:,-1].unsqueeze(-1)\n",
    "\n",
    "rnn = nn.LSTM(f , 16 , 2 , batch_first=True)\n",
    "tra = TRA(16 , num_states)\n",
    "hidden = rnn(x)[0]\n",
    "\n",
    "tra.predictors(hidden)\n",
    "hist_loss1 = tra.predictors(hidden) - y.unsqueeze(-1)\n",
    "pred , all_preds , prob = tra(hidden[:,-1] , hist_loss1[:,:-horizon])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0093, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scripts.special.TRA import *\n",
    "\n",
    "\n",
    "def optimal_transport_penalty(preds , label , prob , global_batch_steps = 0 , lamb = 0.01 , rho = 0.99):\n",
    "    if prob is not None:\n",
    "        square_error = (preds - label).square()\n",
    "        square_error -= square_error.min(dim=-1, keepdim=True).values  # normalize & ensure positive input\n",
    "        P = sinkhorn(-square_error, epsilon=0.01)  # sample assignment matrix\n",
    "        lamb = lamb * (rho**global_batch_steps)\n",
    "        reg = prob.log().mul(P).sum(dim=-1).mean()\n",
    "        return -lamb * reg\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "init_lamb = 0.01\n",
    "rho = 0.999\n",
    "global_batch_steps = 500\n",
    "b = 100\n",
    "s = 60\n",
    "f = 10\n",
    "horizon = 20\n",
    "num_states = 3\n",
    "x = torch.rand(b , s , f)\n",
    "y = torch.rand(b , s)\n",
    "label = y[:,-1].unsqueeze(-1)\n",
    "\n",
    "rnn = nn.LSTM(f , 16 , 2 , batch_first=True)\n",
    "tra = TRA(16 , num_states)\n",
    "hidden = rnn(x)[0]\n",
    "\n",
    "tra.predictors(hidden)\n",
    "hist_loss1 = tra.predictors(hidden) - y.unsqueeze(-1)\n",
    "pred , all_preds , prob = tra(hidden[:,-1] , hist_loss1[:,:-horizon])\n",
    "\n",
    "optimal_transport_penalty(all_preds , label , prob , global_batch_steps , init_lamb , rho)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
